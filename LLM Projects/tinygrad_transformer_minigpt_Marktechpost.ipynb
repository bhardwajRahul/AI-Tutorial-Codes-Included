{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import subprocess, sys, os\n",
        "print(\"Installing dependencies...\")\n",
        "subprocess.check_call([\"apt-get\", \"install\", \"-qq\", \"clang\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"git+https://github.com/tinygrad/tinygrad.git\"])\n",
        "\n",
        "import numpy as np\n",
        "from tinygrad import Tensor, nn, Device\n",
        "from tinygrad.nn import optim\n",
        "import time\n",
        "\n",
        "print(f\"üöÄ Using device: {Device.DEFAULT}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüìö PART 1: Tensor Operations & Autograd\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "x = Tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
        "y = Tensor([[2.0, 0.0], [1.0, 2.0]], requires_grad=True)\n",
        "\n",
        "z = (x @ y).sum() + (x ** 2).mean()\n",
        "z.backward()\n",
        "\n",
        "print(f\"x:\\n{x.numpy()}\")\n",
        "print(f\"y:\\n{y.numpy()}\")\n",
        "print(f\"z (scalar): {z.numpy()}\")\n",
        "print(f\"‚àÇz/‚àÇx:\\n{x.grad.numpy()}\")\n",
        "print(f\"‚àÇz/‚àÇy:\\n{y.grad.numpy()}\")"
      ],
      "metadata": {
        "id": "E9ArN2brj7co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nüß† PART 2: Building Custom Layers\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, dim, num_heads):\n",
        "        self.num_heads = num_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.qkv = Tensor.glorot_uniform(dim, 3 * dim)\n",
        "        self.out = Tensor.glorot_uniform(dim, dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape[0], x.shape[1], x.shape[2]\n",
        "        qkv = x.reshape(B * T, C).dot(self.qkv).reshape(B, T, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]\n",
        "        scale = (self.head_dim ** -0.5)\n",
        "        attn = (q @ k.transpose(-2, -1)) * scale\n",
        "        attn = attn.softmax(axis=-1)\n",
        "        out = (attn @ v).transpose(1, 2).reshape(B, T, C)\n",
        "        return out.reshape(B * T, C).dot(self.out).reshape(B, T, C)\n",
        "\n",
        "class TransformerBlock:\n",
        "    def __init__(self, dim, num_heads):\n",
        "        self.attn = MultiHeadAttention(dim, num_heads)\n",
        "        self.ff1 = Tensor.glorot_uniform(dim, 4 * dim)\n",
        "        self.ff2 = Tensor.glorot_uniform(4 * dim, dim)\n",
        "        self.ln1_w = Tensor.ones(dim)\n",
        "        self.ln2_w = Tensor.ones(dim)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x + self.attn(self._layernorm(x, self.ln1_w))\n",
        "        ff = x.reshape(-1, x.shape[-1])\n",
        "        ff = ff.dot(self.ff1).gelu().dot(self.ff2)\n",
        "        x = x + ff.reshape(x.shape)\n",
        "        return self._layernorm(x, self.ln2_w)\n",
        "\n",
        "    def _layernorm(self, x, w):\n",
        "        mean = x.mean(axis=-1, keepdim=True)\n",
        "        var = ((x - mean) ** 2).mean(axis=-1, keepdim=True)\n",
        "        return w * (x - mean) / (var + 1e-5).sqrt()"
      ],
      "metadata": {
        "id": "mgiX_kZRj7Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nü§ñ PART 3: Mini-GPT Architecture\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "class MiniGPT:\n",
        "    def __init__(self, vocab_size=256, dim=128, num_heads=4, num_layers=2, max_len=32):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dim = dim\n",
        "        self.tok_emb = Tensor.glorot_uniform(vocab_size, dim)\n",
        "        self.pos_emb = Tensor.glorot_uniform(max_len, dim)\n",
        "        self.blocks = [TransformerBlock(dim, num_heads) for _ in range(num_layers)]\n",
        "        self.ln_f = Tensor.ones(dim)\n",
        "        self.head = Tensor.glorot_uniform(dim, vocab_size)\n",
        "\n",
        "    def __call__(self, idx):\n",
        "        B, T = idx.shape[0], idx.shape[1]\n",
        "        tok_emb = self.tok_emb[idx.flatten()].reshape(B, T, self.dim)\n",
        "        pos_emb = self.pos_emb[:T].reshape(1, T, self.dim)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        mean = x.mean(axis=-1, keepdim=True)\n",
        "        var = ((x - mean) ** 2).mean(axis=-1, keepdim=True)\n",
        "        x = self.ln_f * (x - mean) / (var + 1e-5).sqrt()\n",
        "        return x.reshape(B * T, self.dim).dot(self.head).reshape(B, T, self.vocab_size)\n",
        "\n",
        "    def get_params(self):\n",
        "        params = [self.tok_emb, self.pos_emb, self.ln_f, self.head]\n",
        "        for block in self.blocks:\n",
        "            params.extend([block.attn.qkv, block.attn.out, block.ff1, block.ff2, block.ln1_w, block.ln2_w])\n",
        "        return params\n",
        "\n",
        "model = MiniGPT(vocab_size=256, dim=64, num_heads=4, num_layers=2, max_len=16)\n",
        "params = model.get_params()\n",
        "total_params = sum(p.numel() for p in params)\n",
        "print(f\"Model initialized with {total_params:,} parameters\")"
      ],
      "metadata": {
        "id": "5P0HcciIj7UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\nüèãÔ∏è PART 4: Training Loop\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "def gen_data(batch_size, seq_len):\n",
        "    x = np.random.randint(0, 256, (batch_size, seq_len))\n",
        "    y = np.roll(x, 1, axis=1)\n",
        "    y[:, 0] = x[:, 0]\n",
        "    return Tensor(x, dtype='int32'), Tensor(y, dtype='int32')\n",
        "\n",
        "optimizer = optim.Adam(params, lr=0.001)\n",
        "losses = []\n",
        "\n",
        "print(\"Training to predict previous token in sequence...\")\n",
        "with Tensor.train():\n",
        "    for step in range(20):\n",
        "        start = time.time()\n",
        "        x_batch, y_batch = gen_data(batch_size=16, seq_len=16)\n",
        "        logits = model(x_batch)\n",
        "        B, T, V = logits.shape[0], logits.shape[1], logits.shape[2]\n",
        "        loss = logits.reshape(B * T, V).sparse_categorical_crossentropy(y_batch.reshape(B * T))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.numpy())\n",
        "        elapsed = time.time() - start\n",
        "        if step % 5 == 0:\n",
        "            print(f\"Step {step:3d} | Loss: {loss.numpy():.4f} | Time: {elapsed*1000:.1f}ms\")\n",
        "\n",
        "print(\"\\n\\n‚ö° PART 5: Lazy Evaluation & Kernel Fusion\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "N = 512\n",
        "a = Tensor.randn(N, N)\n",
        "b = Tensor.randn(N, N)\n",
        "\n",
        "print(\"Creating computation: (A @ B.T + A).sum()\")\n",
        "lazy_result = (a @ b.T + a).sum()\n",
        "print(\"‚Üí No computation done yet (lazy evaluation)\")\n",
        "\n",
        "print(\"\\nCalling .realize() to execute...\")\n",
        "start = time.time()\n",
        "realized = lazy_result.realize()\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"‚úì Computed in {elapsed*1000:.2f}ms\")\n",
        "print(f\"Result: {realized.numpy():.4f}\")\n",
        "print(\"\\nNote: Operations were fused into optimized kernels!\")"
      ],
      "metadata": {
        "id": "uBiKIFLbj7RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b4hNHZWe1aq",
        "outputId": "39e22040-0dbc-4f0d-96a7-3d79f0e01383"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing dependencies...\n",
            "üöÄ Using device: CPU\n",
            "============================================================\n",
            "\n",
            "üìö PART 1: Tensor Operations & Autograd\n",
            "------------------------------------------------------------\n",
            "x:\n",
            "[[1. 2.]\n",
            " [3. 4.]]\n",
            "y:\n",
            "[[2. 0.]\n",
            " [1. 2.]]\n",
            "z (scalar): 33.5\n",
            "‚àÇz/‚àÇx:\n",
            "[[2.5 4. ]\n",
            " [3.5 5. ]]\n",
            "‚àÇz/‚àÇy:\n",
            "[[4. 4.]\n",
            " [6. 6.]]\n",
            "\n",
            "\n",
            "üß† PART 2: Building Custom Layers\n",
            "------------------------------------------------------------\n",
            "\n",
            "ü§ñ PART 3: Mini-GPT Architecture\n",
            "------------------------------------------------------------\n",
            "Model initialized with 132,416 parameters\n",
            "\n",
            "\n",
            "üèãÔ∏è PART 4: Training Loop\n",
            "------------------------------------------------------------\n",
            "Training to predict previous token in sequence...\n",
            "Step   0 | Loss: 5.8010 | Time: 16499.5ms\n",
            "Step   5 | Loss: 5.6585 | Time: 1381.9ms\n",
            "Step  10 | Loss: 5.7122 | Time: 1135.8ms\n",
            "Step  15 | Loss: 5.6544 | Time: 1170.9ms\n",
            "\n",
            "\n",
            "‚ö° PART 5: Lazy Evaluation & Kernel Fusion\n",
            "------------------------------------------------------------\n",
            "Creating computation: (A @ B.T + A).sum()\n",
            "‚Üí No computation done yet (lazy evaluation)\n",
            "\n",
            "Calling .realize() to execute...\n",
            "‚úì Computed in 1058.66ms\n",
            "Result: -2449.1692\n",
            "\n",
            "Note: Operations were fused into optimized kernels!\n",
            "\n",
            "\n",
            "üîß PART 6: Custom Operations\n",
            "------------------------------------------------------------\n",
            "Input:    [[-2. -1.  0.  1.  2.]]\n",
            "Swish(x): [[-0.23840587 -0.26894143  0.          0.7310586   1.761594  ]]\n",
            "Gradient: [[-0.09078426  0.07232948  0.5         0.92767054  1.0907843 ]]\n",
            "\n",
            "\n",
            "============================================================\n",
            "‚úÖ Tutorial Complete!\n",
            "============================================================\n",
            "\n",
            "Key Concepts Covered:\n",
            "1. Tensor operations with automatic differentiation\n",
            "2. Custom neural network layers (Attention, Transformer)\n",
            "3. Building a mini-GPT language model from scratch\n",
            "4. Training loop with Adam optimizer\n",
            "5. Lazy evaluation and kernel fusion\n",
            "6. Custom activation functions\n",
            "\n",
            "Next Steps:\n",
            "- Try DEBUG=3 or DEBUG=4 to see generated kernels\n",
            "- Implement batch normalization or dropout\n",
            "- Train on real datasets (MNIST, text data)\n",
            "- Explore different optimizers (SGD, AdamW)\n",
            "- Add learning rate scheduling\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\\nüîß PART 6: Custom Operations\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "def custom_activation(x):\n",
        "    return x * x.sigmoid()\n",
        "\n",
        "x = Tensor([[-2.0, -1.0, 0.0, 1.0, 2.0]], requires_grad=True)\n",
        "y = custom_activation(x)\n",
        "loss = y.sum()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"Input:    {x.numpy()}\")\n",
        "print(f\"Swish(x): {y.numpy()}\")\n",
        "print(f\"Gradient: {x.grad.numpy()}\")\n",
        "\n",
        "print(\"\\n\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Tutorial Complete!\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\"\"\n",
        "Key Concepts Covered:\n",
        "1. Tensor operations with automatic differentiation\n",
        "2. Custom neural network layers (Attention, Transformer)\n",
        "3. Building a mini-GPT language model from scratch\n",
        "4. Training loop with Adam optimizer\n",
        "5. Lazy evaluation and kernel fusion\n",
        "6. Custom activation functions\n",
        "\"\"\")"
      ]
    }
  ]
}