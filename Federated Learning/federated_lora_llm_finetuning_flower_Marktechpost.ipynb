{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ebc9d1d49c4425986a69a96a2e6426d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_992f92db62854dcf8771459f4c32b60f",
              "IPY_MODEL_664148f54c5942f7bc3e1aaba2c39561",
              "IPY_MODEL_826da38c82dd4ec4a2cd60c120e39d56"
            ],
            "layout": "IPY_MODEL_6547e56e4dcf4dd28443f12e577cd6d9"
          }
        },
        "992f92db62854dcf8771459f4c32b60f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8eadeba113af4687b8cc337a61c361bf",
            "placeholder": "​",
            "style": "IPY_MODEL_2d0fc71fb8e246ebaa7042d4c14a9b4a",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "664148f54c5942f7bc3e1aaba2c39561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0a49fa7a0b34c59960407071ce583db",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4449fe2fd8254c979846ea4bb1b56eb7",
            "value": 26
          }
        },
        "826da38c82dd4ec4a2cd60c120e39d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46c9ef977bf146748c1ebccd6b25d104",
            "placeholder": "​",
            "style": "IPY_MODEL_ed97c1ed7f8340179664546050e7b59f",
            "value": " 26.0/26.0 [00:00&lt;00:00, 2.25kB/s]"
          }
        },
        "6547e56e4dcf4dd28443f12e577cd6d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8eadeba113af4687b8cc337a61c361bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0fc71fb8e246ebaa7042d4c14a9b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0a49fa7a0b34c59960407071ce583db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4449fe2fd8254c979846ea4bb1b56eb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46c9ef977bf146748c1ebccd6b25d104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed97c1ed7f8340179664546050e7b59f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fda2ca6977654a2baafa47fff7d4b696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4904c0722415494089821e54234efd63",
              "IPY_MODEL_82594ad2433f4371ab574c7bc251002e",
              "IPY_MODEL_01249ccbf1c14f95b1f7e43dcfabdcc4"
            ],
            "layout": "IPY_MODEL_63591a8cc68440e3897bedf3980c5943"
          }
        },
        "4904c0722415494089821e54234efd63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fdc151a875c43aeb868382dc3826dee",
            "placeholder": "​",
            "style": "IPY_MODEL_686a06b6ccde412d97b1413187dfcea9",
            "value": "config.json: 100%"
          }
        },
        "82594ad2433f4371ab574c7bc251002e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ef83fde516047668a1b75d85276247c",
            "max": 762,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ad53fe040514304b115127169472f6b",
            "value": 762
          }
        },
        "01249ccbf1c14f95b1f7e43dcfabdcc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_024037ee539042b69775e69236589990",
            "placeholder": "​",
            "style": "IPY_MODEL_ee52d8df5eae49a9bc3c999274b2299d",
            "value": " 762/762 [00:00&lt;00:00, 40.7kB/s]"
          }
        },
        "63591a8cc68440e3897bedf3980c5943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fdc151a875c43aeb868382dc3826dee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "686a06b6ccde412d97b1413187dfcea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ef83fde516047668a1b75d85276247c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad53fe040514304b115127169472f6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "024037ee539042b69775e69236589990": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee52d8df5eae49a9bc3c999274b2299d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6ef5afff6674895b4b69180519ed116": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a218f8fe56c4771b587da6ae4e7a9d0",
              "IPY_MODEL_cb361320135d408c93a0e620cdccf2c4",
              "IPY_MODEL_7f7fb1f0e5464c608a3efffeedef1f8e"
            ],
            "layout": "IPY_MODEL_8ed227a9374c47e5b67eb9d55f648e91"
          }
        },
        "9a218f8fe56c4771b587da6ae4e7a9d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6fd770461d83437daef059a43d903750",
            "placeholder": "​",
            "style": "IPY_MODEL_42242dabc4d7472da5e9ffea491debb7",
            "value": "vocab.json: 100%"
          }
        },
        "cb361320135d408c93a0e620cdccf2c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9e1a4da2aa49b8b5ddb6247c39f819",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c66a895789c44ad8b53fa1fa5e3f3616",
            "value": 1042301
          }
        },
        "7f7fb1f0e5464c608a3efffeedef1f8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcb615bfc42747e8bd946b80a1c522e8",
            "placeholder": "​",
            "style": "IPY_MODEL_e2597b22e0b24888b91be5db31d9605b",
            "value": " 1.04M/1.04M [00:02&lt;00:00, 464kB/s]"
          }
        },
        "8ed227a9374c47e5b67eb9d55f648e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fd770461d83437daef059a43d903750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42242dabc4d7472da5e9ffea491debb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df9e1a4da2aa49b8b5ddb6247c39f819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c66a895789c44ad8b53fa1fa5e3f3616": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bcb615bfc42747e8bd946b80a1c522e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2597b22e0b24888b91be5db31d9605b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "325ad0325e0749fb8366e529a45b991a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9653fc7844d04edc977ca00ccd08a7f0",
              "IPY_MODEL_dbe1de76f4254d2fb4baf36ea613bb63",
              "IPY_MODEL_fc467a307a984913b537099b30d59f09"
            ],
            "layout": "IPY_MODEL_40d6c13500134dcb9034110f9f214f4e"
          }
        },
        "9653fc7844d04edc977ca00ccd08a7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5f196ff5e6a4455935c8f90121b7746",
            "placeholder": "​",
            "style": "IPY_MODEL_b8ebe8ae831c4275996784500a71bb81",
            "value": "merges.txt: 100%"
          }
        },
        "dbe1de76f4254d2fb4baf36ea613bb63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb1bbc990a914256a4059874445b36a1",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5f8eadcc8f349c6b162a3e5c9ae790e",
            "value": 456318
          }
        },
        "fc467a307a984913b537099b30d59f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4983fc68b2c94122a47fc10045714dfb",
            "placeholder": "​",
            "style": "IPY_MODEL_df719b9ea80e4dcba14b4835a6f4d60b",
            "value": " 456k/456k [00:00&lt;00:00, 9.26MB/s]"
          }
        },
        "40d6c13500134dcb9034110f9f214f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f196ff5e6a4455935c8f90121b7746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8ebe8ae831c4275996784500a71bb81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb1bbc990a914256a4059874445b36a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5f8eadcc8f349c6b162a3e5c9ae790e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4983fc68b2c94122a47fc10045714dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df719b9ea80e4dcba14b4835a6f4d60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de0bcd21ff984a648bf092179ca924b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1a2be94d109440448d6760d8a8c6e8f7",
              "IPY_MODEL_ecc198640fdc483498a3749c1ab59b5e",
              "IPY_MODEL_2f4b9b6c987e449fbe5e9a0ab434322e"
            ],
            "layout": "IPY_MODEL_386ade24f3274bd8921319a2cd5c47ae"
          }
        },
        "1a2be94d109440448d6760d8a8c6e8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_948f059d583240649d764a3ae951514f",
            "placeholder": "​",
            "style": "IPY_MODEL_b673b62c423c47c6b6c1e1088c9a1434",
            "value": "tokenizer.json: 100%"
          }
        },
        "ecc198640fdc483498a3749c1ab59b5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d406fb8d63de4d75a9c4192219f682a6",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c34aa18b9cee46aea51086d8914b3a57",
            "value": 1355256
          }
        },
        "2f4b9b6c987e449fbe5e9a0ab434322e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5066de95a59247439518999d6b036a7e",
            "placeholder": "​",
            "style": "IPY_MODEL_31c4840e9d844f4081a8daf25fee9865",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 5.41MB/s]"
          }
        },
        "386ade24f3274bd8921319a2cd5c47ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "948f059d583240649d764a3ae951514f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b673b62c423c47c6b6c1e1088c9a1434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d406fb8d63de4d75a9c4192219f682a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c34aa18b9cee46aea51086d8914b3a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5066de95a59247439518999d6b036a7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c4840e9d844f4081a8daf25fee9865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"protobuf<5\" \"flwr[simulation]\" transformers peft accelerate datasets sentencepiece\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    !pip -q install -U bitsandbytes\n",
        "import os\n",
        "os.environ[\"RAY_DISABLE_USAGE_STATS\"] = \"1\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset\n",
        "import flwr as fl\n",
        "from flwr.common import Context\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "SEED = 7\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", DEVICE)\n",
        "GPU_MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "CPU_MODEL_ID = \"distilgpt2\"\n",
        "MODEL_ID = GPU_MODEL_ID if DEVICE == \"cuda\" else CPU_MODEL_ID\n",
        "MAX_LEN = 256 if DEVICE == \"cuda\" else 192\n",
        "NUM_CLIENTS = 3\n",
        "ROUNDS = 3\n",
        "LOCAL_EPOCHS = 1\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 4\n",
        "LR = 2e-4\n",
        "WARMUP_STEPS = 5\n",
        "WEIGHT_DECAY = 0.0\n",
        "LOG_EVERY = 10\n",
        "CLIENT_TEXTS: Dict[int, List[str]] = {\n",
        "    0: [\n",
        "        \"Policy memo: Employees must rotate on-call weekly and document incidents in the internal tracker.\",\n",
        "        \"Runbook: If latency spikes, check the database connection pool and recent deploys, then roll back if needed.\",\n",
        "        \"Security note: Never paste customer identifiers into public issue trackers. Use redacted tokens.\",\n",
        "        \"Engineering guideline: Prefer idempotent retries for event processing; avoid duplicate side-effects.\",\n",
        "        \"Postmortem template: impact, timeline, root cause, contributing factors, action items, owners, deadlines.\"\n",
        "    ],\n",
        "    1: [\n",
        "        \"Credit risk review: monitor delinquency curves by cohort and compare against seasonal baselines.\",\n",
        "        \"Fraud signals: repeated small authorizations, device changes, and sudden merchant-category shifts require review.\",\n",
        "        \"Portfolio strategy: tighten limits on volatile segments while maintaining service levels for stable accounts.\",\n",
        "        \"Operational note: reconcile chargebacks weekly and track win-rate by reason code.\",\n",
        "        \"Internal SOP: escalation path is analyst -> manager -> compliance for high-risk cases.\"\n",
        "    ],\n",
        "    2: [\n",
        "        \"Fleet ops: preventive maintenance reduces downtime; prioritize vehicles with repeated fault codes.\",\n",
        "        \"Dispatch note: optimize routes by time windows and driver hours to reduce empty miles.\",\n",
        "        \"Safety policy: enforce rest breaks and log inspections before long-haul trips.\",\n",
        "        \"Inventory update: track spare parts usage; reorder thresholds should reflect lead time and seasonality.\",\n",
        "        \"Customer SLA: late deliveries require proactive notifications and documented root cause.\"\n",
        "    ],\n",
        "}\n",
        "for cid in list(CLIENT_TEXTS.keys()):\n",
        "    base = CLIENT_TEXTS[cid]\n",
        "    CLIENT_TEXTS[cid] = base + [f\"Q: Summarize this for leadership. A: {t}\" for t in base]\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "bnb_config: Optional[BitsAndBytesConfig] = None\n",
        "if DEVICE == \"cuda\":\n",
        "    compute_dtype = torch.bfloat16 if torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=compute_dtype)\n",
        "if \"gpt2\" in MODEL_ID.lower():\n",
        "    TARGET_MODULES = [\"c_attn\", \"c_proj\"]\n",
        "else:\n",
        "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "lora_config = LoraConfig(r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=TARGET_MODULES)\n",
        "def model_primary_device(model) -> torch.device:\n",
        "    return next(model.parameters()).device\n",
        "def build_model_with_lora():\n",
        "    if DEVICE == \"cuda\":\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", quantization_config=bnb_config, torch_dtype=\"auto\")\n",
        "        model = prepare_model_for_kbit_training(model)\n",
        "    else:\n",
        "        model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float32)\n",
        "        model.to(\"cpu\")\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.train()\n",
        "    return model\n",
        "def make_dataset(texts: List[str]) -> Dataset:\n",
        "    ds = Dataset.from_dict({\"text\": texts})\n",
        "    def tok(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, max_length=MAX_LEN, padding=\"max_length\")\n",
        "    ds = ds.map(tok, batched=True, remove_columns=[\"text\"])\n",
        "    return ds\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "def lora_state_keys(model) -> List[str]:\n",
        "    sd = model.state_dict()\n",
        "    keys = sorted([k for k in sd.keys() if \"lora_\" in k])\n",
        "    if not keys:\n",
        "        raise RuntimeError(\"No LoRA keys found. Your model might not have the target_modules specified. \" f\"Current TARGET_MODULES={TARGET_MODULES}, MODEL_ID={MODEL_ID}\")\n",
        "    return keys\n",
        "def get_lora_ndarrays(model) -> List[np.ndarray]:\n",
        "    sd = model.state_dict()\n",
        "    keys = lora_state_keys(model)\n",
        "    return [sd[k].detach().float().cpu().numpy() for k in keys]\n",
        "def set_lora_ndarrays(model, arrays: List[np.ndarray]) -> None:\n",
        "    keys = lora_state_keys(model)\n",
        "    if len(keys) != len(arrays):\n",
        "        raise ValueError(f\"Mismatch: got {len(arrays)} arrays but expected {len(keys)}.\")\n",
        "    sd = model.state_dict()\n",
        "    for k, arr in zip(keys, arrays):\n",
        "        t = torch.from_numpy(arr).to(sd[k].device).to(sd[k].dtype)\n",
        "        sd[k].copy_(t)\n",
        "def cosine_warmup_lr(step: int, total_steps: int, base_lr: float, warmup_steps: int) -> float:\n",
        "    if step < warmup_steps:\n",
        "        return base_lr * (step + 1) / max(1, warmup_steps)\n",
        "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    return base_lr * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "@torch.no_grad()\n",
        "def eval_loss(model, ds: Dataset, max_batches: int = 20) -> float:\n",
        "    model.eval()\n",
        "    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
        "    losses = []\n",
        "    dev = model_primary_device(model)\n",
        "    for i, batch in enumerate(dl):\n",
        "        if i >= max_batches:\n",
        "            break\n",
        "        batch = {k: v.to(dev) for k, v in batch.items()}\n",
        "        out = model(**batch, labels=batch[\"input_ids\"])\n",
        "        losses.append(float(out.loss.detach().cpu()))\n",
        "    model.train()\n",
        "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
        "def train_one_client_round(model, ds: Dataset, epochs: int, lr: float, grad_accum: int, warmup_steps: int) -> Tuple[float, int]:\n",
        "    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
        "    total_steps = max(1, (len(dl) * epochs) // max(1, grad_accum))\n",
        "    step = 0\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    running = []\n",
        "    examples = 0\n",
        "    dev = model_primary_device(model)\n",
        "    for _ in range(epochs):\n",
        "        for bi, batch in enumerate(dl):\n",
        "            batch = {k: v.to(dev) for k, v in batch.items()}\n",
        "            out = model(**batch, labels=batch[\"input_ids\"])\n",
        "            loss = out.loss / grad_accum\n",
        "            loss.backward()\n",
        "            running.append(float(loss.detach().cpu()) * grad_accum)\n",
        "            examples += batch[\"input_ids\"].shape[0]\n",
        "            if (bi + 1) % grad_accum == 0:\n",
        "                lr_t = cosine_warmup_lr(step, total_steps, lr, warmup_steps)\n",
        "                for pg in optimizer.param_groups:\n",
        "                    pg[\"lr\"] = lr_t\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                step += 1\n",
        "                if step % LOG_EVERY == 0:\n",
        "                    print(f\"  step={step}/{total_steps} loss={np.mean(running[-LOG_EVERY:]):.4f} lr={lr_t:.2e}\")\n",
        "    return float(np.mean(running)) if running else float(\"nan\"), examples"
      ],
      "metadata": {
        "id": "ykthGREfhO85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FedLoRAClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid: int):\n",
        "        self.cid = cid\n",
        "        self._model = None\n",
        "        self._ds_train = None\n",
        "        self._ds_eval = None\n",
        "    def _ensure(self):\n",
        "        if self._model is None:\n",
        "            print(f\"[Client {self.cid}] Loading model + LoRA (MODEL_ID={MODEL_ID})...\")\n",
        "            self._model = build_model_with_lora()\n",
        "            texts = CLIENT_TEXTS[self.cid].copy()\n",
        "            random.shuffle(texts)\n",
        "            split = max(1, int(0.8 * len(texts)))\n",
        "            self._ds_train = make_dataset(texts[:split])\n",
        "            self._ds_eval = make_dataset(texts[split:])\n",
        "    def get_parameters(self, config):\n",
        "        self._ensure()\n",
        "        return get_lora_ndarrays(self._model)\n",
        "    def fit(self, parameters, config):\n",
        "        self._ensure()\n",
        "        set_lora_ndarrays(self._model, parameters)\n",
        "        loss_before = eval_loss(self._model, self._ds_eval, max_batches=10)\n",
        "        print(f\"[Client {self.cid}] eval_loss_before={loss_before:.4f}\")\n",
        "        train_loss, n_examples = train_one_client_round(self._model, self._ds_train, epochs=int(config.get(\"local_epochs\", LOCAL_EPOCHS)), lr=float(config.get(\"lr\", LR)), grad_accum=int(config.get(\"grad_accum\", GRAD_ACCUM)), warmup_steps=int(config.get(\"warmup_steps\", WARMUP_STEPS)))\n",
        "        loss_after = eval_loss(self._model, self._ds_eval, max_batches=10)\n",
        "        print(f\"[Client {self.cid}] train_loss={train_loss:.4f} eval_loss_after={loss_after:.4f}\")\n",
        "        new_params = get_lora_ndarrays(self._model)\n",
        "        metrics = {\"eval_loss_before\": loss_before, \"eval_loss_after\": loss_after, \"train_loss\": train_loss}\n",
        "        return new_params, n_examples, metrics\n",
        "    def evaluate(self, parameters, config):\n",
        "        self._ensure()\n",
        "        set_lora_ndarrays(self._model, parameters)\n",
        "        loss = eval_loss(self._model, self._ds_eval, max_batches=20)\n",
        "        return float(loss), len(self._ds_eval), {\"eval_loss\": float(loss)}\n",
        "def client_fn(context: Context):\n",
        "    cid = None\n",
        "    try:\n",
        "        cid = int(context.node_config.get(\"partition-id\"))\n",
        "    except Exception:\n",
        "        try:\n",
        "            cid = int(context.node_id)\n",
        "        except Exception:\n",
        "            cid = 0\n",
        "    return FedLoRAClient(cid).to_client()"
      ],
      "metadata": {
        "id": "bWE3carPhOvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_config(server_round: int):\n",
        "    return {\"local_epochs\": LOCAL_EPOCHS, \"lr\": LR, \"grad_accum\": GRAD_ACCUM, \"warmup_steps\": WARMUP_STEPS}\n",
        "strategy = fl.server.strategy.FedAvg(fraction_fit=1.0, fraction_evaluate=1.0, min_fit_clients=NUM_CLIENTS, min_evaluate_clients=NUM_CLIENTS, min_available_clients=NUM_CLIENTS, on_fit_config_fn=fit_config)\n",
        "print(\"\\nStarting Flower simulation...\\n\")\n",
        "client_resources = {\"num_cpus\": 2, \"num_gpus\": 0.0}\n",
        "if DEVICE == \"cuda\":\n",
        "    client_resources = {\"num_cpus\": 2, \"num_gpus\": 0.25}\n",
        "history = fl.simulation.start_simulation(client_fn=client_fn, num_clients=NUM_CLIENTS, config=fl.server.ServerConfig(num_rounds=ROUNDS), strategy=strategy, client_resources=client_resources, ray_init_args={\"include_dashboard\": False, \"ignore_reinit_error\": True})\n",
        "print(\"\\nSimulation done.\")"
      ],
      "metadata": {
        "id": "NY666g-ShOsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_model = build_model_with_lora()\n",
        "demo_model.eval()\n",
        "prompt = \"Summarize this internal note for leadership in 2 bullets:\\nDispatch note: optimize routes by time windows and driver hours to reduce empty miles.\\n\\nAnswer:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "dev = model_primary_device(demo_model)\n",
        "inputs = {k: v.to(dev) for k, v in inputs.items()}\n",
        "with torch.no_grad():\n",
        "    out = demo_model.generate(**inputs, max_new_tokens=80, do_sample=True, temperature=0.8, top_p=0.95, repetition_penalty=1.05, eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.pad_token_id)\n",
        "print(\"\\n=== Generation output ===\\n\")\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "qbd7bSdLhOpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(history))\n",
        "print(history.__dict__.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1ebc9d1d49c4425986a69a96a2e6426d",
            "992f92db62854dcf8771459f4c32b60f",
            "664148f54c5942f7bc3e1aaba2c39561",
            "826da38c82dd4ec4a2cd60c120e39d56",
            "6547e56e4dcf4dd28443f12e577cd6d9",
            "8eadeba113af4687b8cc337a61c361bf",
            "2d0fc71fb8e246ebaa7042d4c14a9b4a",
            "c0a49fa7a0b34c59960407071ce583db",
            "4449fe2fd8254c979846ea4bb1b56eb7",
            "46c9ef977bf146748c1ebccd6b25d104",
            "ed97c1ed7f8340179664546050e7b59f",
            "fda2ca6977654a2baafa47fff7d4b696",
            "4904c0722415494089821e54234efd63",
            "82594ad2433f4371ab574c7bc251002e",
            "01249ccbf1c14f95b1f7e43dcfabdcc4",
            "63591a8cc68440e3897bedf3980c5943",
            "4fdc151a875c43aeb868382dc3826dee",
            "686a06b6ccde412d97b1413187dfcea9",
            "1ef83fde516047668a1b75d85276247c",
            "3ad53fe040514304b115127169472f6b",
            "024037ee539042b69775e69236589990",
            "ee52d8df5eae49a9bc3c999274b2299d",
            "c6ef5afff6674895b4b69180519ed116",
            "9a218f8fe56c4771b587da6ae4e7a9d0",
            "cb361320135d408c93a0e620cdccf2c4",
            "7f7fb1f0e5464c608a3efffeedef1f8e",
            "8ed227a9374c47e5b67eb9d55f648e91",
            "6fd770461d83437daef059a43d903750",
            "42242dabc4d7472da5e9ffea491debb7",
            "df9e1a4da2aa49b8b5ddb6247c39f819",
            "c66a895789c44ad8b53fa1fa5e3f3616",
            "bcb615bfc42747e8bd946b80a1c522e8",
            "e2597b22e0b24888b91be5db31d9605b",
            "325ad0325e0749fb8366e529a45b991a",
            "9653fc7844d04edc977ca00ccd08a7f0",
            "dbe1de76f4254d2fb4baf36ea613bb63",
            "fc467a307a984913b537099b30d59f09",
            "40d6c13500134dcb9034110f9f214f4e",
            "a5f196ff5e6a4455935c8f90121b7746",
            "b8ebe8ae831c4275996784500a71bb81",
            "cb1bbc990a914256a4059874445b36a1",
            "a5f8eadcc8f349c6b162a3e5c9ae790e",
            "4983fc68b2c94122a47fc10045714dfb",
            "df719b9ea80e4dcba14b4835a6f4d60b",
            "de0bcd21ff984a648bf092179ca924b5",
            "1a2be94d109440448d6760d8a8c6e8f7",
            "ecc198640fdc483498a3749c1ab59b5e",
            "2f4b9b6c987e449fbe5e9a0ab434322e",
            "386ade24f3274bd8921319a2cd5c47ae",
            "948f059d583240649d764a3ae951514f",
            "b673b62c423c47c6b6c1e1088c9a1434",
            "d406fb8d63de4d75a9c4192219f682a6",
            "c34aa18b9cee46aea51086d8914b3a57",
            "5066de95a59247439518999d6b036a7e",
            "31c4840e9d844f4081a8daf25fee9865"
          ]
        },
        "id": "aHpUqZ1mWBd3",
        "outputId": "0af45265-e66d-4561-bd80-724afb2b9acd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m113.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m732.0/732.0 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ebc9d1d49c4425986a69a96a2e6426d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fda2ca6977654a2baafa47fff7d4b696"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6ef5afff6674895b4b69180519ed116"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "325ad0325e0749fb8366e529a45b991a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de0bcd21ff984a648bf092179ca924b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "WARNING:flwr:DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=3, no round_timeout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Flower simulation...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "2026-01-16 15:08:13,373\tINFO worker.py:1771 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'object_store_memory': 3947532288.0, 'node:172.28.0.12': 1.0, 'memory': 7895064576.0, 'node:__internal_head__': 1.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 2, 'num_gpus': 0.0}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=928)\u001b[0m 2026-01-16 15:08:27.283140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\u001b[36m(pid=928)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=928)\u001b[0m E0000 00:00:1768576107.324367     928 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=928)\u001b[0m E0000 00:00:1768576107.336078     928 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=928)\u001b[0m W0000 00:00:1768576107.366324     928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=928)\u001b[0m W0000 00:00:1768576107.366647     928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=928)\u001b[0m W0000 00:00:1768576107.366661     928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=928)\u001b[0m W0000 00:00:1768576107.366664     928 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 1] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 71020859-8af3-4624-826c-e53f3dd09be7)')' thrown while requesting HEAD https://huggingface.co/distilgpt2/resolve/main/adapter_config.json\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 71020859-8af3-4624-826c-e53f3dd09be7)')' thrown while requesting HEAD https://huggingface.co/distilgpt2/resolve/main/adapter_config.json\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m Retrying in 1s [Retry 1/5].\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m `torch_dtype` is deprecated! Use `dtype` instead!\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m /usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m   warnings.warn(\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      Evaluation returned no results (`None`)\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
            "Map: 100%|██████████| 8/8 [00:00<00:00, 884.01 examples/s]\n",
            "Map: 100%|██████████| 2/2 [00:00<00:00, 479.16 examples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 0] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 996.06 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 364.63 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 2] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1370.69 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 536.70 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 1] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1435.48 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 513.57 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 2] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1272.45 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 422.75 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 0] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1030.89 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 448.25 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 1] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1232.08 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 384.78 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 2] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1413.18 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 480.58 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 1] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1070.97 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 342.53 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 0] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
            "Map: 100%|██████████| 8/8 [00:00<00:00, 1514.26 examples/s]\n",
            "Map: 100%|██████████| 2/2 [00:00<00:00, 523.63 examples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 1] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1101.19 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 438.32 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 2] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1403.83 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 509.91 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 0] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 3 clients (out of 3)\n",
            "Map: 100%|██████████| 8/8 [00:00<00:00, 1557.99 examples/s]\n",
            "Map: 100%|██████████| 2/2 [00:00<00:00, 505.55 examples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 1] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1117.81 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 494.55 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 0] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1524.58 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "Map: 100%|██████████| 2/2 [00:00<00:00, 513.38 examples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 2] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 916.39 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 128, in handle_legacy_message_from_msgtype\n",
            "    fit_res = maybe_call_fit(\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 224, in maybe_call_fit\n",
            "    return client.fit(fit_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 227, in _fit\n",
            "    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 304, in fit\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 3 failures\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 3 clients (out of 3)\n",
            "Map: 100%|██████████| 2/2 [00:00<00:00, 266.80 examples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 2] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1588.53 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 562.58 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 0] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1429.73 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "Map: 100%|██████████| 2/2 [00:00<00:00, 514.39 examples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m [Client 1] Loading model + LoRA (MODEL_ID=distilgpt2)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/8 [00:00<?, ? examples/s]\rMap: 100%|██████████| 8/8 [00:00<00:00, 1100.40 examples/s]\n",
            "\u001b[36m(ClientAppActor pid=928)\u001b[0m \rMap:   0%|          | 0/2 [00:00<?, ? examples/s]\rMap: 100%|██████████| 2/2 [00:00<00:00, 419.68 examples/s]\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 864, in get_objects\n",
            "    raise value.as_instanceof_cause()\n",
            "ray.exceptions.RayTaskError(ClientAppException): \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     \u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 144, in __call__\n",
            "    return self._call(message, context)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/clientapp/client_app.py\", line 128, in ffn\n",
            "    out_message = handle_legacy_message_from_msgtype(\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/message_handler/message_handler.py\", line 135, in handle_legacy_message_from_msgtype\n",
            "    evaluate_res = maybe_call_evaluate(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/client.py\", line 244, in maybe_call_evaluate\n",
            "    return client.evaluate(evaluate_ins)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/client/numpy_client.py\", line 251, in _evaluate\n",
            "    results = self.numpy_client.evaluate(parameters, ins.config)  # type: ignore\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 326, in evaluate\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-654610284.py\", line 229, in eval_loss\n",
            "TypeError: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "\u001b[36mray::ClientAppActor.run()\u001b[39m (pid=928, ip=172.28.0.12, actor_id=bddd8171a197a967648fd37901000000, repr=<flwr.simulation.ray_transport.ray_actor.ClientAppActor object at 0x7eeb77b42ea0>)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 64, in run\n",
            "    raise ClientAppException(str(ex)) from ex\n",
            "flwr.clientapp.client_app.ClientAppException: \n",
            "Exception ClientAppException occurred. Message: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): GPT2LMHeadModel(\n",
            "      (transformer): GPT2Model(\n",
            "        (wte): Embedding(50257, 768)\n",
            "        (wpe): Embedding(1024, 768)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "        (h): ModuleList(\n",
            "          (0-5): 6 x GPT2Block(\n",
            "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (attn): GPT2Attention(\n",
            "              (c_attn): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=2304, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=768)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=768, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): GPT2MLP(\n",
            "              (c_fc): Conv1D(nf=3072, nx=768)\n",
            "              (c_proj): lora.Linear(\n",
            "                (base_layer): Conv1D(nf=768, nx=3072)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=768, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "                (lora_magnitude_vector): ModuleDict()\n",
            "              )\n",
            "              (act): NewGELUActivation()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "    )\n",
            "  )\n",
            ") got multiple values for keyword argument 'labels'\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 3 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 42.23s\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Simulation done.\n",
            "\n",
            "Loading model for generation demo...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generation output ===\n",
            "\n",
            "Summarize this internal note for leadership in 2 bullets:\n",
            "Dispatch note: optimize routes by time windows and driver hours to reduce empty miles.\n",
            "\n",
            "Answer: add 5 extra rounds per second (or 0.5 seconds).\n",
            "Add 4 additional rounds per second (or 0.5 seconds).\n",
            "Change the default value of a single bullet each time, depending on how many rounds you've been using it. The total number will be set if the maximum amount of ammo is within the range of that shot you fired first. You can see the difference when we\n"
          ]
        }
      ]
    }
  ]
}