{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch scikit-learn numpy openai\n",
        "\n",
        "import time, random, json, os, getpass\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
        "from openai import OpenAI\n",
        "\n",
        "SEED = 7\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "id": "P3ZzVaQsZjTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(\n",
        "    n_samples=60000,\n",
        "    n_features=30,\n",
        "    n_informative=18,\n",
        "    n_redundant=8,\n",
        "    weights=[0.985, 0.015],\n",
        "    class_sep=1.5,\n",
        "    flip_y=0.01,\n",
        "    random_state=SEED\n",
        ")\n",
        "\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.int64)\n",
        "\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=SEED\n",
        ")\n",
        "\n",
        "server_scaler = StandardScaler()\n",
        "X_train_full_s = server_scaler.fit_transform(X_train_full).astype(np.float32)\n",
        "X_test_s = server_scaler.transform(X_test).astype(np.float32)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    TensorDataset(torch.from_numpy(X_test_s), torch.from_numpy(y_test)),\n",
        "    batch_size=1024,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "EHD5F0ISZjGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dirichlet_partition(y, n_clients=10, alpha=0.35):\n",
        "    classes = np.unique(y)\n",
        "    idx_by_class = [np.where(y == c)[0] for c in classes]\n",
        "    client_idxs = [[] for _ in range(n_clients)]\n",
        "    for idxs in idx_by_class:\n",
        "        np.random.shuffle(idxs)\n",
        "        props = np.random.dirichlet(alpha * np.ones(n_clients))\n",
        "        cuts = (np.cumsum(props) * len(idxs)).astype(int)\n",
        "        prev = 0\n",
        "        for cid, cut in enumerate(cuts):\n",
        "            client_idxs[cid].extend(idxs[prev:cut].tolist())\n",
        "            prev = cut\n",
        "    return [np.array(ci, dtype=np.int64) for ci in client_idxs]\n",
        "\n",
        "NUM_CLIENTS = 10\n",
        "client_idxs = dirichlet_partition(y_train_full, NUM_CLIENTS, 0.35)\n",
        "\n",
        "def make_client_split(X, y, idxs):\n",
        "    Xi, yi = X[idxs], y[idxs]\n",
        "    if len(np.unique(yi)) < 2:\n",
        "        other = np.where(y == (1 - yi[0]))[0]\n",
        "        add = np.random.choice(other, size=min(10, len(other)), replace=False)\n",
        "        Xi = np.concatenate([Xi, X[add]])\n",
        "        yi = np.concatenate([yi, y[add]])\n",
        "    return train_test_split(Xi, yi, test_size=0.15, stratify=yi, random_state=SEED)\n",
        "\n",
        "client_data = [make_client_split(X_train_full, y_train_full, client_idxs[c]) for c in range(NUM_CLIENTS)]\n",
        "\n",
        "def make_client_loaders(Xtr, ytr, Xva, yva):\n",
        "    sc = StandardScaler()\n",
        "    Xtr_s = sc.fit_transform(Xtr).astype(np.float32)\n",
        "    Xva_s = sc.transform(Xva).astype(np.float32)\n",
        "    tr = DataLoader(TensorDataset(torch.from_numpy(Xtr_s), torch.from_numpy(ytr)), batch_size=512, shuffle=True)\n",
        "    va = DataLoader(TensorDataset(torch.from_numpy(Xva_s), torch.from_numpy(yva)), batch_size=512)\n",
        "    return tr, va\n",
        "\n",
        "client_loaders = [make_client_loaders(*cd) for cd in client_data]"
      ],
      "metadata": {
        "id": "im0JJHoDZjEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FraudNet(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "def get_weights(model):\n",
        "    return [p.detach().cpu().numpy() for p in model.state_dict().values()]\n",
        "\n",
        "def set_weights(model, weights):\n",
        "    keys = list(model.state_dict().keys())\n",
        "    model.load_state_dict({k: torch.tensor(w) for k, w in zip(keys, weights)}, strict=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    ys, ps, losses = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        logits = model(xb)\n",
        "        losses.append(bce(logits, yb.float()).item())\n",
        "        ys.append(yb.numpy())\n",
        "        ps.append(torch.sigmoid(logits).numpy())\n",
        "    y_true = np.concatenate(ys)\n",
        "    y_prob = np.concatenate(ps)\n",
        "    return {\n",
        "        \"loss\": float(np.mean(losses)),\n",
        "        \"auc\": roc_auc_score(y_true, y_prob),\n",
        "        \"ap\": average_precision_score(y_true, y_prob),\n",
        "        \"acc\": accuracy_score(y_true, (y_prob >= 0.5).astype(int))\n",
        "    }\n",
        "\n",
        "def train_local(model, loader, lr):\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    bce = nn.BCEWithLogitsLoss()\n",
        "    model.train()\n",
        "    for xb, yb in loader:\n",
        "        opt.zero_grad()\n",
        "        loss = bce(model(xb), yb.float())\n",
        "        loss.backward()\n",
        "        opt.step()"
      ],
      "metadata": {
        "id": "FXzfgsTYZjCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fedavg(weights, sizes):\n",
        "    total = sum(sizes)\n",
        "    return [\n",
        "        sum(w[i] * (s / total) for w, s in zip(weights, sizes))\n",
        "        for i in range(len(weights[0]))\n",
        "    ]\n",
        "\n",
        "ROUNDS = 10\n",
        "LR = 5e-4\n",
        "\n",
        "global_model = FraudNet(X_train_full.shape[1])\n",
        "global_weights = get_weights(global_model)\n",
        "\n",
        "for r in range(1, ROUNDS + 1):\n",
        "    client_weights, client_sizes = [], []\n",
        "    for cid in range(NUM_CLIENTS):\n",
        "        local = FraudNet(X_train_full.shape[1])\n",
        "        set_weights(local, global_weights)\n",
        "        train_local(local, client_loaders[cid][0], LR)\n",
        "        client_weights.append(get_weights(local))\n",
        "        client_sizes.append(len(client_loaders[cid][0].dataset))\n",
        "    global_weights = fedavg(client_weights, client_sizes)\n",
        "    set_weights(global_model, global_weights)\n",
        "    metrics = evaluate(global_model, test_loader)\n",
        "    print(f\"Round {r}: {metrics}\")"
      ],
      "metadata": {
        "id": "TTtzKN06ZjAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = getpass.getpass(\"Enter OPENAI_API_KEY (input hidden): \").strip()\n",
        "\n",
        "if OPENAI_API_KEY:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "    client = OpenAI()\n",
        "\n",
        "    summary = {\n",
        "        \"rounds\": ROUNDS,\n",
        "        \"num_clients\": NUM_CLIENTS,\n",
        "        \"final_metrics\": metrics,\n",
        "        \"client_sizes\": [len(client_loaders[c][0].dataset) for c in range(NUM_CLIENTS)],\n",
        "        \"client_fraud_rates\": [float(client_data[c][1].mean()) for c in range(NUM_CLIENTS)]\n",
        "    }\n",
        "\n",
        "    prompt = (\n",
        "        \"Write a concise internal fraud-risk report.\\n\"\n",
        "        \"Include executive summary, metric interpretation, risks, and next steps.\\n\\n\"\n",
        "        + json.dumps(summary, indent=2)\n",
        "    )\n",
        "\n",
        "    resp = client.responses.create(model=\"gpt-5.2\", input=prompt)\n",
        "    print(resp.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT81TEWtXpzq",
        "outputId": "4c8ce2f6-b77a-47b8-d2c4-cb36dc404d0f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client sizes: [4157, 3254, 3823, 6567, 6137, 361, 4943, 881, 8794, 1893]\n",
            "Client fraud rates: [0.008900649506855906, 0.002458512599877074, 0.020664399686110384, 0.0012182122734886553, 0.0008147303242626691, 0.2548476454293629, 0.11673073032571313, 0.004540295119182747, 0.00045485558335228563, 0.002641310089804543]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 01] test_loss=0.6095 AUC=0.2353 AP=0.0118 Acc=0.9767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 02] test_loss=0.5224 AUC=0.2456 AP=0.0120 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 03] test_loss=0.4338 AUC=0.2648 AP=0.0123 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 04] test_loss=0.3482 AUC=0.2865 AP=0.0128 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 05] test_loss=0.2712 AUC=0.3121 AP=0.0133 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 06] test_loss=0.2084 AUC=0.3437 AP=0.0142 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 07] test_loss=0.1622 AUC=0.3793 AP=0.0153 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 08] test_loss=0.1321 AUC=0.4171 AP=0.0165 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 09] test_loss=0.1139 AUC=0.4557 AP=0.0180 Acc=0.9802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Round 10] test_loss=0.1044 AUC=0.4962 AP=0.0203 Acc=0.9802\n",
            "Done in 15.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter OPENAI_API_KEY (input hidden): ··········\n",
            "\n",
            "=== OpenAI Risk Report ===\n",
            "\n",
            "1) **Executive summary**\n",
            "- The FedAvg run over 10 banks (10 rounds, 1 local epoch, lr=5e-4) produced **near-random discrimination** on the global test set (**AUC 0.496**, **AP 0.020**), despite high overall accuracy (**0.980**).\n",
            "- Client label distributions are **extremely non-IID**: fraud rates range from **0.045% to 25.5%** (≈560× spread), implying strong gradient conflict and a high risk that a single global model underperforms for many banks.\n",
            "- The reported **0.980 accuracy is not meaningful** in this imbalanced setting; it is consistent with predicting “non-fraud” most of the time.\n",
            "- While FL avoids centralizing raw transaction data, it **does not guarantee perfect privacy**; model updates can still leak information without additional protections.\n",
            "\n",
            "2) **Metrics interpretation (AUC/AP/Acc)**\n",
            "AUC ~0.50 indicates the model is essentially unable to rank fraud above non-fraud on the global test distribution.  \n",
            "AP 0.020 is very low and close to what we’d expect near the base rate, suggesting limited practical lift for investigation queues.  \n",
            "Accuracy 0.980 is driven by class imbalance and likely reflects majority-class prediction rather than true fraud detection capability.  \n",
            "Given the non-IID setup and per-client scaling, these aggregate metrics likely mask large cross-bank variance (some clients may improve while others degrade).\n",
            "\n",
            "3) **Risks & governance**\n",
            "- **Non-IID/representation risk:** Banks with very low fraud prevalence may dominate updates by volume, potentially harming performance for high-fraud or behaviorally distinct banks; requires per-client reporting and fairness/utility guardrails.\n",
            "- **Privacy risk remains:** Gradients/weights can enable inference (e.g., membership/property leakage) without secure aggregation and/or DP; “no raw data shared” ≠ “no sensitive signal shared.”\n",
            "- **Operational model risk:** A single global model may be unfit for heterogeneous portfolios; need governance on when to allow local adaptation vs. centralized aggregation and how to monitor drift per bank.\n",
            "\n",
            "4) **Next steps**\n",
            "- **Modeling:** Add **client-weighting and/or rebalancing** (e.g., loss weighting, focal loss, fraud upsampling) to counter extreme imbalance and reduce majority-class collapse.\n",
            "- **Modeling:** Use **personalization** (FedProx/FedBN, clustered FL, or global + per-bank adapter/calibration) to handle the 0.045%–25.5% fraud-rate skew.\n",
            "- **Modeling:** Fix preprocessing mismatch: move to a **federated/consistent feature scaling strategy** (shared normalization stats via secure aggregation, or server-side evaluation aligned with client transforms).\n",
            "- **Privacy/security:** Deploy **secure aggregation** so the server cannot inspect individual client updates; add controls for dropouts and robustness.\n",
            "- **Privacy/security:** Evaluate **differential privacy (client-level DP)** and run privacy risk assessments (membership/property inference tests) with documented privacy-utility tradeoffs before any production consideration.\n"
          ]
        }
      ]
    }
  ]
}