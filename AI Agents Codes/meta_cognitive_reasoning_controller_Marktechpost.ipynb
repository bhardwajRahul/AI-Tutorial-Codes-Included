{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "OPS = ['+', '*']\n",
        "\n",
        "def make_task():\n",
        "    op = random.choice(OPS)\n",
        "    if op == '+':\n",
        "        a, b = random.randint(1, 99), random.randint(1, 99)\n",
        "    else:\n",
        "        a, b = random.randint(2, 19), random.randint(2, 19)\n",
        "    return a, b, op\n",
        "\n",
        "def true_answer(a, b, op):\n",
        "    return a + b if op == '+' else a * b\n",
        "\n",
        "def true_difficulty(a, b, op):\n",
        "    if op == '+' and a <= 30 and b <= 30:\n",
        "        return 0\n",
        "    if op == '*' and a <= 10 and b <= 10:\n",
        "        return 1\n",
        "    return 2\n",
        "\n",
        "def heuristic_difficulty(a, b, op):\n",
        "    score = 0\n",
        "    if op == '*':\n",
        "        score += 0.6\n",
        "    score += max(a, b) / 100.0\n",
        "    return min(score, 1.0)\n",
        "\n",
        "def fast_heuristic(a, b, op):\n",
        "    if op == '+':\n",
        "        base = a + b\n",
        "        noise = random.choice([-2, -1, 0, 0, 0, 1, 2, 3])\n",
        "    else:\n",
        "        base = int(0.8 * a * b)\n",
        "        noise = random.choice([-5, -3, 0, 0, 2, 5, 8])\n",
        "    return base + noise, 0.5\n",
        "\n",
        "def deep_chain_of_thought(a, b, op, verbose=False):\n",
        "    if op == '+':\n",
        "        x, y = a, b\n",
        "        carry = 0\n",
        "        pos = 1\n",
        "        result = 0\n",
        "        step = 0\n",
        "        while x > 0 or y > 0 or carry:\n",
        "            dx, dy = x % 10, y % 10\n",
        "            s = dx + dy + carry\n",
        "            carry, digit = divmod(s, 10)\n",
        "            result += digit * pos\n",
        "            x //= 10; y //= 10; pos *= 10\n",
        "            step += 1\n",
        "    else:\n",
        "        result = 0\n",
        "        step = 0\n",
        "        for i, d in enumerate(reversed(str(b))):\n",
        "            row = a * int(d) * (10 ** i)\n",
        "            result += row\n",
        "            step += 1\n",
        "    return result, max(2.0, 0.4 * step)\n",
        "\n",
        "def tool_solver(a, b, op):\n",
        "    return eval(f\"{a}{op}{b}\"), 1.2\n",
        "\n",
        "ACTION_NAMES = [\"fast\", \"deep\", \"tool\"]"
      ],
      "metadata": {
        "id": "kvKIli88a2g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_state(a, b, op, rem_budget, error_ema, last_action):\n",
        "    a_n = a / 100.0\n",
        "    b_n = b / 100.0\n",
        "    op_plus = 1.0 if op == '+' else 0.0\n",
        "    op_mul = 1.0 - op_plus\n",
        "    diff_hat = heuristic_difficulty(a, b, op)\n",
        "    rem_n = rem_budget / MAX_BUDGET\n",
        "    last_onehot = [0.0, 0.0, 0.0]\n",
        "    if last_action is not None:\n",
        "        last_onehot[last_action] = 1.0\n",
        "    feats = [\n",
        "        a_n, b_n, op_plus, op_mul,\n",
        "        diff_hat, rem_n, error_ema\n",
        "    ] + last_onehot\n",
        "    return torch.tensor(feats, dtype=torch.float32, device=device)\n",
        "\n",
        "STATE_DIM = 10\n",
        "N_ACTIONS = 3\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_dim, hidden=48, n_actions=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, hidden),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden, n_actions)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "policy = PolicyNet(STATE_DIM, hidden=48, n_actions=N_ACTIONS).to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=3e-3)"
      ],
      "metadata": {
        "id": "d0AX3_QXa2d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GAMMA = 0.98\n",
        "COST_PENALTY = 0.25\n",
        "MAX_BUDGET = 25.0\n",
        "EPISODES = 600\n",
        "STEPS_PER_EP = 20\n",
        "ERROR_EMA_DECAY = 0.9\n",
        "\n",
        "def run_episode(train=True):\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "    info = []\n",
        "    rem_budget = MAX_BUDGET\n",
        "    error_ema = 0.0\n",
        "    last_action = None\n",
        "\n",
        "    for _ in range(STEPS_PER_EP):\n",
        "        a, b, op = make_task()\n",
        "        state = encode_state(a, b, op, rem_budget, error_ema, last_action)\n",
        "        logits = policy(state)\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample() if train else torch.argmax(logits)\n",
        "        act_idx = int(action.item())\n",
        "\n",
        "        if act_idx == 0:\n",
        "            pred, cost = fast_heuristic(a, b, op)\n",
        "        elif act_idx == 1:\n",
        "            pred, cost = deep_chain_of_thought(a, b, op, verbose=False)\n",
        "        else:\n",
        "            pred, cost = tool_solver(a, b, op)\n",
        "\n",
        "        correct = (pred == true_answer(a, b, op))\n",
        "        acc_reward = 1.0 if correct else 0.0\n",
        "        budget_penalty = 0.0\n",
        "\n",
        "        rem_budget -= cost\n",
        "        if rem_budget < 0:\n",
        "            budget_penalty = -1.5 * (abs(rem_budget) / MAX_BUDGET)\n",
        "\n",
        "        step_reward = acc_reward - COST_PENALTY * cost + budget_penalty\n",
        "        rewards.append(step_reward)\n",
        "\n",
        "        if train:\n",
        "            log_probs.append(dist.log_prob(action))\n",
        "\n",
        "        err = 0.0 if correct else 1.0\n",
        "        error_ema = ERROR_EMA_DECAY * error_ema + (1 - ERROR_EMA_DECAY) * err\n",
        "        last_action = act_idx\n",
        "\n",
        "        info.append({\n",
        "            \"correct\": correct,\n",
        "            \"cost\": cost,\n",
        "            \"difficulty\": true_difficulty(a, b, op),\n",
        "            \"action\": act_idx\n",
        "        })\n",
        "\n",
        "    if train:\n",
        "        returns = []\n",
        "        G = 0.0\n",
        "        for r in reversed(rewards):\n",
        "            G = r + GAMMA * G\n",
        "            returns.append(G)\n",
        "        returns = list(reversed(returns))\n",
        "        returns_t = torch.tensor(returns, dtype=torch.float32, device=device)\n",
        "        baseline = returns_t.mean()\n",
        "        adv = returns_t - baseline\n",
        "        loss = -(torch.stack(log_probs) * adv).mean()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return rewards, info"
      ],
      "metadata": {
        "id": "pRTaJpREa2a4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training meta-cognitive controller...\")\n",
        "for ep in range(EPISODES):\n",
        "    rewards, _ = run_episode(train=True)\n",
        "    if (ep + 1) % 100 == 0:\n",
        "        print(f\" episode {ep+1:4d} | avg reward {np.mean(rewards):.3f}\")\n",
        "\n",
        "def evaluate(n_episodes=50):\n",
        "    all_actions = {0: [0,0,0], 1: [0,0,0], 2: [0,0,0]}\n",
        "    stats = {0: {\"n\":0,\"acc\":0,\"cost\":0},\n",
        "             1: {\"n\":0,\"acc\":0,\"cost\":0},\n",
        "             2: {\"n\":0,\"acc\":0,\"cost\":0}}\n",
        "\n",
        "    for _ in range(n_episodes):\n",
        "        _, info = run_episode(train=False)\n",
        "        for step in info:\n",
        "            d = step[\"difficulty\"]\n",
        "            a_idx = step[\"action\"]\n",
        "            all_actions[d][a_idx] += 1\n",
        "            stats[d][\"n\"] += 1\n",
        "            stats[d][\"acc\"] += 1 if step[\"correct\"] else 0\n",
        "            stats[d][\"cost\"] += step[\"cost\"]\n",
        "\n",
        "    for d in [0,1,2]:\n",
        "        if stats[d][\"n\"] == 0:\n",
        "            continue\n",
        "        n = stats[d][\"n\"]\n",
        "        print(f\"Difficulty {d}:\")\n",
        "        print(\" action counts [fast, deep, tool]:\", all_actions[d])\n",
        "        print(\" accuracy:\", stats[d][\"acc\"]/n)\n",
        "        print(\" avg cost:\", stats[d][\"cost\"]/n)\n",
        "        print()\n",
        "\n",
        "print(\"Policy behavior by difficulty:\")\n",
        "evaluate()"
      ],
      "metadata": {
        "id": "8HosRj5Ha2YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IaTZXDbYqYY",
        "outputId": "549981d0-6739-4f0b-c9ef-174211c1fea1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training meta-cognitive controller...\n",
            " episode  100 | avg reward 0.700\n",
            " episode  200 | avg reward 0.700\n",
            " episode  300 | avg reward 0.700\n",
            " episode  400 | avg reward 0.700\n",
            " episode  500 | avg reward 0.700\n",
            " episode  600 | avg reward 0.700\n",
            "\n",
            "Training complete.\n",
            "\n",
            "Policy behavior by difficulty:\n",
            "Difficulty 0 (easy):\n",
            " action counts [fast, deep, tool]: [0, 0, 43]\n",
            " accuracy: 1.0\n",
            " avg cost: 1.2000000000000008\n",
            "\n",
            "Difficulty 1 (medium):\n",
            " action counts [fast, deep, tool]: [0, 0, 127]\n",
            " accuracy: 1.0\n",
            " avg cost: 1.1999999999999997\n",
            "\n",
            "Difficulty 2 (hard):\n",
            " action counts [fast, deep, tool]: [0, 0, 830]\n",
            " accuracy: 1.0\n",
            " avg cost: 1.200000000000018\n",
            "\n",
            "\n",
            "Example hard task with meta-selected thinking mode:\n",
            "Task: 47 * 18\n",
            "Meta-agent chooses mode: tool\n",
            "[Tool solver]\n",
            " result: 846\n",
            "True answer: 846 | cost used: 1.2\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nExample hard task with meta-selected thinking mode:\")\n",
        "a, b, op = 47, 18, '*'\n",
        "state = encode_state(a, b, op, MAX_BUDGET, 0.3, None)\n",
        "with torch.no_grad():\n",
        "    logits = policy(state)\n",
        "    act = int(torch.argmax(logits).item())\n",
        "\n",
        "print(f\"Task: {a} {op} {b}\")\n",
        "print(\"Chosen mode:\", ACTION_NAMES[act])\n",
        "\n",
        "if act == 1:\n",
        "    pred, cost = deep_chain_of_thought(a, b, op, verbose=True)\n",
        "elif act == 0:\n",
        "    pred, cost = fast_heuristic(a, b, op)\n",
        "    print(\"Fast heuristic:\", pred)\n",
        "else:\n",
        "    pred, cost = tool_solver(a, b, op)\n",
        "    print(\"Tool solver:\", pred)\n",
        "\n",
        "print(\"True:\", true_answer(a,b,op), \"| cost:\", cost)"
      ]
    }
  ]
}