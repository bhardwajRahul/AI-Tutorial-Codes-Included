{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math, random, torch, torch.nn as nn, torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"; torch.manual_seed(0); random.seed(0)\n",
        "V = 18; CTX = 10; MUL, ADD, SUB, ANS, STO, RCL, EOS = 11, 12, 13, 14, 15, 16, 17\n",
        "tok2str = {**{i: str(i) for i in range(10)}, CTX:\"[CTX]\", MUL:\"[MUL]\", ADD:\"[ADD]\", SUB:\"[SUB]\", ANS:\"[ANS]\", STO:\"[STO]\", RCL:\"[RCL]\", EOS:\"[EOS]\"}\n",
        "\n",
        "class ToolEnv:\n",
        "    def __init__(self, max_steps=7):\n",
        "        self.max_steps = max_steps\n",
        "    def sample(self, stage):\n",
        "        a,b,c,d,e = [random.randint(0,9) for _ in range(5)]\n",
        "        if stage==0: ctx=[a,b,c]; target=a*b+c\n",
        "        elif stage==1: ctx=[a,b,c,d]; target=(a*b+c)-d\n",
        "        else: ctx=[a,b,c,d,e]; target=(a*b+c)-(d*e)\n",
        "        return ctx, target, (a,b,c,d,e)\n",
        "    def step_seq(self, actions, abc, stage):\n",
        "        a,b,c,d,e = abc; last=None; mem=None; steps=0; shaped=0.0\n",
        "        goal0=a*b; goal1=goal0+c; goal2=goal1-d; goal3=d*e; goal4=goal1-goal3\n",
        "        for act in actions:\n",
        "            steps+=1\n",
        "            if act==MUL: last=(a*b if last is None else last*(d if stage>0 else 1))\n",
        "            elif act==ADD and last is not None: last+=c\n",
        "            elif act==SUB and last is not None:\n",
        "                last -= (e if stage==2 and mem==\"use_d\" else (d if stage>0 else 0))\n",
        "            elif act==STO: mem=\"use_d\" if stage>=1 else \"ok\"\n",
        "            elif act==RCL and mem is not None:\n",
        "                last = (d*e) if (stage==2 and mem==\"use_d\") else (last if last else 0)\n",
        "            elif act==ANS:\n",
        "                target=[goal0,goal1,goal2,goal4][stage] if stage==2 else [goal0,goal1,goal2][stage]\n",
        "                correct=(last==target)\n",
        "                if stage==0: shaped += 0.25*(last==goal0)+0.5*(last==goal1)\n",
        "                if stage==1: shaped += 0.25*(last==goal0)+0.5*(last==goal1)+0.75*(last==goal2)\n",
        "                if stage==2: shaped += 0.2*(last==goal0)+0.4*(last==goal1)+0.6*(last==goal4)+0.6*(last==goal3)\n",
        "                return (1.0 if correct else 0.0)+0.2*shaped, steps\n",
        "            if steps>=self.max_steps: break\n",
        "        return 0.0, steps"
      ],
      "metadata": {
        "id": "ysnL1FVMZ75z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self,V,d=96,nstage=3):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(V,d); self.stage_emb=nn.Embedding(nstage,d)\n",
        "        self.rnn=nn.GRU(d,d,1,batch_first=True); self.pi=nn.Linear(d,V); self.v=nn.Linear(d,1)\n",
        "    def forward(self,ctx,stage,max_len=6,greedy=False):\n",
        "        B=ctx.shape[0]; ce=self.emb(ctx).mean(1)+self.stage_emb(stage).unsqueeze(1)\n",
        "        h=torch.tanh(ce.mean(1)).unsqueeze(0); inp=self.emb(torch.full((B,1),CTX,device=device))\n",
        "        acts,logps,ents,vals=[],[],[],[]\n",
        "        for _ in range(max_len):\n",
        "            out,h=self.rnn(inp,h); val=self.v(out[:,-1]); logits=self.pi(out[:,-1])\n",
        "            pi=F.log_softmax(logits,dim=-1).exp(); ent=-(pi*torch.log(pi+1e-9)).sum(1)\n",
        "            a=torch.argmax(logits,1) if greedy else torch.distributions.Categorical(pi).sample()\n",
        "            logp=F.log_softmax(logits,dim=-1).gather(1,a.unsqueeze(1)).squeeze(1)\n",
        "            inp=self.emb(a.unsqueeze(1))\n",
        "            acts.append(a); logps.append(logp); ents.append(ent); vals.append(val.squeeze(1))\n",
        "        return torch.stack(acts,1), torch.stack(logps,1), torch.stack(ents,1), torch.stack(vals,1)"
      ],
      "metadata": {
        "id": "Se_Q99gEZ727"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env=ToolEnv(); net=ActorCritic(V).to(device)\n",
        "opt=torch.optim.Adam(net.parameters(),lr=3e-4)\n",
        "def pad_batch(ctxs):\n",
        "    L=max(len(c)+1 for c in ctxs)\n",
        "    out=torch.full((len(ctxs),L),EOS,dtype=torch.long,device=device)\n",
        "    for i,c in enumerate(ctxs): out[i,:len(c)+1]=torch.tensor(c+[CTX],device=device)\n",
        "    return out\n",
        "def run_batch(stage,batch=128,train=True,greedy=False):\n",
        "    ctxs=[]; metas=[]\n",
        "    for _ in range(batch):\n",
        "        c,t,abc=env.sample(stage); ctxs.append(c); metas.append((t,abc))\n",
        "    ctx=pad_batch(ctxs); stage_t=torch.full((batch,),stage,device=device,dtype=torch.long)\n",
        "    acts,logps,ents,vals=net(ctx,stage_t,max_len=6,greedy=greedy)\n",
        "    rewards=[]\n",
        "    for i in range(batch):\n",
        "        traj = acts[i].tolist()\n",
        "        abc = metas[i][1]\n",
        "        r,_ = env.step_seq(traj,abc,stage)\n",
        "        rewards.append(r)\n",
        "    R=torch.tensor(rewards,device=device).float()\n",
        "    adv=(R-vals.sum(1)).detach()\n",
        "    if not train: return R.mean().item(), 0.0\n",
        "    pg=-(logps.sum(1)*adv).mean(); vloss=F.mse_loss(vals.sum(1),R); ent=-ents.mean()\n",
        "    loss=pg+0.5*vloss+0.01*ent\n",
        "    opt.zero_grad(); loss.backward(); nn.utils.clip_grad_norm_(net.parameters(),1.0); opt.step()\n",
        "    return R.mean().item(), loss.item()"
      ],
      "metadata": {
        "id": "TqbVSmF-Z70I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training…\")\n",
        "stages=[0,0,0,1,1,2]\n",
        "for ep in range(1,61):\n",
        "    stage=stages[min((ep-1)//10,len(stages)-1)]\n",
        "    acc,loss=run_batch(stage,batch=192,train=True)\n",
        "    if ep%5==0:\n",
        "        with torch.no_grad():\n",
        "            evals=[run_batch(s,train=False,greedy=True)[0] for s in [0,1,2]]\n",
        "        print(f\"ep={ep:02d} stage={stage} acc={acc:.3f} | eval T0={evals[0]:.3f} \"\n",
        "              f\"T1={evals[1]:.3f} T2={evals[2]:.3f} loss={loss:.3f}\")"
      ],
      "metadata": {
        "id": "-Ky8iH7RZ7x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZimxZ3BWsBkX",
        "outputId": "39387a5b-1488-42be-dcbc-163c3807262a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training…\n",
            "ep=05 stage=0 acc=0.055 | eval T0=0.000 T1=0.000 T2=0.000 loss=1.562\n",
            "ep=10 stage=0 acc=0.067 | eval T0=0.000 T1=0.000 T2=0.000 loss=-0.110\n",
            "ep=15 stage=0 acc=0.067 | eval T0=1.057 T1=0.160 T2=0.072 loss=-0.346\n",
            "ep=20 stage=0 acc=0.105 | eval T0=1.057 T1=0.207 T2=0.149 loss=0.257\n",
            "ep=25 stage=0 acc=0.101 | eval T0=1.063 T1=0.154 T2=0.216 loss=-0.895\n",
            "ep=30 stage=0 acc=0.078 | eval T0=1.059 T1=0.171 T2=0.135 loss=0.389\n",
            "ep=35 stage=1 acc=0.015 | eval T0=1.058 T1=0.121 T2=0.167 loss=0.052\n",
            "ep=40 stage=1 acc=0.015 | eval T0=1.059 T1=0.000 T2=0.149 loss=1.098\n",
            "ep=45 stage=1 acc=0.022 | eval T0=1.060 T1=0.000 T2=0.168 loss=-1.325\n",
            "ep=50 stage=1 acc=0.011 | eval T0=1.062 T1=0.000 T2=0.129 loss=1.317\n",
            "ep=55 stage=2 acc=0.022 | eval T0=1.058 T1=0.000 T2=0.178 loss=-9.004\n",
            "ep=60 stage=2 acc=0.020 | eval T0=1.059 T1=0.000 T2=0.132 loss=3.918\n",
            "\n",
            "Stage 0 samples:\n",
            "{'stage': 0, 'ctx': [4, 2, 7], 'target': 15, 'actions': '[MUL] [ANS] [MUL] [ANS] [CTX] [CTX]', 'reward': 1.05}\n",
            "{'stage': 0, 'ctx': [3, 2, 1], 'target': 7, 'actions': '[MUL] [ANS] [CTX] [CTX] [CTX] [CTX]', 'reward': 1.05}\n",
            "{'stage': 0, 'ctx': [5, 5, 6], 'target': 31, 'actions': '[MUL] [ANS] [MUL] [ANS] [MUL] [ANS]', 'reward': 1.05}\n",
            "{'stage': 0, 'ctx': [4, 0, 1], 'target': 1, 'actions': '[MUL] [ANS] [MUL] [ANS] [CTX] [CTX]', 'reward': 1.05}\n",
            "{'stage': 0, 'ctx': [2, 1, 5], 'target': 7, 'actions': '[MUL] [ANS] [MUL] [ANS] [MUL] [ANS]', 'reward': 1.05}\n",
            "\n",
            "Stage 1 samples:\n",
            "{'stage': 1, 'ctx': [5, 5, 7, 9], 'target': 23, 'actions': '2 [CTX] [CTX] [CTX] [CTX] [CTX]', 'reward': 0.0}\n",
            "{'stage': 1, 'ctx': [2, 9, 3, 6], 'target': 15, 'actions': '6 7 [CTX] [CTX] [CTX] [CTX]', 'reward': 0.0}\n",
            "{'stage': 1, 'ctx': [3, 6, 0, 5], 'target': 13, 'actions': '1 [STO] 3 [CTX] [CTX] [CTX]', 'reward': 0.0}\n",
            "{'stage': 1, 'ctx': [4, 8, 7, 2], 'target': 37, 'actions': '2 [CTX] [CTX] [CTX] [CTX] [CTX]', 'reward': 0.0}\n",
            "{'stage': 1, 'ctx': [0, 6, 1, 1], 'target': 0, 'actions': '2 [CTX] [CTX] [CTX] [CTX] [CTX]', 'reward': 0.0}\n",
            "\n",
            "Stage 2 samples:\n",
            "{'stage': 2, 'ctx': [5, 6, 6, 3, 7], 'target': 15, 'actions': '[MUL] [ANS] [MUL] [ANS] [CTX] [CTX]', 'reward': 0.04}\n",
            "{'stage': 2, 'ctx': [3, 2, 0, 1, 0], 'target': 6, 'actions': '4 [ADD] 4 [ADD] 4 [ADD]', 'reward': 0.0}\n",
            "{'stage': 2, 'ctx': [5, 2, 6, 3, 7], 'target': -5, 'actions': '[MUL] [ANS] [MUL] [ANS] [CTX] [CTX]', 'reward': 0.04}\n",
            "{'stage': 2, 'ctx': [7, 5, 5, 2, 1], 'target': 38, 'actions': '[MUL] [ANS] [MUL] [ANS] [MUL] [ANS]', 'reward': 0.04}\n",
            "{'stage': 2, 'ctx': [9, 8, 5, 8, 7], 'target': 21, 'actions': '[MUL] [ANS] [MUL] [ANS] [MUL] [ANS]', 'reward': 0.04}\n",
            "\n",
            "Final greedy accuracies → T0=1.061, T1=0.000, T2=0.176\n"
          ]
        }
      ],
      "source": [
        "def explain(stage):\n",
        "    c,t,abc=env.sample(stage)\n",
        "    ctx=pad_batch([c]); stage_t=torch.tensor([stage],device=device)\n",
        "    with torch.no_grad(): a,_,_,_=net(ctx,stage_t,greedy=True)\n",
        "    seq=[tok2str[x] for x in a[0].tolist()]\n",
        "    r,_=env.step_seq(a[0].tolist(),abc,stage)\n",
        "    return dict(stage=stage,ctx=c,target=t,actions=\" \".join(seq),reward=round(float(r),2))\n",
        "with torch.no_grad():\n",
        "    for s in [0,1,2]:\n",
        "        print(f\"\\nStage {s} samples:\")\n",
        "        for _ in range(5): print(explain(s))\n",
        "with torch.no_grad():\n",
        "    finals=[run_batch(s,train=False,greedy=True,batch=1000)[0] for s in [0,1,2]]\n",
        "print(f\"\\nFinal greedy accuracies → T0={finals[0]:.3f}, T1={finals[1]:.3f}, T2={finals[2]:.3f}\")"
      ]
    }
  ]
}