{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"hydra-core\"])\n",
        "\n",
        "import hydra\n",
        "from hydra import compose, initialize_config_dir\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "import os\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "gQ5yaJRoif6f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class OptimizerConfig:\n",
        "    _target_: str = \"torch.optim.SGD\"\n",
        "    lr: float = 0.01\n",
        "\n",
        "@dataclass\n",
        "class AdamConfig(OptimizerConfig):\n",
        "    _target_: str = \"torch.optim.Adam\"\n",
        "    lr: float = 0.001\n",
        "    betas: tuple = (0.9, 0.999)\n",
        "    weight_decay: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class SGDConfig(OptimizerConfig):\n",
        "    _target_: str = \"torch.optim.SGD\"\n",
        "    lr: float = 0.01\n",
        "    momentum: float = 0.9\n",
        "    nesterov: bool = True\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    name: str = \"resnet\"\n",
        "    num_layers: int = 50\n",
        "    hidden_dim: int = 512\n",
        "    dropout: float = 0.1\n",
        "\n",
        "@dataclass\n",
        "class DataConfig:\n",
        "    dataset: str = \"cifar10\"\n",
        "    batch_size: int = 32\n",
        "    num_workers: int = 4\n",
        "    augmentation: bool = True\n",
        "\n",
        "@dataclass\n",
        "class TrainingConfig:\n",
        "    model: ModelConfig = field(default_factory=ModelConfig)\n",
        "    data: DataConfig = field(default_factory=DataConfig)\n",
        "    optimizer: OptimizerConfig = field(default_factory=AdamConfig)\n",
        "    epochs: int = 100\n",
        "    seed: int = 42\n",
        "    device: str = \"cuda\"\n",
        "    experiment_name: str = \"exp_001\""
      ],
      "metadata": {
        "id": "Pu2GFr31ihIy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_config_dir():\n",
        "    config_dir = Path(\"./hydra_configs\")\n",
        "    config_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    main_config = \"\"\"\n",
        "defaults:\n",
        "  - model: resnet\n",
        "  - data: cifar10\n",
        "  - optimizer: adam\n",
        "  - _self_\n",
        "\n",
        "epochs: 100\n",
        "seed: 42\n",
        "device: cuda\n",
        "experiment_name: exp_001\n",
        "\"\"\"\n",
        "    (config_dir / \"config.yaml\").write_text(main_config)\n",
        "\n",
        "    model_dir = config_dir / \"model\"\n",
        "    model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    (model_dir / \"resnet.yaml\").write_text(\"\"\"\n",
        "name: resnet\n",
        "num_layers: 50\n",
        "hidden_dim: 512\n",
        "dropout: 0.1\n",
        "\"\"\")\n",
        "\n",
        "    (model_dir / \"vit.yaml\").write_text(\"\"\"\n",
        "name: vision_transformer\n",
        "num_layers: 12\n",
        "hidden_dim: 768\n",
        "dropout: 0.1\n",
        "patch_size: 16\n",
        "\"\"\")\n",
        "\n",
        "    data_dir = config_dir / \"data\"\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    (data_dir / \"cifar10.yaml\").write_text(\"\"\"\n",
        "dataset: cifar10\n",
        "batch_size: 32\n",
        "num_workers: 4\n",
        "augmentation: true\n",
        "\"\"\")\n",
        "\n",
        "    (data_dir / \"imagenet.yaml\").write_text(\"\"\"\n",
        "dataset: imagenet\n",
        "batch_size: 128\n",
        "num_workers: 8\n",
        "augmentation: true\n",
        "\"\"\")\n",
        "\n",
        "    opt_dir = config_dir / \"optimizer\"\n",
        "    opt_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    (opt_dir / \"adam.yaml\").write_text(\"\"\"\n",
        "_target_: torch.optim.Adam\n",
        "lr: 0.001\n",
        "betas: [0.9, 0.999]\n",
        "weight_decay: 0.0\n",
        "\"\"\")\n",
        "\n",
        "    (opt_dir / \"sgd.yaml\").write_text(\"\"\"\n",
        "_target_: torch.optim.SGD\n",
        "lr: 0.01\n",
        "momentum: 0.9\n",
        "nesterov: true\n",
        "\"\"\")\n",
        "\n",
        "    return str(config_dir.absolute())"
      ],
      "metadata": {
        "id": "w7x-qASBihGL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@hydra.main(version_base=None, config_path=\"hydra_configs\", config_name=\"config\")\n",
        "def train(cfg: DictConfig) -> float:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CONFIGURATION\")\n",
        "    print(\"=\" * 80)\n",
        "    print(OmegaConf.to_yaml(cfg))\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"ACCESSING CONFIGURATION VALUES\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Model: {cfg.model.name}\")\n",
        "    print(f\"Dataset: {cfg.data.dataset}\")\n",
        "    print(f\"Batch Size: {cfg.data.batch_size}\")\n",
        "    print(f\"Optimizer LR: {cfg.optimizer.lr}\")\n",
        "    print(f\"Epochs: {cfg.epochs}\")\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(min(cfg.epochs, 3)):\n",
        "        acc = 0.5 + (epoch * 0.1) + (cfg.optimizer.lr * 10)\n",
        "        best_acc = max(best_acc, acc)\n",
        "        print(f\"Epoch {epoch+1}/{cfg.epochs}: Accuracy = {acc:.4f}\")\n",
        "\n",
        "    return best_acc"
      ],
      "metadata": {
        "id": "2ACXIjY3ihDU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def demo_basic_usage():\n",
        "    print(\"\\n\" + \"ðŸš€ DEMO 1: Basic Configuration\\n\")\n",
        "    config_dir = setup_config_dir()\n",
        "    with initialize_config_dir(version_base=None, config_dir=config_dir):\n",
        "        cfg = compose(config_name=\"config\")\n",
        "        print(OmegaConf.to_yaml(cfg))\n",
        "\n",
        "def demo_config_override():\n",
        "    print(\"\\n\" + \"ðŸš€ DEMO 2: Configuration Overrides\\n\")\n",
        "    config_dir = setup_config_dir()\n",
        "    with initialize_config_dir(version_base=None, config_dir=config_dir):\n",
        "        cfg = compose(\n",
        "            config_name=\"config\",\n",
        "            overrides=[\n",
        "                \"model=vit\",\n",
        "                \"data=imagenet\",\n",
        "                \"optimizer=sgd\",\n",
        "                \"optimizer.lr=0.1\",\n",
        "                \"epochs=50\"\n",
        "            ]\n",
        "        )\n",
        "        print(OmegaConf.to_yaml(cfg))\n",
        "\n",
        "def demo_structured_config():\n",
        "    print(\"\\n\" + \"ðŸš€ DEMO 3: Structured Config Validation\\n\")\n",
        "    from hydra.core.config_store import ConfigStore\n",
        "    cs = ConfigStore.instance()\n",
        "    cs.store(name=\"training_config\", node=TrainingConfig)\n",
        "    with initialize_config_dir(version_base=None, config_dir=setup_config_dir()):\n",
        "        cfg = compose(config_name=\"config\")\n",
        "        print(f\"Config type: {type(cfg)}\")\n",
        "        print(f\"Epochs (validated as int): {cfg.epochs}\")\n",
        "\n",
        "def demo_multirun_simulation():\n",
        "    print(\"\\n\" + \"ðŸš€ DEMO 4: Multirun Simulation\\n\")\n",
        "    config_dir = setup_config_dir()\n",
        "    experiments = [\n",
        "        [\"model=resnet\", \"optimizer=adam\", \"optimizer.lr=0.001\"],\n",
        "        [\"model=resnet\", \"optimizer=sgd\", \"optimizer.lr=0.01\"],\n",
        "        [\"model=vit\", \"optimizer=adam\", \"optimizer.lr=0.0001\"],\n",
        "    ]\n",
        "    results = {}\n",
        "    for i, overrides in enumerate(experiments):\n",
        "        print(f\"\\n--- Experiment {i+1} ---\")\n",
        "        with initialize_config_dir(version_base=None, config_dir=config_dir):\n",
        "            cfg = compose(config_name=\"config\", overrides=overrides)\n",
        "            print(f\"Model: {cfg.model.name}, Optimizer: {cfg.optimizer._target_}\")\n",
        "            print(f\"Learning Rate: {cfg.optimizer.lr}\")\n",
        "            results[f\"exp_{i+1}\"] = cfg\n",
        "    return results\n",
        "\n",
        "def demo_interpolation():\n",
        "    print(\"\\n\" + \"ðŸš€ DEMO 5: Variable Interpolation\\n\")\n",
        "    cfg = OmegaConf.create({\n",
        "        \"model\": {\"name\": \"resnet\", \"layers\": 50},\n",
        "        \"experiment\": \"${model.name}_${model.layers}\",\n",
        "        \"output_dir\": \"/outputs/${experiment}\",\n",
        "        \"checkpoint\": \"${output_dir}/best.ckpt\"\n",
        "    })\n",
        "    print(OmegaConf.to_yaml(cfg))\n",
        "    print(f\"\\nResolved experiment name: {cfg.experiment}\")\n",
        "    print(f\"Resolved checkpoint path: {cfg.checkpoint}\")"
      ],
      "metadata": {
        "id": "fIeyU393ihAM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    demo_basic_usage()\n",
        "    demo_config_override()\n",
        "    demo_structured_config()\n",
        "    demo_multirun_simulation()\n",
        "    demo_interpolation()\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Tutorial complete! Key takeaways:\")\n",
        "    print(\"âœ“ Config composition with defaults\")\n",
        "    print(\"âœ“ Runtime overrides via command line\")\n",
        "    print(\"âœ“ Structured configs with type safety\")\n",
        "    print(\"âœ“ Multirun for hyperparameter sweeps\")\n",
        "    print(\"âœ“ Variable interpolation\")\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQbIH5F5ig9Z",
        "outputId": "feac54ef-9095-4052-c67f-46a23d35ee41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ DEMO 1: Basic Configuration\n",
            "\n",
            "model:\n",
            "  name: resnet\n",
            "  num_layers: 50\n",
            "  hidden_dim: 512\n",
            "  dropout: 0.1\n",
            "data:\n",
            "  dataset: cifar10\n",
            "  batch_size: 32\n",
            "  num_workers: 4\n",
            "  augmentation: true\n",
            "optimizer:\n",
            "  _target_: torch.optim.Adam\n",
            "  lr: 0.001\n",
            "  betas:\n",
            "  - 0.9\n",
            "  - 0.999\n",
            "  weight_decay: 0.0\n",
            "epochs: 100\n",
            "seed: 42\n",
            "device: cuda\n",
            "experiment_name: exp_001\n",
            "\n",
            "\n",
            "ðŸš€ DEMO 2: Configuration Overrides\n",
            "\n",
            "model:\n",
            "  name: vision_transformer\n",
            "  num_layers: 12\n",
            "  hidden_dim: 768\n",
            "  dropout: 0.1\n",
            "  patch_size: 16\n",
            "data:\n",
            "  dataset: imagenet\n",
            "  batch_size: 128\n",
            "  num_workers: 8\n",
            "  augmentation: true\n",
            "optimizer:\n",
            "  _target_: torch.optim.SGD\n",
            "  lr: 0.1\n",
            "  momentum: 0.9\n",
            "  nesterov: true\n",
            "epochs: 50\n",
            "seed: 42\n",
            "device: cuda\n",
            "experiment_name: exp_001\n",
            "\n",
            "\n",
            "ðŸš€ DEMO 3: Structured Config Validation\n",
            "\n",
            "Config type: <class 'omegaconf.dictconfig.DictConfig'>\n",
            "Epochs (validated as int): 100\n",
            "\n",
            "ðŸš€ DEMO 4: Multirun Simulation\n",
            "\n",
            "\n",
            "--- Experiment 1 ---\n",
            "Model: resnet, Optimizer: torch.optim.Adam\n",
            "Learning Rate: 0.001\n",
            "\n",
            "--- Experiment 2 ---\n",
            "Model: resnet, Optimizer: torch.optim.SGD\n",
            "Learning Rate: 0.01\n",
            "\n",
            "--- Experiment 3 ---\n",
            "Model: vision_transformer, Optimizer: torch.optim.Adam\n",
            "Learning Rate: 0.0001\n",
            "\n",
            "ðŸš€ DEMO 5: Variable Interpolation\n",
            "\n",
            "model:\n",
            "  name: resnet\n",
            "  layers: 50\n",
            "experiment: ${model.name}_${model.layers}\n",
            "output_dir: /outputs/${experiment}\n",
            "checkpoint: ${output_dir}/best.ckpt\n",
            "\n",
            "\n",
            "Resolved experiment name: resnet_50\n",
            "Resolved checkpoint path: /outputs/resnet_50/best.ckpt\n",
            "\n",
            "================================================================================\n",
            "Tutorial complete! Key takeaways:\n",
            "âœ“ Config composition with defaults\n",
            "âœ“ Runtime overrides via command line\n",
            "âœ“ Structured configs with type safety\n",
            "âœ“ Multirun for hyperparameter sweeps\n",
            "âœ“ Variable interpolation\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}