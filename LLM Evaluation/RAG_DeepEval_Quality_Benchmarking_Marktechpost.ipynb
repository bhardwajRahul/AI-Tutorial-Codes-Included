{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5ba0665c8534d76a9ecc673ae33f1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d69d1c66137e4dd88cc9e9068f15d952",
              "IPY_MODEL_ad89fba15d464532a7a35850113e1ca2",
              "IPY_MODEL_a35c389765ea4a4aaa5abbe8bd2ca14c"
            ],
            "layout": "IPY_MODEL_6860c9547e1944d5a1823e4ef8f8e0ea"
          }
        },
        "d69d1c66137e4dd88cc9e9068f15d952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0b761a1f72c410dba90926569523d7b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9757cb1964a1442e905302ae7f9462ca",
            "value": "100%"
          }
        },
        "ad89fba15d464532a7a35850113e1ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc4d877a3a4d43078da9b2612bd86872",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff5f610d73c44a8c8a1c2c30bf1f2bfb",
            "value": 5
          }
        },
        "a35c389765ea4a4aaa5abbe8bd2ca14c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6549818e57a4bbdb7468943f5c2276e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_125d5748dd7a4087ad3304c2d611c254",
            "value": "‚Äá5/5‚Äá[00:05&lt;00:00,‚Äá‚Äá1.25s/it]"
          }
        },
        "6860c9547e1944d5a1823e4ef8f8e0ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0b761a1f72c410dba90926569523d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9757cb1964a1442e905302ae7f9462ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc4d877a3a4d43078da9b2612bd86872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5f610d73c44a8c8a1c2c30bf1f2bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6549818e57a4bbdb7468943f5c2276e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "125d5748dd7a4087ad3304c2d611c254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "217c45ff011d4903bc9a68ca8947f337": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_88d709671ada46028ff096fd7b820502",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Evaluating 5 test case(s) in parallel \u001b[38;2;17;255;0m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[38;2;0;229;255m 80%\u001b[0m \u001b[38;2;87;3;255m0:01:39\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Evaluating 5 test case(s) in parallel <span style=\"color: #11ff00; text-decoration-color: #11ff00\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">‚ï∫‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span> <span style=\"color: #00e5ff; text-decoration-color: #00e5ff\"> 80%</span> <span style=\"color: #5703ff; text-decoration-color: #5703ff\">0:01:39</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "88d709671ada46028ff096fd7b820502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys, os, textwrap, json, math, re\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"üîß Hardening environment (prevents common Colab/py3.12 numpy corruption)...\")\n",
        "\n",
        "!pip -q uninstall -y numpy || true\n",
        "!pip -q install --no-cache-dir --force-reinstall \"numpy==1.26.4\"\n",
        "\n",
        "!pip -q install -U deepeval openai scikit-learn pandas tqdm\n",
        "\n",
        "print(\"‚úÖ Packages installed.\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from deepeval import evaluate\n",
        "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
        "from deepeval.metrics import (\n",
        "    AnswerRelevancyMetric,\n",
        "    FaithfulnessMetric,\n",
        "    ContextualRelevancyMetric,\n",
        "    ContextualPrecisionMetric,\n",
        "    ContextualRecallMetric,\n",
        "    GEval,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Imports loaded successfully.\")\n",
        "\n",
        "\n",
        "OPENAI_API_KEY = getpass(\"üîë Enter OPENAI_API_KEY (leave empty to run without OpenAI): \").strip()\n",
        "openai_enabled = bool(OPENAI_API_KEY)\n",
        "\n",
        "if openai_enabled:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "print(f\"üîå OpenAI enabled: {openai_enabled}\")"
      ],
      "metadata": {
        "id": "l2Imf5_nwt3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DOCS = [\n",
        "    {\n",
        "        \"id\": \"doc_01\",\n",
        "        \"title\": \"DeepEval Overview\",\n",
        "        \"text\": (\n",
        "            \"DeepEval is an open-source LLM evaluation framework for unit testing LLM apps. \"\n",
        "            \"It supports LLM-as-a-judge metrics, custom metrics like G-Eval, and RAG metrics \"\n",
        "            \"such as contextual precision and faithfulness.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_02\",\n",
        "        \"title\": \"RAG Evaluation: Why Faithfulness Matters\",\n",
        "        \"text\": (\n",
        "            \"Faithfulness checks whether the answer is supported by retrieved context. \"\n",
        "            \"In RAG, hallucinations occur when the model states claims not grounded in context.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_03\",\n",
        "        \"title\": \"Contextual Precision\",\n",
        "        \"text\": (\n",
        "            \"Contextual precision evaluates how well retrieved chunks are ranked by relevance \"\n",
        "            \"to a query. High precision means relevant chunks appear earlier in the ranked list.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_04\",\n",
        "        \"title\": \"Contextual Recall\",\n",
        "        \"text\": (\n",
        "            \"Contextual recall measures whether the retriever returns enough relevant context \"\n",
        "            \"to answer the query. Low recall means key information was missed in retrieval.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_05\",\n",
        "        \"title\": \"Answer Relevancy\",\n",
        "        \"text\": (\n",
        "            \"Answer relevancy measures whether the generated answer addresses the user's query. \"\n",
        "            \"Even grounded answers can be irrelevant if they don't respond to the question.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_06\",\n",
        "        \"title\": \"G-Eval (GEval) Custom Rubrics\",\n",
        "        \"text\": (\n",
        "            \"G-Eval lets you define evaluation criteria in natural language. \"\n",
        "            \"It uses an LLM judge to score outputs against your rubric (e.g., correctness, tone, policy).\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_07\",\n",
        "        \"title\": \"What a DeepEval Test Case Contains\",\n",
        "        \"text\": (\n",
        "            \"A test case typically includes input (query), actual_output (model answer), \"\n",
        "            \"expected_output (gold answer), and retrieval_context (ranked retrieved passages) for RAG.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"doc_08\",\n",
        "        \"title\": \"Common Pitfall: Missing expected_output\",\n",
        "        \"text\": (\n",
        "            \"Some RAG metrics require expected_output in addition to input and retrieval_context. \"\n",
        "            \"If expected_output is None, evaluation fails for metrics like contextual precision/recall.\"\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "EVAL_QUERIES = [\n",
        "    {\n",
        "        \"query\": \"What is DeepEval used for?\",\n",
        "        \"expected\": \"DeepEval is used to evaluate and unit test LLM applications using metrics like LLM-as-a-judge, G-Eval, and RAG metrics.\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What does faithfulness measure in a RAG system?\",\n",
        "        \"expected\": \"Faithfulness measures whether the generated answer is supported by the retrieved context and avoids hallucinations not grounded in that context.\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What does contextual precision mean?\",\n",
        "        \"expected\": \"Contextual precision evaluates whether relevant retrieved chunks are ranked higher than irrelevant ones for a given query.\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What does contextual recall mean in retrieval?\",\n",
        "        \"expected\": \"Contextual recall measures whether the retriever returns enough relevant context to answer the query, capturing key missing information issues.\",\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Why might an answer be relevant but still low quality in RAG?\",\n",
        "        \"expected\": \"An answer can address the question (relevant) but still be low quality if it is not grounded in retrieved context or misses important details.\",\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "m2N8MB7vwzIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfRetriever:\n",
        "    def __init__(self, docs):\n",
        "        self.docs = docs\n",
        "        self.texts = [f\"{d['title']}\\n{d['text']}\" for d in docs]\n",
        "        self.vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
        "        self.matrix = self.vectorizer.fit_transform(self.texts)\n",
        "\n",
        "    def retrieve(self, query, k=4):\n",
        "        qv = self.vectorizer.transform([query])\n",
        "        sims = cosine_similarity(qv, self.matrix).flatten()\n",
        "        top_idx = np.argsort(-sims)[:k]\n",
        "        results = []\n",
        "        for i in top_idx:\n",
        "            results.append(\n",
        "                {\n",
        "                    \"id\": self.docs[i][\"id\"],\n",
        "                    \"score\": float(sims[i]),\n",
        "                    \"text\": self.texts[i],\n",
        "                }\n",
        "            )\n",
        "        return results\n",
        "\n",
        "retriever = TfidfRetriever(DOCS)"
      ],
      "metadata": {
        "id": "R9oY5Ypbw3DY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extractive_baseline_answer(query, retrieved_contexts):\n",
        "    \"\"\"\n",
        "    Offline fallback: we create a short answer by extracting the most relevant sentences.\n",
        "    This keeps the notebook runnable even without OpenAI.\n",
        "    \"\"\"\n",
        "    joined = \"\\n\".join(retrieved_contexts)\n",
        "    sents = re.split(r\"(?<=[.!?])\\s+\", joined)\n",
        "    keywords = [w.lower() for w in re.findall(r\"[a-zA-Z]{4,}\", query)]\n",
        "    scored = []\n",
        "    for s in sents:\n",
        "        s_l = s.lower()\n",
        "        score = sum(1 for k in keywords if k in s_l)\n",
        "        if len(s.strip()) > 20:\n",
        "            scored.append((score, s.strip()))\n",
        "    scored.sort(key=lambda x: (-x[0], -len(x[1])))\n",
        "    best = [s for sc, s in scored[:3] if sc > 0]\n",
        "    if not best:\n",
        "        best = [s.strip() for s in sents[:2] if len(s.strip()) > 20]\n",
        "    ans = \" \".join(best).strip()\n",
        "    if not ans:\n",
        "        ans = \"I could not find enough context to answer confidently.\"\n",
        "    return ans\n",
        "\n",
        "def openai_answer(query, retrieved_contexts, model=\"gpt-4.1-mini\"):\n",
        "    \"\"\"\n",
        "    Simple RAG prompt for demonstration. DeepEval metrics can still evaluate even if\n",
        "    your generation prompt differs; the key is we store retrieval_context separately.\n",
        "    \"\"\"\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI()\n",
        "\n",
        "    context_block = \"\\n\\n\".join([f\"[CTX {i+1}]\\n{c}\" for i, c in enumerate(retrieved_contexts)])\n",
        "    prompt = f\"\"\"You are a concise technical assistant.\n",
        "Use ONLY the provided context to answer the query. If the answer is not in context, say you don't know.\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\n",
        "Context:\n",
        "{context_block}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "def rag_answer(query, retrieved_contexts):\n",
        "    if openai_enabled:\n",
        "        try:\n",
        "            return openai_answer(query, retrieved_contexts)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è OpenAI generation failed, falling back to extractive baseline. Error: {e}\")\n",
        "            return extractive_baseline_answer(query, retrieved_contexts)\n",
        "    else:\n",
        "        return extractive_baseline_answer(query, retrieved_contexts)"
      ],
      "metadata": {
        "id": "uQIifPqdw7-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüöÄ Running RAG to create test cases...\")\n",
        "\n",
        "test_cases = []\n",
        "K = 4\n",
        "\n",
        "for item in tqdm(EVAL_QUERIES):\n",
        "    q = item[\"query\"]\n",
        "    expected = item[\"expected\"]\n",
        "\n",
        "    retrieved = retriever.retrieve(q, k=K)\n",
        "    retrieval_context = [r[\"text\"] for r in retrieved]\n",
        "\n",
        "    actual = rag_answer(q, retrieval_context)\n",
        "\n",
        "    tc = LLMTestCase(\n",
        "        input=q,\n",
        "        actual_output=actual,\n",
        "        expected_output=expected,\n",
        "        retrieval_context=retrieval_context,\n",
        "    )\n",
        "    test_cases.append(tc)\n",
        "\n",
        "print(f\"‚úÖ Built {len(test_cases)} LLMTestCase objects.\")\n",
        "\n",
        "print(\"\\n‚úÖ Metrics configured.\")\n",
        "\n",
        "metrics = [\n",
        "    AnswerRelevancyMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n",
        "    FaithfulnessMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n",
        "    ContextualRelevancyMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n",
        "    ContextualPrecisionMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n",
        "    ContextualRecallMetric(threshold=0.5, model=\"gpt-4.1\", include_reason=True, async_mode=True),\n",
        "\n",
        "    GEval(\n",
        "        name=\"RAG Correctness Rubric (GEval)\",\n",
        "        criteria=(\n",
        "            \"Score the answer for correctness and usefulness. \"\n",
        "            \"The answer must directly address the query, must not invent facts not supported by context, \"\n",
        "            \"and should be concise but complete.\"\n",
        "        ),\n",
        "        evaluation_params=[\n",
        "            LLMTestCaseParams.INPUT,\n",
        "            LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "            LLMTestCaseParams.EXPECTED_OUTPUT,\n",
        "            LLMTestCaseParams.RETRIEVAL_CONTEXT,\n",
        "        ],\n",
        "        model=\"gpt-4.1\",\n",
        "        threshold=0.5,\n",
        "        async_mode=True,\n",
        "    ),\n",
        "]\n",
        "\n",
        "if not openai_enabled:\n",
        "    print(\"\\n‚ö†Ô∏è You did NOT provide an OpenAI API key.\")\n",
        "    print(\"DeepEval's LLM-as-a-judge metrics (AnswerRelevancy/Faithfulness/Contextual* and GEval) require an LLM judge.\")\n",
        "    print(\"Re-run this cell and provide OPENAI_API_KEY to run DeepEval metrics.\")\n",
        "    print(\"\\n‚úÖ However, your RAG pipeline + test case construction succeeded end-to-end.\")\n",
        "    rows = []\n",
        "    for i, tc in enumerate(test_cases):\n",
        "        rows.append({\n",
        "            \"id\": i,\n",
        "            \"query\": tc.input,\n",
        "            \"actual_output\": tc.actual_output[:220] + (\"...\" if len(tc.actual_output) > 220 else \"\"),\n",
        "            \"expected_output\": tc.expected_output[:220] + (\"...\" if len(tc.expected_output) > 220 else \"\"),\n",
        "            \"contexts\": len(tc.retrieval_context or []),\n",
        "        })\n",
        "    display(pd.DataFrame(rows))\n",
        "    raise SystemExit(\"Stopped before evaluation (no OpenAI key).\")"
      ],
      "metadata": {
        "id": "nsXx3tx5w8y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UwiK-aRfoFqU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a5ba0665c8534d76a9ecc673ae33f1b8",
            "d69d1c66137e4dd88cc9e9068f15d952",
            "ad89fba15d464532a7a35850113e1ca2",
            "a35c389765ea4a4aaa5abbe8bd2ca14c",
            "6860c9547e1944d5a1823e4ef8f8e0ea",
            "d0b761a1f72c410dba90926569523d7b",
            "9757cb1964a1442e905302ae7f9462ca",
            "bc4d877a3a4d43078da9b2612bd86872",
            "ff5f610d73c44a8c8a1c2c30bf1f2bfb",
            "a6549818e57a4bbdb7468943f5c2276e",
            "125d5748dd7a4087ad3304c2d611c254",
            "217c45ff011d4903bc9a68ca8947f337",
            "88d709671ada46028ff096fd7b820502"
          ]
        },
        "outputId": "0f10628f-d648-4365-bb65-33ead75ecc81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Hardening environment (prevents common Colab/py3.12 numpy corruption)...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.4.4 requires click!=8.2.*,>=4.0, but you have click 8.2.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ Packages installed.\n",
            "‚úÖ Imports loaded successfully.\n",
            "üîë Enter OPENAI_API_KEY (leave empty to run without OpenAI): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "üîå OpenAI enabled: True\n",
            "\n",
            "üöÄ Running RAG to create test cases...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5ba0665c8534d76a9ecc673ae33f1b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Built 5 LLMTestCase objects.\n",
            "\n",
            "‚úÖ Metrics configured.\n",
            "\n",
            "üß™ Running DeepEval evaluate(...) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mAnswer Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Answer Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Precision Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Precision Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Recall Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Recall Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "‚ú® You're running DeepEval's latest \u001b[38;2;106;0;255mRAG Correctness Rubric \u001b[0m\u001b[1;38;2;106;0;255m(\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m)\u001b[0m\u001b[38;2;106;0;255m \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
              "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚ú® You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">RAG Correctness Rubric </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">(</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">)</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
              "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "217c45ff011d4903bc9a68ca8947f337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n",
            "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n",
            "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n",
            "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n",
            "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n",
            "ERROR:deepeval.retry.openai:call timed out after 88.5s (per attempt). Increase DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE (None disables) or reduce work per attempt. Retrying: 1 time(s)...\n",
            "INFO:deepeval.retry.openai:Retrying in 1.6284550566606426 s (attempt 1) after TimeoutError('call timed out after 88.5s (per attempt). Increase DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE (None disables) or reduce work per attempt.')\n",
            "ERROR:deepeval.retry.openai:call timed out after 88.5s (per attempt). Increase DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE (None disables) or reduce work per attempt. Retrying: 1 time(s)...\n",
            "INFO:deepeval.retry.openai:Retrying in 2.088525996982983 s (attempt 1) after TimeoutError('call timed out after 88.5s (per attempt). Increase DEEPEVAL_PER_ATTEMPT_TIMEOUT_SECONDS_OVERRIDE (None disables) or reduce work per attempt.')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the question without any irrelevant information. Great job!, error: None)\n",
            "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because there are no contradictions‚Äîgreat job staying true to the retrieval context!, error: None)\n",
            "  - ‚ùå Contextual Relevancy (score: 0.25, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.25 because while most of the context does not define or explain 'contextual precision' (e.g., 'does not define or explain what contextual precision means'), there are a couple of relevant statements that do address the input ('Contextual precision evaluates how well retrieved chunks are ranked by relevance to a query.' and 'High precision means relevant chunks appear earlier in the ranked list.')., error: None)\n",
            "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the first node in the retrieval contexts directly defines contextual precision: 'Contextual precision evaluates how well retrieved chunks are ranked by relevance to a query. High precision means relevant chunks appear earlier in the ranked list.' All irrelevant nodes, such as the second node ('discusses a pitfall related to missing expected_output'), the third node ('provides an overview of DeepEval and mentions contextual precision as a supported metric'), and the fourth node ('defines contextual recall, not contextual precision'), are ranked after the relevant node. Great job keeping the most relevant information at the top!, error: None)\n",
            "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the sentence in the expected output is fully supported by the information in node 1 in the retrieval context. Great job!, error: None)\n",
            "  - ‚úÖ RAG Correctness Rubric (GEval) [GEval] (score: 0.996267311613631, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The Actual Output directly and fully answers the input query by defining contextual precision in alignment with both the Expected Output and the Retrieval Context. It accurately explains that contextual precision evaluates how well relevant chunks are ranked by relevance and that high precision means relevant chunks appear earlier, which is supported by the provided context. The response is concise, avoids unnecessary details, and does not introduce any invented facts., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What does contextual precision mean?\n",
            "  - actual output: Contextual precision evaluates how well retrieved chunks are ranked by relevance to a query. High precision means relevant chunks appear earlier in the ranked list.\n",
            "  - expected output: Contextual precision evaluates whether relevant retrieved chunks are ranked higher than irrelevant ones for a given query.\n",
            "  - context: None\n",
            "  - retrieval context: ['Contextual Precision\\nContextual precision evaluates how well retrieved chunks are ranked by relevance to a query. High precision means relevant chunks appear earlier in the ranked list.', 'Common Pitfall: Missing expected_output\\nSome RAG metrics require expected_output in addition to input and retrieval_context. If expected_output is None, evaluation fails for metrics like contextual precision/recall.', 'DeepEval Overview\\nDeepEval is an open-source LLM evaluation framework for unit testing LLM apps. It supports LLM-as-a-judge metrics, custom metrics like G-Eval, and RAG metrics such as contextual precision and faithfulness.', 'Contextual Recall\\nContextual recall measures whether the retriever returns enough relevant context to answer the query. Low recall means key information was missed in retrieval.']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed the question without any irrelevant statements. Great job staying focused and clear!, error: None)\n",
            "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because there are no contradictions‚Äîgreat job staying true to the retrieval context!, error: None)\n",
            "  - ‚úÖ Contextual Relevancy (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.50 because while some statements directly define contextual recall ('Contextual recall measures whether the retriever returns enough relevant context to answer the query.'), other parts of the context are irrelevant, focusing on different metrics or unrelated tools ('The statement discusses contextual precision', 'DeepEval is an open-source LLM evaluation framework...')., error: None)\n",
            "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the top-ranked node in the retrieval contexts directly defines contextual recall, as shown by the quote: 'Contextual recall measures whether the retriever returns enough relevant context to answer the query.' All irrelevant nodes, such as the second node ('discusses a pitfall about missing expected_output'), the third node ('defines contextual precision, not recall'), and the fourth node ('general overview of DeepEval and mentions RAG metrics'), are ranked lower than the relevant node. Great job‚Äîrelevant information is prioritized perfectly!, error: None)\n",
            "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because both sentences in the expected output are fully supported by the information in node 1 in the retrieval context. Great job!, error: None)\n",
            "  - ‚úÖ RAG Correctness Rubric (GEval) [GEval] (score: 0.9939913345618183, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The Actual Output directly and fully answers the input query by defining contextual recall in retrieval, matching the Expected Output in correctness and completeness. All information is supported by the Retrieval Context, with no invented facts. The response is concise and avoids unnecessary details while remaining useful., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What does contextual recall mean in retrieval?\n",
            "  - actual output: Contextual recall in retrieval measures whether the retriever returns enough relevant context to answer the query. Low contextual recall means key information was missed in retrieval.\n",
            "  - expected output: Contextual recall measures whether the retriever returns enough relevant context to answer the query, capturing key missing information issues.\n",
            "  - context: None\n",
            "  - retrieval context: ['Contextual Recall\\nContextual recall measures whether the retriever returns enough relevant context to answer the query. Low recall means key information was missed in retrieval.', 'Common Pitfall: Missing expected_output\\nSome RAG metrics require expected_output in addition to input and retrieval_context. If expected_output is None, evaluation fails for metrics like contextual precision/recall.', 'Contextual Precision\\nContextual precision evaluates how well retrieved chunks are ranked by relevance to a query. High precision means relevant chunks appear earlier in the ranked list.', 'DeepEval Overview\\nDeepEval is an open-source LLM evaluation framework for unit testing LLM apps. It supports LLM-as-a-judge metrics, custom metrics like G-Eval, and RAG metrics such as contextual precision and faithfulness.']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and directly addressed what faithfulness measures in a RAG system. Great job staying on topic!, error: None)\n",
            "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because there are no contradictions‚Äîgreat job staying true to the retrieval context!, error: None)\n",
            "  - ‚úÖ Contextual Relevancy (score: 0.5, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.50 because while some statements like 'Faithfulness checks whether the answer is supported by retrieved context.' are directly relevant, much of the context discusses unrelated topics such as DeepEval's features and test case components, which do not address what faithfulness measures in a RAG system., error: None)\n",
            "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the most relevant node, which clearly states 'Faithfulness checks whether the answer is supported by retrieved context,' is ranked first. All less relevant nodes, such as the second node ('describes DeepEval as an evaluation framework and lists supported metrics, but does not explain what faithfulness measures'), are ranked lower, ensuring perfect contextual precision., error: None)\n",
            "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the sentence in the expected output is fully supported by node 1 in the retrieval context, with no missing information. Great job!, error: None)\n",
            "  - ‚úÖ RAG Correctness Rubric (GEval) [GEval] (score: 0.8, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The Actual Output correctly states that faithfulness measures whether the answer is supported by the retrieved context, directly addressing the Input and aligning with the Retrieval Context. However, it omits the aspect of avoiding hallucinations, which is present in both the Expected Output and the Retrieval Context. The response is concise and accurate but lacks full completeness., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What does faithfulness measure in a RAG system?\n",
            "  - actual output: Faithfulness measures whether the answer is supported by the retrieved context in a RAG system.\n",
            "  - expected output: Faithfulness measures whether the generated answer is supported by the retrieved context and avoids hallucinations not grounded in that context.\n",
            "  - context: None\n",
            "  - retrieval context: ['RAG Evaluation: Why Faithfulness Matters\\nFaithfulness checks whether the answer is supported by retrieved context. In RAG, hallucinations occur when the model states claims not grounded in context.', 'DeepEval Overview\\nDeepEval is an open-source LLM evaluation framework for unit testing LLM apps. It supports LLM-as-a-judge metrics, custom metrics like G-Eval, and RAG metrics such as contextual precision and faithfulness.', 'Common Pitfall: Missing expected_output\\nSome RAG metrics require expected_output in addition to input and retrieval_context. If expected_output is None, evaluation fails for metrics like contextual precision/recall.', 'What a DeepEval Test Case Contains\\nA test case typically includes input (query), actual_output (model answer), expected_output (gold answer), and retrieval_context (ranked retrieved passages) for RAG.']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant and addressed the question directly without any irrelevant information. Great job staying focused and concise!, error: None)\n",
            "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because there are no contradictions‚Äîgreat job staying true to the retrieval context!, error: None)\n",
            "  - ‚ùå Contextual Relevancy (score: 0.42857142857142855, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.43 because, while there are some relevant statements like 'DeepEval is an open-source LLM evaluation framework for unit testing LLM apps,' much of the context discusses unrelated topics such as faithfulness in RAG evaluation and hallucinations, which do not directly answer what DeepEval is used for., error: None)\n",
            "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the first node in the retrieval contexts directly answers the input by stating what DeepEval is used for, while the irrelevant nodes (ranks 2, 3, and 4) do not address the use or purpose of DeepEval. All relevant information is ranked at the top, and unrelated nodes are correctly ranked lower. Great job!, error: None)\n",
            "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the sentence in the expected output is fully supported by the information in node 1 in the retrieval context., error: None)\n",
            "  - ‚úÖ RAG Correctness Rubric (GEval) [GEval] (score: 0.903732689042298, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The Actual Output directly and fully answers the input query by explaining that DeepEval is an open-source evaluation framework for unit testing LLM applications and lists supported metrics, aligning closely with the Expected Output. All information is supported by the Retrieval Context, and the response is concise and useful. The only minor shortcoming is slightly more detail than the Expected Output, but it remains relevant and accurate., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: What is DeepEval used for?\n",
            "  - actual output: DeepEval is used as an open-source evaluation framework for unit testing large language model (LLM) applications, supporting various metrics including LLM-as-a-judge, custom metrics like G-Eval, and retrieval-augmented generation (RAG) metrics such as contextual precision and faithfulness.\n",
            "  - expected output: DeepEval is used to evaluate and unit test LLM applications using metrics like LLM-as-a-judge, G-Eval, and RAG metrics.\n",
            "  - context: None\n",
            "  - retrieval context: ['DeepEval Overview\\nDeepEval is an open-source LLM evaluation framework for unit testing LLM apps. It supports LLM-as-a-judge metrics, custom metrics like G-Eval, and RAG metrics such as contextual precision and faithfulness.', 'What a DeepEval Test Case Contains\\nA test case typically includes input (query), actual_output (model answer), expected_output (gold answer), and retrieval_context (ranked retrieved passages) for RAG.', 'RAG Evaluation: Why Faithfulness Matters\\nFaithfulness checks whether the answer is supported by retrieved context. In RAG, hallucinations occur when the model states claims not grounded in context.', 'Contextual Precision\\nContextual precision evaluates how well retrieved chunks are ranked by relevance to a query. High precision means relevant chunks appear earlier in the ranked list.']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Metrics Summary\n",
            "\n",
            "  - ‚úÖ Answer Relevancy (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the answer was fully relevant to the input and contained no irrelevant statements. Great job staying focused and on-topic!, error: None)\n",
            "  - ‚úÖ Faithfulness (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because there are no contradictions‚Äîgreat job staying true to the retrieval context!, error: None)\n",
            "  - ‚úÖ Contextual Relevancy (score: 0.8333333333333334, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 0.83 because while several statements like 'Faithfulness checks whether the answer is supported by retrieved context' and 'Even grounded answers can be irrelevant if they don't respond to the question' are relevant to why an answer might be relevant but low quality in RAG, there is also some context that only describes DeepEval test case components and does not directly address the input question., error: None)\n",
            "  - ‚úÖ Contextual Precision (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because all relevant nodes are ranked above the irrelevant nodes. The first and second nodes directly address why an answer might be relevant but low quality, as shown by their explanations: 'Low recall means key information was missed in retrieval,' and 'Faithfulness checks whether the answer is supported by retrieved context.' The third and fourth nodes, which are less relevant‚Äîfocusing on grounded but irrelevant answers and test case structure‚Äîare correctly ranked lower. Great job keeping the most useful information at the top!, error: None)\n",
            "  - ‚úÖ Contextual Recall (score: 1.0, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The score is 1.00 because the sentence in the expected output is fully supported by the 3rd node in the retrieval context, with no missing or unsupported details. Great job!, error: None)\n",
            "  - ‚úÖ RAG Correctness Rubric (GEval) [GEval] (score: 0.8731058578630007, threshold: 0.5, strict: False, evaluation model: gpt-4.1, reason: The Actual Output directly answers the query by explaining that an answer can be relevant but low quality if it is not faithful or does not sufficiently address the user's query, which aligns with the Expected Output and is supported by the Retrieval Context discussing faithfulness and answer relevancy. The explanation is concise and avoids unsupported information. However, it could be slightly improved by explicitly mentioning missing important details, as highlighted in the Expected Output., error: None)\n",
            "\n",
            "For test case:\n",
            "\n",
            "  - input: Why might an answer be relevant but still low quality in RAG?\n",
            "  - actual output: An answer might be relevant but still low quality in RAG if it is not faithful‚Äîmeaning it includes claims not supported by the retrieved context (hallucinations)‚Äîor if it does not sufficiently address the user's query despite being grounded.\n",
            "  - expected output: An answer can address the question (relevant) but still be low quality if it is not grounded in retrieved context or misses important details.\n",
            "  - context: None\n",
            "  - retrieval context: ['Contextual Recall\\nContextual recall measures whether the retriever returns enough relevant context to answer the query. Low recall means key information was missed in retrieval.', 'RAG Evaluation: Why Faithfulness Matters\\nFaithfulness checks whether the answer is supported by retrieved context. In RAG, hallucinations occur when the model states claims not grounded in context.', \"Answer Relevancy\\nAnswer relevancy measures whether the generated answer addresses the user's query. Even grounded answers can be irrelevant if they don't respond to the question.\", 'What a DeepEval Test Case Contains\\nA test case typically includes input (query), actual_output (model answer), expected_output (gold answer), and retrieval_context (ranked retrieved passages) for RAG.']\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Overall Metric Pass Rates\n",
            "\n",
            "Answer Relevancy: 100.00% pass rate\n",
            "Faithfulness: 100.00% pass rate\n",
            "Contextual Relevancy: 60.00% pass rate\n",
            "Contextual Precision: 100.00% pass rate\n",
            "Contextual Recall: 100.00% pass rate\n",
            "RAG Correctness Rubric (GEval) [GEval]: 100.00% pass rate\n",
            "\n",
            "======================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1;33m‚ö† WARNING:\u001b[0m No hyperparameters logged.\n",
              "¬ª \u001b]8;id=937191;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
              "\n",
              "================================================================================\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">‚ö† WARNING:</span> No hyperparameters logged.\n",
              "¬ª <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
              "\n",
              "================================================================================\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\n",
              "\u001b[38;2;5;245;141m‚úì\u001b[0m Evaluation completed üéâ! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m99.\u001b[0m25s | token cost: \u001b[1;36m0.152546\u001b[0m USD\u001b[1m)\u001b[0m\n",
              "¬ª Test Results \u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
              "   ¬ª Pass Rate: \u001b[1;36m60.0\u001b[0m% | Passed: \u001b[1;32m3\u001b[0m | Failed: \u001b[1;31m2\u001b[0m\n",
              "\n",
              " ================================================================================ \n",
              "\n",
              "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
              "  ¬ª Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
              "\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "\n",
              "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">‚úì</span> Evaluation completed üéâ! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.</span>25s | token cost: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.152546</span> USD<span style=\"font-weight: bold\">)</span>\n",
              "¬ª Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
              "   ¬ª Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2</span>\n",
              "\n",
              " ================================================================================ \n",
              "\n",
              "¬ª Want to share evals with your team, or a place for your test cases to live? ‚ù§Ô∏è üè°\n",
              "  ¬ª Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
              "\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Compact score table:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   case_id                                              query  \\\n",
              "0        0                         What is DeepEval used for?   \n",
              "1        1    What does faithfulness measure in a RAG system?   \n",
              "2        2               What does contextual precision mean?   \n",
              "3        3     What does contextual recall mean in retrieval?   \n",
              "4        4  Why might an answer be relevant but still low ...   \n",
              "\n",
              "   Answer Relevancy_score  Faithfulness_score  Contextual Relevancy_score  \\\n",
              "0                     1.0                 1.0                    0.250000   \n",
              "1                     1.0                 1.0                    0.500000   \n",
              "2                     1.0                 1.0                    0.500000   \n",
              "3                     1.0                 1.0                    0.428571   \n",
              "4                     1.0                 1.0                    0.833333   \n",
              "\n",
              "   Contextual Precision_score  Contextual Recall_score  \\\n",
              "0                         1.0                      1.0   \n",
              "1                         1.0                      1.0   \n",
              "2                         1.0                      1.0   \n",
              "3                         1.0                      1.0   \n",
              "4                         1.0                      1.0   \n",
              "\n",
              "   RAG Correctness Rubric (GEval) [GEval]_score  \n",
              "0                                      0.996267  \n",
              "1                                      0.993991  \n",
              "2                                      0.800000  \n",
              "3                                      0.903733  \n",
              "4                                      0.873106  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a52a7b59-146f-4c7d-a9a8-fed5c4297aad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>case_id</th>\n",
              "      <th>query</th>\n",
              "      <th>Answer Relevancy_score</th>\n",
              "      <th>Faithfulness_score</th>\n",
              "      <th>Contextual Relevancy_score</th>\n",
              "      <th>Contextual Precision_score</th>\n",
              "      <th>Contextual Recall_score</th>\n",
              "      <th>RAG Correctness Rubric (GEval) [GEval]_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What is DeepEval used for?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>What does faithfulness measure in a RAG system?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.993991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>What does contextual precision mean?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>What does contextual recall mean in retrieval?</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.903733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Why might an answer be relevant but still low ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.873106</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a52a7b59-146f-4c7d-a9a8-fed5c4297aad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a52a7b59-146f-4c7d-a9a8-fed5c4297aad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a52a7b59-146f-4c7d-a9a8-fed5c4297aad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-47cdb15f-88dd-455d-8b59-5cd3284f7d2b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-47cdb15f-88dd-455d-8b59-5cd3284f7d2b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-47cdb15f-88dd-455d-8b59-5cd3284f7d2b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3f20e5d5-8a00-4272-b03d-aabb34d2bb19\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('compact')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3f20e5d5-8a00-4272-b03d-aabb34d2bb19 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('compact');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "compact",
              "summary": "{\n  \"name\": \"compact\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"case_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What does faithfulness measure in a RAG system?\",\n          \"Why might an answer be relevant but still low quality in RAG?\",\n          \"What does contextual precision mean?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer Relevancy_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Faithfulness_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Relevancy_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2113556287730681,\n        \"min\": 0.25,\n        \"max\": 0.8333333333333334,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Precision_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Recall_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RAG Correctness Rubric (GEval) [GEval]_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0835741997313408,\n        \"min\": 0.8,\n        \"max\": 0.996267311613631,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9939913345618183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßæ Full details (includes reasons):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   case_id                                              query  \\\n",
              "0        0                         What is DeepEval used for?   \n",
              "1        1    What does faithfulness measure in a RAG system?   \n",
              "2        2               What does contextual precision mean?   \n",
              "3        3     What does contextual recall mean in retrieval?   \n",
              "4        4  Why might an answer be relevant but still low ...   \n",
              "\n",
              "                                       actual_output  \\\n",
              "0  DeepEval is used as an open-source evaluation ...   \n",
              "1  Faithfulness measures whether the answer is su...   \n",
              "2  Contextual precision evaluates how well retrie...   \n",
              "3  Contextual recall in retrieval measures whethe...   \n",
              "4  An answer might be relevant but still low qual...   \n",
              "\n",
              "                                     expected_output  Answer Relevancy_score  \\\n",
              "0  DeepEval is used to evaluate and unit test LLM...                     1.0   \n",
              "1  Faithfulness measures whether the generated an...                     1.0   \n",
              "2  Contextual precision evaluates whether relevan...                     1.0   \n",
              "3  Contextual recall measures whether the retriev...                     1.0   \n",
              "4  An answer can address the question (relevant) ...                     1.0   \n",
              "\n",
              "                             Answer Relevancy_reason  Faithfulness_score  \\\n",
              "0  The score is 1.00 because the answer was fully...                 1.0   \n",
              "1  The score is 1.00 because the answer was fully...                 1.0   \n",
              "2  The score is 1.00 because the answer was fully...                 1.0   \n",
              "3  The score is 1.00 because the answer was fully...                 1.0   \n",
              "4  The score is 1.00 because the answer was fully...                 1.0   \n",
              "\n",
              "                                 Faithfulness_reason  \\\n",
              "0  The score is 1.00 because there are no contrad...   \n",
              "1  The score is 1.00 because there are no contrad...   \n",
              "2  The score is 1.00 because there are no contrad...   \n",
              "3  The score is 1.00 because there are no contrad...   \n",
              "4  The score is 1.00 because there are no contrad...   \n",
              "\n",
              "   Contextual Relevancy_score  \\\n",
              "0                    0.250000   \n",
              "1                    0.500000   \n",
              "2                    0.500000   \n",
              "3                    0.428571   \n",
              "4                    0.833333   \n",
              "\n",
              "                         Contextual Relevancy_reason  \\\n",
              "0  The score is 0.25 because while most of the co...   \n",
              "1  The score is 0.50 because while some statement...   \n",
              "2  The score is 0.50 because while some statement...   \n",
              "3  The score is 0.43 because, while there are som...   \n",
              "4  The score is 0.83 because while several statem...   \n",
              "\n",
              "   Contextual Precision_score  \\\n",
              "0                         1.0   \n",
              "1                         1.0   \n",
              "2                         1.0   \n",
              "3                         1.0   \n",
              "4                         1.0   \n",
              "\n",
              "                         Contextual Precision_reason  Contextual Recall_score  \\\n",
              "0  The score is 1.00 because the first node in th...                      1.0   \n",
              "1  The score is 1.00 because the top-ranked node ...                      1.0   \n",
              "2  The score is 1.00 because the most relevant no...                      1.0   \n",
              "3  The score is 1.00 because the first node in th...                      1.0   \n",
              "4  The score is 1.00 because all relevant nodes a...                      1.0   \n",
              "\n",
              "                            Contextual Recall_reason  \\\n",
              "0  The score is 1.00 because the sentence in the ...   \n",
              "1  The score is 1.00 because both sentences in th...   \n",
              "2  The score is 1.00 because the sentence in the ...   \n",
              "3  The score is 1.00 because the sentence in the ...   \n",
              "4  The score is 1.00 because the sentence in the ...   \n",
              "\n",
              "   RAG Correctness Rubric (GEval) [GEval]_score  \\\n",
              "0                                      0.996267   \n",
              "1                                      0.993991   \n",
              "2                                      0.800000   \n",
              "3                                      0.903733   \n",
              "4                                      0.873106   \n",
              "\n",
              "       RAG Correctness Rubric (GEval) [GEval]_reason  \n",
              "0  The Actual Output directly and fully answers t...  \n",
              "1  The Actual Output directly and fully answers t...  \n",
              "2  The Actual Output correctly states that faithf...  \n",
              "3  The Actual Output directly and fully answers t...  \n",
              "4  The Actual Output directly answers the query b...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6f36c34-9da7-4517-be51-dd68680b8fac\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>case_id</th>\n",
              "      <th>query</th>\n",
              "      <th>actual_output</th>\n",
              "      <th>expected_output</th>\n",
              "      <th>Answer Relevancy_score</th>\n",
              "      <th>Answer Relevancy_reason</th>\n",
              "      <th>Faithfulness_score</th>\n",
              "      <th>Faithfulness_reason</th>\n",
              "      <th>Contextual Relevancy_score</th>\n",
              "      <th>Contextual Relevancy_reason</th>\n",
              "      <th>Contextual Precision_score</th>\n",
              "      <th>Contextual Precision_reason</th>\n",
              "      <th>Contextual Recall_score</th>\n",
              "      <th>Contextual Recall_reason</th>\n",
              "      <th>RAG Correctness Rubric (GEval) [GEval]_score</th>\n",
              "      <th>RAG Correctness Rubric (GEval) [GEval]_reason</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What is DeepEval used for?</td>\n",
              "      <td>DeepEval is used as an open-source evaluation ...</td>\n",
              "      <td>DeepEval is used to evaluate and unit test LLM...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the answer was fully...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because there are no contrad...</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>The score is 0.25 because while most of the co...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the first node in th...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the sentence in the ...</td>\n",
              "      <td>0.996267</td>\n",
              "      <td>The Actual Output directly and fully answers t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>What does faithfulness measure in a RAG system?</td>\n",
              "      <td>Faithfulness measures whether the answer is su...</td>\n",
              "      <td>Faithfulness measures whether the generated an...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the answer was fully...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because there are no contrad...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>The score is 0.50 because while some statement...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the top-ranked node ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because both sentences in th...</td>\n",
              "      <td>0.993991</td>\n",
              "      <td>The Actual Output directly and fully answers t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>What does contextual precision mean?</td>\n",
              "      <td>Contextual precision evaluates how well retrie...</td>\n",
              "      <td>Contextual precision evaluates whether relevan...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the answer was fully...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because there are no contrad...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>The score is 0.50 because while some statement...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the most relevant no...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the sentence in the ...</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>The Actual Output correctly states that faithf...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>What does contextual recall mean in retrieval?</td>\n",
              "      <td>Contextual recall in retrieval measures whethe...</td>\n",
              "      <td>Contextual recall measures whether the retriev...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the answer was fully...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because there are no contrad...</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>The score is 0.43 because, while there are som...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the first node in th...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the sentence in the ...</td>\n",
              "      <td>0.903733</td>\n",
              "      <td>The Actual Output directly and fully answers t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Why might an answer be relevant but still low ...</td>\n",
              "      <td>An answer might be relevant but still low qual...</td>\n",
              "      <td>An answer can address the question (relevant) ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the answer was fully...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because there are no contrad...</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>The score is 0.83 because while several statem...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because all relevant nodes a...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>The score is 1.00 because the sentence in the ...</td>\n",
              "      <td>0.873106</td>\n",
              "      <td>The Actual Output directly answers the query b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6f36c34-9da7-4517-be51-dd68680b8fac')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c6f36c34-9da7-4517-be51-dd68680b8fac button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c6f36c34-9da7-4517-be51-dd68680b8fac');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-863a3220-df90-4eb7-80d3-846966c621c8\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-863a3220-df90-4eb7-80d3-846966c621c8')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-863a3220-df90-4eb7-80d3-846966c621c8 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3364cd6c-11ae-4acd-9af7-38be57d98d7c\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3364cd6c-11ae-4acd-9af7-38be57d98d7c button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"case_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 4,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1,\n          4,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"What does faithfulness measure in a RAG system?\",\n          \"Why might an answer be relevant but still low quality in RAG?\",\n          \"What does contextual precision mean?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"actual_output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Faithfulness measures whether the answer is supported by the retrieved context in a RAG system.\",\n          \"An answer might be relevant but still low quality in RAG if it is not faithful\\u2014meaning it includes claims not supported by the retrieved context (hallucinations)\\u2014or if it does not sufficiently address the user's query despite being grounded.\",\n          \"Contextual precision evaluates how well retrieved chunks are ranked by relevance to a query. High precision means relevant chunks appear earlier in the ranked list.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Faithfulness measures whether the generated answer is supported by the retrieved context and avoids hallucinations not grounded in that context.\",\n          \"An answer can address the question (relevant) but still be low quality if it is not grounded in retrieved context or misses important details.\",\n          \"Contextual precision evaluates whether relevant retrieved chunks are ranked higher than irrelevant ones for a given query.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer Relevancy_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Answer Relevancy_reason\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The score is 1.00 because the answer was fully relevant and directly addressed the question without any irrelevant statements. Great job staying focused and clear!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Faithfulness_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Faithfulness_reason\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"The score is 1.00 because there are no contradictions\\u2014great job staying true to the retrieval context!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Relevancy_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2113556287730681,\n        \"min\": 0.25,\n        \"max\": 0.8333333333333334,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Relevancy_reason\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The score is 0.50 because while some statements directly define contextual recall ('Contextual recall measures whether the retriever returns enough relevant context to answer the query.'), other parts of the context are irrelevant, focusing on different metrics or unrelated tools ('The statement discusses contextual precision', 'DeepEval is an open-source LLM evaluation framework...').\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Precision_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Precision_reason\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The score is 1.00 because the top-ranked node in the retrieval contexts directly defines contextual recall, as shown by the quote: 'Contextual recall measures whether the retriever returns enough relevant context to answer the query.' All irrelevant nodes, such as the second node ('discusses a pitfall about missing expected_output'), the third node ('defines contextual precision, not recall'), and the fourth node ('general overview of DeepEval and mentions RAG metrics'), are ranked lower than the relevant node. Great job\\u2014relevant information is prioritized perfectly!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Recall_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Contextual Recall_reason\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The score is 1.00 because both sentences in the expected output are fully supported by the information in node 1 in the retrieval context. Great job!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RAG Correctness Rubric (GEval) [GEval]_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0835741997313408,\n        \"min\": 0.8,\n        \"max\": 0.996267311613631,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.9939913345618183\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RAG Correctness Rubric (GEval) [GEval]_reason\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The Actual Output directly and fully answers the input query by defining contextual recall in retrieval, matching the Expected Output in correctness and completeness. All information is supported by the Retrieval Context, with no invented facts. The response is concise and avoids unnecessary details while remaining useful.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Done. Tip: if contextual precision/recall are low, improve retriever ranking/coverage; if faithfulness is low, tighten generation to only use context.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüß™ Running DeepEval evaluate(...) ...\")\n",
        "\n",
        "results = evaluate(test_cases=test_cases, metrics=metrics)\n",
        "\n",
        "summary_rows = []\n",
        "for idx, tc in enumerate(test_cases):\n",
        "    row = {\n",
        "        \"case_id\": idx,\n",
        "        \"query\": tc.input,\n",
        "        \"actual_output\": tc.actual_output[:200] + (\"...\" if len(tc.actual_output) > 200 else \"\"),\n",
        "    }\n",
        "    for m in metrics:\n",
        "        row[m.__class__.__name__ if hasattr(m, \"__class__\") else str(m)] = None\n",
        "\n",
        "    summary_rows.append(row)\n",
        "\n",
        "def try_extract_case_metrics(results_obj):\n",
        "    extracted = []\n",
        "    candidates = []\n",
        "    for attr in [\"test_results\", \"results\", \"evaluations\"]:\n",
        "        if hasattr(results_obj, attr):\n",
        "            candidates = getattr(results_obj, attr)\n",
        "            break\n",
        "    if not candidates and isinstance(results_obj, list):\n",
        "        candidates = results_obj\n",
        "\n",
        "    for case_i, case_result in enumerate(candidates or []):\n",
        "        item = {\"case_id\": case_i}\n",
        "        metrics_list = None\n",
        "        for attr in [\"metrics_data\", \"metrics\", \"metric_results\"]:\n",
        "            if hasattr(case_result, attr):\n",
        "                metrics_list = getattr(case_result, attr)\n",
        "                break\n",
        "        if isinstance(metrics_list, dict):\n",
        "            for k, v in metrics_list.items():\n",
        "                item[f\"{k}_score\"] = getattr(v, \"score\", None) if v is not None else None\n",
        "                item[f\"{k}_reason\"] = getattr(v, \"reason\", None) if v is not None else None\n",
        "        else:\n",
        "            for mr in metrics_list or []:\n",
        "                name = getattr(mr, \"name\", None) or getattr(getattr(mr, \"metric\", None), \"name\", None)\n",
        "                if not name:\n",
        "                    name = mr.__class__.__name__\n",
        "                item[f\"{name}_score\"] = getattr(mr, \"score\", None)\n",
        "                item[f\"{name}_reason\"] = getattr(mr, \"reason\", None)\n",
        "        extracted.append(item)\n",
        "    return extracted\n",
        "\n",
        "case_metrics = try_extract_case_metrics(results)\n",
        "\n",
        "df_base = pd.DataFrame([{\n",
        "    \"case_id\": i,\n",
        "    \"query\": tc.input,\n",
        "    \"actual_output\": tc.actual_output,\n",
        "    \"expected_output\": tc.expected_output,\n",
        "} for i, tc in enumerate(test_cases)])\n",
        "\n",
        "df_metrics = pd.DataFrame(case_metrics) if case_metrics else pd.DataFrame([])\n",
        "df = df_base.merge(df_metrics, on=\"case_id\", how=\"left\")\n",
        "\n",
        "score_cols = [c for c in df.columns if c.endswith(\"_score\")]\n",
        "compact = df[[\"case_id\", \"query\"] + score_cols].copy()\n",
        "\n",
        "print(\"\\nüìä Compact score table:\")\n",
        "display(compact)\n",
        "\n",
        "print(\"\\nüßæ Full details (includes reasons):\")\n",
        "display(df)\n",
        "\n",
        "print(\"\\n‚úÖ Done. Tip: if contextual precision/recall are low, improve retriever ranking/coverage; if faithfulness is low, tighten generation to only use context.\")"
      ]
    }
  ]
}