{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REdfVjP6y2OS"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U google-generativeai networkx pyvis scikit-learn numpy\n",
        "\n",
        "import os\n",
        "import json\n",
        "import uuid\n",
        "import time\n",
        "import getpass\n",
        "import random\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from IPython.display import display, HTML\n",
        "from pyvis.network import Network\n",
        "from google.api_core import exceptions\n",
        "\n",
        "def retry_with_backoff(func, *args, **kwargs):\n",
        "    max_retries = 5\n",
        "    base_delay = 5\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except exceptions.ResourceExhausted:\n",
        "            wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
        "            print(f\"   ‚è≥ Quota limit hit. Cooling down for {wait_time:.1f}s...\")\n",
        "            time.sleep(wait_time)\n",
        "        except Exception as e:\n",
        "            if \"429\" in str(e):\n",
        "                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
        "                print(f\"   ‚è≥ Quota limit hit (HTTP 429). Cooling down for {wait_time:.1f}s...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è Unexpected Error: {e}\")\n",
        "                return None\n",
        "    print(\"   ‚ùå Max retries reached.\")\n",
        "    return None\n",
        "\n",
        "print(\"Enter your Google AI Studio API Key (Input will be hidden):\")\n",
        "API_KEY = getpass.getpass()\n",
        "\n",
        "genai.configure(api_key=API_KEY)\n",
        "MODEL_NAME = \"gemini-2.5-flash\"\n",
        "EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
        "\n",
        "print(f\"‚úÖ API Key configured. Using model: {MODEL_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CcJIju6y2DO"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class MemoryNode:\n",
        "    id: str\n",
        "    content: str\n",
        "    type: str\n",
        "    embedding: List[float] = field(default_factory=list)\n",
        "    timestamp: int = 0\n",
        "\n",
        "class RobustZettelkasten:\n",
        "    def __init__(self):\n",
        "        self.graph = nx.Graph()\n",
        "        self.model = genai.GenerativeModel(MODEL_NAME)\n",
        "        self.step_counter = 0\n",
        "\n",
        "    def _get_embedding(self, text):\n",
        "        result = retry_with_backoff(\n",
        "            genai.embed_content,\n",
        "            model=EMBEDDING_MODEL,\n",
        "            content=text\n",
        "        )\n",
        "        return result['embedding'] if result else [0.0] * 768"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0jcofHGy2An"
      },
      "outputs": [],
      "source": [
        "def _atomize_input(self, text):\n",
        "        prompt = f\"\"\"\n",
        "        Break the following text into independent atomic facts.\n",
        "        Output JSON: {{ \"facts\": [\"fact1\", \"fact2\"] }}\n",
        "        Text: \"{text}\"\n",
        "        \"\"\"\n",
        "        response = retry_with_backoff(\n",
        "            self.model.generate_content,\n",
        "            prompt,\n",
        "            generation_config={\"response_mime_type\": \"application/json\"}\n",
        "        )\n",
        "        try:\n",
        "            return json.loads(response.text).get(\"facts\", []) if response else [text]\n",
        "        except:\n",
        "            return [text]\n",
        "\n",
        "    def _find_similar_nodes(self, embedding, top_k=3, threshold=0.45):\n",
        "        if not self.graph.nodes: return []\n",
        "\n",
        "        nodes = list(self.graph.nodes(data=True))\n",
        "        embeddings = [n[1]['data'].embedding for n in nodes]\n",
        "        valid_embeddings = [e for e in embeddings if len(e) > 0]\n",
        "\n",
        "        if not valid_embeddings: return []\n",
        "\n",
        "        sims = cosine_similarity([embedding], embeddings)[0]\n",
        "        sorted_indices = np.argsort(sims)[::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in sorted_indices[:top_k]:\n",
        "            if sims[idx] > threshold:\n",
        "                results.append((nodes[idx][0], sims[idx]))\n",
        "        return results\n",
        "\n",
        "    def add_memory(self, user_input):\n",
        "        self.step_counter += 1\n",
        "        print(f\"\\nüß† [Step {self.step_counter}] Processing: \\\"{user_input}\\\"\")\n",
        "\n",
        "        facts = self._atomize_input(user_input)\n",
        "\n",
        "        for fact in facts:\n",
        "            print(f\"   -> Atom: {fact}\")\n",
        "            emb = self._get_embedding(fact)\n",
        "            candidates = self._find_similar_nodes(emb)\n",
        "\n",
        "            node_id = str(uuid.uuid4())[:6]\n",
        "            node = MemoryNode(id=node_id, content=fact, type='fact', embedding=emb, timestamp=self.step_counter)\n",
        "            self.graph.add_node(node_id, data=node, title=fact, label=fact[:15]+\"...\")\n",
        "\n",
        "            if candidates:\n",
        "                context_str = \"\\n\".join([f\"ID {c[0]}: {self.graph.nodes[c[0]]['data'].content}\" for c in candidates])\n",
        "                prompt = f\"\"\"\n",
        "                I am adding: \"{fact}\"\n",
        "                Existing Memory:\n",
        "                {context_str}\n",
        "\n",
        "                Are any of these directly related? If yes, provide the relationship label.\n",
        "                JSON: {{ \"links\": [{{ \"target_id\": \"ID\", \"rel\": \"label\" }}] }}\n",
        "                \"\"\"\n",
        "                response = retry_with_backoff(\n",
        "                    self.model.generate_content,\n",
        "                    prompt,\n",
        "                    generation_config={\"response_mime_type\": \"application/json\"}\n",
        "                )\n",
        "\n",
        "                if response:\n",
        "                    try:\n",
        "                        links = json.loads(response.text).get(\"links\", [])\n",
        "                        for link in links:\n",
        "                            if self.graph.has_node(link['target_id']):\n",
        "                                self.graph.add_edge(node_id, link['target_id'], label=link['rel'])\n",
        "                                print(f\"      üîó Linked to {link['target_id']} ({link['rel']})\")\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "            time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6EmluFy1-G"
      },
      "outputs": [],
      "source": [
        "def consolidate_memory(self):\n",
        "        print(f\"\\nüí§ [Consolidation Phase] Reflecting...\")\n",
        "        high_degree_nodes = [n for n, d in self.graph.degree() if d >= 2]\n",
        "        processed_clusters = set()\n",
        "\n",
        "        for main_node in high_degree_nodes:\n",
        "            neighbors = list(self.graph.neighbors(main_node))\n",
        "            cluster_ids = tuple(sorted([main_node] + neighbors))\n",
        "\n",
        "            if cluster_ids in processed_clusters: continue\n",
        "            processed_clusters.add(cluster_ids)\n",
        "\n",
        "            cluster_content = [self.graph.nodes[n]['data'].content for n in cluster_ids]\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Generate a single high-level insight summary from these facts.\n",
        "            Facts: {json.dumps(cluster_content)}\n",
        "            JSON: {{ \"insight\": \"Your insight here\" }}\n",
        "            \"\"\"\n",
        "            response = retry_with_backoff(\n",
        "                self.model.generate_content,\n",
        "                prompt,\n",
        "                generation_config={\"response_mime_type\": \"application/json\"}\n",
        "            )\n",
        "\n",
        "            if response:\n",
        "                try:\n",
        "                    insight_text = json.loads(response.text).get(\"insight\")\n",
        "                    if insight_text:\n",
        "                        insight_id = f\"INSIGHT-{uuid.uuid4().hex[:4]}\"\n",
        "                        print(f\"   ‚ú® Insight: {insight_text}\")\n",
        "                        emb = self._get_embedding(insight_text)\n",
        "\n",
        "                        insight_node = MemoryNode(id=insight_id, content=insight_text, type='insight', embedding=emb)\n",
        "                        self.graph.add_node(insight_id, data=insight_node, title=f\"INSIGHT: {insight_text}\", label=\"INSIGHT\", color=\"#ff7f7f\")\n",
        "                        self.graph.add_edge(insight_id, main_node, label=\"abstracted_from\")\n",
        "                except:\n",
        "                    continue\n",
        "            time.sleep(1)\n",
        "\n",
        "    def answer_query(self, query):\n",
        "        print(f\"\\nüîç Querying: \\\"{query}\\\"\")\n",
        "        emb = self._get_embedding(query)\n",
        "        candidates = self._find_similar_nodes(emb, top_k=2)\n",
        "\n",
        "        if not candidates:\n",
        "            print(\"No relevant memory found.\")\n",
        "            return\n",
        "\n",
        "        relevant_context = set()\n",
        "        for node_id, score in candidates:\n",
        "            node_content = self.graph.nodes[node_id]['data'].content\n",
        "            relevant_context.add(f\"- {node_content} (Direct Match)\")\n",
        "            for n1 in self.graph.neighbors(node_id):\n",
        "                rel = self.graph[node_id][n1].get('label', 'related')\n",
        "                content = self.graph.nodes[n1]['data'].content\n",
        "                relevant_context.add(f\"  - linked via '{rel}' to: {content}\")\n",
        "\n",
        "        context_text = \"\\n\".join(relevant_context)\n",
        "        prompt = f\"\"\"\n",
        "        Answer based ONLY on context.\n",
        "        Question: {query}\n",
        "        Context:\n",
        "        {context_text}\n",
        "        \"\"\"\n",
        "        response = retry_with_backoff(self.model.generate_content, prompt)\n",
        "        if response:\n",
        "            print(f\"ü§ñ Agent Answer:\\n{response.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1PRCcseDuefS",
        "outputId": "22430496-8948-44de-d24d-d40bc3a6294e"
      },
      "outputs": [],
      "source": [
        "def show_graph(self):\n",
        "        try:\n",
        "            net = Network(notebook=True, cdn_resources='remote', height=\"500px\", width=\"100%\", bgcolor='#222222', font_color='white')\n",
        "            for n, data in self.graph.nodes(data=True):\n",
        "                color = \"#97c2fc\" if data['data'].type == 'fact' else \"#ff7f7f\"\n",
        "                net.add_node(n, label=data.get('label', ''), title=data['data'].content, color=color)\n",
        "            for u, v, data in self.graph.edges(data=True):\n",
        "                net.add_edge(u, v, label=data.get('label', ''))\n",
        "            net.show(\"memory_graph.html\")\n",
        "            display(HTML(\"memory_graph.html\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Graph visualization error: {e}\")\n",
        "\n",
        "brain = RobustZettelkasten()\n",
        "\n",
        "events = [\n",
        "    \"The project 'Apollo' aims to build a dashboard for tracking solar panel efficiency.\",\n",
        "    \"We chose React for the frontend because the team knows it well.\",\n",
        "    \"The backend must be Python to support the data science libraries.\",\n",
        "    \"Client called. They are unhappy with React performance on low-end devices.\",\n",
        "    \"We are switching the frontend to Svelte for better performance.\"\n",
        "]\n",
        "\n",
        "print(\"--- PHASE 1: INGESTION ---\")\n",
        "for event in events:\n",
        "    brain.add_memory(event)\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"--- PHASE 2: CONSOLIDATION ---\")\n",
        "brain.consolidate_memory()\n",
        "\n",
        "print(\"--- PHASE 3: RETRIEVAL ---\")\n",
        "brain.answer_query(\"What is the current frontend technology for Apollo and why?\")\n",
        "\n",
        "print(\"--- PHASE 4: VISUALIZATION ---\")\n",
        "brain.show_graph()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
