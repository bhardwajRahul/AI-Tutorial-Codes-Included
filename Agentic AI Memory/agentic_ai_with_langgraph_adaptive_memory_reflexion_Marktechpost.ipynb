{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U langgraph langchain-openai langchain-core pydantic numpy networkx requests\n",
        "\n",
        "import os, getpass, json, time, operator\n",
        "from typing import List, Dict, Any, Optional, Literal\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AnyMessage\n",
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import InMemorySaver"
      ],
      "metadata": {
        "id": "zh46FA410QTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OPENAI_API_KEY: \")\n",
        "\n",
        "MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
        "EMB_MODEL = os.environ.get(\"OPENAI_EMBED_MODEL\", \"text-embedding-3-small\")\n",
        "\n",
        "llm_fast = ChatOpenAI(model=MODEL, temperature=0)\n",
        "llm_deep = ChatOpenAI(model=MODEL, temperature=0)\n",
        "llm_reflect = ChatOpenAI(model=MODEL, temperature=0)\n",
        "emb = OpenAIEmbeddings(model=EMB_MODEL)"
      ],
      "metadata": {
        "id": "su3G1FTO0QD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Note(BaseModel):\n",
        "    note_id: str\n",
        "    title: str\n",
        "    content: str\n",
        "    tags: List[str] = Field(default_factory=list)\n",
        "    created_at_unix: float\n",
        "    context: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "class MemoryGraph:\n",
        "    def __init__(self):\n",
        "        self.g = nx.Graph()\n",
        "        self.note_vectors = {}\n",
        "\n",
        "    def _cos(self, a, b):\n",
        "        return float(np.dot(a, b) / ((np.linalg.norm(a) + 1e-9) * (np.linalg.norm(b) + 1e-9)))\n",
        "\n",
        "    def add_note(self, note, vec):\n",
        "        self.g.add_node(note.note_id, **note.model_dump())\n",
        "        self.note_vectors[note.note_id] = vec\n",
        "\n",
        "    def topk_related(self, vec, k=5):\n",
        "        scored = [(nid, self._cos(vec, v)) for nid, v in self.note_vectors.items()]\n",
        "        scored.sort(key=lambda x: x[1], reverse=True)\n",
        "        return [{\"note_id\": n, \"score\": s, \"title\": self.g.nodes[n][\"title\"]} for n, s in scored[:k]]\n",
        "\n",
        "    def link_note(self, a, b, w, r):\n",
        "        if a != b:\n",
        "            self.g.add_edge(a, b, weight=w, reason=r)\n",
        "\n",
        "    def evolve_links(self, nid, vec):\n",
        "        for r in self.topk_related(vec, 8):\n",
        "            if r[\"score\"] >= 0.78:\n",
        "                self.link_note(nid, r[\"note_id\"], r[\"score\"], \"evolve\")\n",
        "\n",
        "MEM = MemoryGraph()"
      ],
      "metadata": {
        "id": "q3IVKEGf0P1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def web_get(url: str) -> str:\n",
        "    import urllib.request\n",
        "    with urllib.request.urlopen(url, timeout=15) as r:\n",
        "        return r.read(25000).decode(\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "@tool\n",
        "def memory_search(query: str, k: int = 5) -> str:\n",
        "    qv = np.array(emb.embed_query(query))\n",
        "    hits = MEM.topk_related(qv, k)\n",
        "    return json.dumps(hits, ensure_ascii=False)\n",
        "\n",
        "@tool\n",
        "def memory_neighbors(note_id: str) -> str:\n",
        "    if note_id not in MEM.g:\n",
        "        return \"[]\"\n",
        "    return json.dumps([\n",
        "        {\"note_id\": n, \"weight\": MEM.g[note_id][n][\"weight\"]}\n",
        "        for n in MEM.g.neighbors(note_id)\n",
        "    ])\n",
        "\n",
        "TOOLS = [web_get, memory_search, memory_neighbors]\n",
        "TOOLS_BY_NAME = {t.name: t for t in TOOLS}"
      ],
      "metadata": {
        "id": "QF9o7lu10Pyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeliberationDecision(BaseModel):\n",
        "    mode: Literal[\"fast\", \"deep\"]\n",
        "    reason: str\n",
        "    suggested_steps: List[str]\n",
        "\n",
        "class RunSpec(BaseModel):\n",
        "    goal: str\n",
        "    constraints: List[str]\n",
        "    deliverable_format: str\n",
        "    must_use_memory: bool\n",
        "    max_tool_calls: int\n",
        "\n",
        "class Reflection(BaseModel):\n",
        "    note_title: str\n",
        "    note_tags: List[str]\n",
        "    new_rules: List[str]\n",
        "    what_worked: List[str]\n",
        "    what_failed: List[str]\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "    run_spec: Dict[str, Any]\n",
        "    messages: Annotated[List[AnyMessage], operator.add]\n",
        "    decision: Dict[str, Any]\n",
        "    final: str\n",
        "    budget_calls_remaining: int\n",
        "    tool_calls_used: int\n",
        "    max_tool_calls: int\n",
        "    last_note_id: str\n",
        "\n",
        "DECIDER_SYS = \"Decide fast vs deep.\"\n",
        "AGENT_FAST = \"Operate fast.\"\n",
        "AGENT_DEEP = \"Operate deep.\"\n",
        "REFLECT_SYS = \"Reflect and store learnings.\""
      ],
      "metadata": {
        "id": "LuxuYbjF0PwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deliberate(st):\n",
        "    spec = RunSpec.model_validate(st[\"run_spec\"])\n",
        "    d = llm_fast.with_structured_output(DeliberationDecision).invoke([\n",
        "        SystemMessage(content=DECIDER_SYS),\n",
        "        HumanMessage(content=json.dumps(spec.model_dump()))\n",
        "    ])\n",
        "    return {\"decision\": d.model_dump(), \"budget_calls_remaining\": st[\"budget_calls_remaining\"] - 1}\n",
        "\n",
        "def agent(st):\n",
        "    spec = RunSpec.model_validate(st[\"run_spec\"])\n",
        "    d = DeliberationDecision.model_validate(st[\"decision\"])\n",
        "    llm = llm_deep if d.mode == \"deep\" else llm_fast\n",
        "    sys = AGENT_DEEP if d.mode == \"deep\" else AGENT_FAST\n",
        "    out = llm.bind_tools(TOOLS).invoke([\n",
        "        SystemMessage(content=sys),\n",
        "        *st.get(\"messages\", []),\n",
        "        HumanMessage(content=json.dumps(spec.model_dump()))\n",
        "    ])\n",
        "    return {\"messages\": [out], \"budget_calls_remaining\": st[\"budget_calls_remaining\"] - 1}\n",
        "\n",
        "def route(st):\n",
        "    return \"tools\" if st[\"messages\"][-1].tool_calls else \"finalize\"\n",
        "\n",
        "def tools_node(st):\n",
        "    msgs = []\n",
        "    used = st.get(\"tool_calls_used\", 0)\n",
        "    for c in st[\"messages\"][-1].tool_calls:\n",
        "        obs = TOOLS_BY_NAME[c[\"name\"]].invoke(c[\"args\"])\n",
        "        msgs.append(ToolMessage(content=str(obs), tool_call_id=c[\"id\"]))\n",
        "        used += 1\n",
        "    return {\"messages\": msgs, \"tool_calls_used\": used}\n",
        "\n",
        "def finalize(st):\n",
        "    out = llm_deep.invoke(st[\"messages\"] + [HumanMessage(content=\"Return final output\")])\n",
        "    return {\"final\": out.content}\n",
        "\n",
        "def reflect(st):\n",
        "    r = llm_reflect.with_structured_output(Reflection).invoke([\n",
        "        SystemMessage(content=REFLECT_SYS),\n",
        "        HumanMessage(content=st[\"final\"])\n",
        "    ])\n",
        "    note = Note(\n",
        "        note_id=str(time.time()),\n",
        "        title=r.note_title,\n",
        "        content=st[\"final\"],\n",
        "        tags=r.note_tags,\n",
        "        created_at_unix=time.time()\n",
        "    )\n",
        "    vec = np.array(emb.embed_query(note.title + note.content))\n",
        "    MEM.add_note(note, vec)\n",
        "    MEM.evolve_links(note.note_id, vec)\n",
        "    return {\"last_note_id\": note.note_id}"
      ],
      "metadata": {
        "id": "VwvU8Z7z0PtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OMfKkwezL53",
        "outputId": "42ee068d-853e-426d-8ec4-9567cc16cb72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Ready. Call run_agent(...)\n",
            "\n",
            "====================\n",
            "RUN #1 FINAL\n",
            "====================\n",
            "\n",
            "```markdown\n",
            "# Workflow Design for On-Call Incident Triage\n",
            "\n",
            "## 1. Define the Roles of Each Component\n",
            "\n",
            "- **Deliberation Controller**: Manages the decision-making process, ensuring that all relevant information is considered before taking action. It facilitates discussions among agents and evaluates the potential impact of decisions.\n",
            "  \n",
            "- **Tool Governance**: Establishes guidelines for the use of tools during the triage process. It ensures that tools are used effectively and that their outputs are validated before being acted upon.\n",
            "\n",
            "- **Memory Graph Usage**: Utilizes a memory graph to store and retrieve past incidents, decisions made, and their outcomes. This aids in learning from previous experiences and improving future responses.\n",
            "\n",
            "- **Reflexion**: A mechanism for self-assessment and improvement. After each incident, the system reflects on the decisions made and their effectiveness, allowing for continuous learning.\n",
            "\n",
            "## 2. Outline the Incident Triage Process\n",
            "\n",
            "1. **Incident Detection**: Identify and log the incident through monitoring tools or alerts.\n",
            "  \n",
            "2. **Initial Assessment**: The deliberation controller gathers initial information about the incident, including severity and impact.\n",
            "\n",
            "3. **Information Retrieval**: Use the memory graph to retrieve similar past incidents and their resolutions.\n",
            "\n",
            "4. **Decision Making**: The deliberation controller evaluates options based on current data and historical insights, considering the guidelines set by tool governance.\n",
            "\n",
            "5. **Action Implementation**: Execute the chosen action, utilizing appropriate tools as per governance guidelines.\n",
            "\n",
            "6. **Monitoring and Feedback**: Continuously monitor the incident resolution process and gather feedback.\n",
            "\n",
            "7. **Post-Incident Review**: Conduct a reflexion session to analyze the effectiveness of the response and update the memory graph with new insights.\n",
            "\n",
            "## 3. Identify Potential Failure Modes and Their Detection Signals\n",
            "\n",
            "- **Failure Mode**: Incomplete Information\n",
            "  - **Detection Signal**: Deliberation controller flags insufficient data during initial assessment.\n",
            "\n",
            "- **Failure Mode**: Tool Misuse\n",
            "  - **Detection Signal**: Anomalies in tool outputs or unexpected results during action implementation.\n",
            "\n",
            "- **Failure Mode**: Memory Graph Retrieval Failure\n",
            "  - **Detection Signal**: Inability to retrieve relevant past incidents, leading to repeated mistakes.\n",
            "\n",
            "- **Failure Mode**: Ineffective Decision Making\n",
            "  - **Detection Signal**: High number of escalations or unresolved incidents post-action.\n",
            "\n",
            "## 4. Incorporate Memory Graph Usage\n",
            "\n",
            "- The memory graph serves as a repository for past incidents, decisions, and outcomes. It allows the deliberation controller to access historical data quickly, facilitating informed decision-making. The graph should be updated after each incident to include new learnings.\n",
            "\n",
            "## 5. Create a Checklist for Implementation in LangGraph\n",
            "\n",
            "- [ ] Define roles and responsibilities for each component.\n",
            "- [ ] Establish guidelines for tool governance.\n",
            "- [ ] Design the memory graph structure for incident data.\n",
            "- [ ] Implement the deliberation controller logic.\n",
            "- [ ] Create mechanisms for reflexion and feedback.\n",
            "- [ ] Develop incident detection and logging capabilities.\n",
            "- [ ] Integrate monitoring tools for real-time feedback.\n",
            "- [ ] Test the workflow with simulated incidents.\n",
            "- [ ] Review and iterate based on feedback and performance.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "This workflow provides a structured approach to on-call incident triage, incorporating essential components for effective decision-making and continuous improvement. The checklist serves as a practical guide for implementation in LangGraph.\n",
            "```\n",
            "\n",
            "--- Note created ---\n",
            "note_id: note_1766218255928\n",
            "\n",
            "Neighbors:\n",
            "[]\n",
            "\n",
            "====================\n",
            "RUN #2 FINAL\n",
            "====================\n",
            "\n",
            "```markdown\n",
            "# Workflow Design for On-Call Incident Triage\n",
            "\n",
            "## 1. Define the Roles of Each Component\n",
            "\n",
            "- **Deliberation Controller**: Manages the decision-making process, ensuring that all relevant information is considered before taking action. It facilitates discussions among agents and evaluates the potential impact of decisions.\n",
            "  \n",
            "- **Tool Governance**: Establishes guidelines for the use of tools during the triage process. It ensures that tools are used effectively and that their outputs are validated before being acted upon.\n",
            "\n",
            "- **Memory Graph Usage**: Utilizes a memory graph to store and retrieve past incidents, decisions made, and their outcomes. This aids in learning from previous experiences and improving future responses.\n",
            "\n",
            "- **Reflexion**: A mechanism for self-assessment and improvement. After each incident, the system reflects on the decisions made and their effectiveness, allowing for continuous learning.\n",
            "\n",
            "## 2. Outline the Incident Triage Process\n",
            "\n",
            "1. **Incident Detection**: Identify and log the incident through monitoring tools or alerts.\n",
            "  \n",
            "2. **Initial Assessment**: The deliberation controller gathers initial information about the incident, including severity and impact.\n",
            "\n",
            "3. **Information Retrieval**: Use the memory graph to retrieve similar past incidents and their resolutions.\n",
            "\n",
            "4. **Decision Making**: The deliberation controller evaluates options based on current data and historical insights, considering the guidelines set by tool governance.\n",
            "\n",
            "5. **Action Implementation**: Execute the chosen action, utilizing appropriate tools as per governance guidelines.\n",
            "\n",
            "6. **Monitoring and Feedback**: Continuously monitor the incident resolution process and gather feedback.\n",
            "\n",
            "7. **Post-Incident Review**: Conduct a reflexion session to analyze the effectiveness of the response and update the memory graph with new insights.\n",
            "\n",
            "## 3. Identify Potential Failure Modes and Their Detection Signals\n",
            "\n",
            "- **Failure Mode**: Incomplete Information\n",
            "  - **Detection Signal**: Deliberation controller flags insufficient data during initial assessment.\n",
            "\n",
            "- **Failure Mode**: Tool Misuse\n",
            "  - **Detection Signal**: Anomalies in tool outputs or unexpected results during action implementation.\n",
            "\n",
            "- **Failure Mode**: Memory Graph Retrieval Failure\n",
            "  - **Detection Signal**: Inability to retrieve relevant past incidents, leading to repeated mistakes.\n",
            "\n",
            "- **Failure Mode**: Ineffective Decision Making\n",
            "  - **Detection Signal**: High number of escalations or unresolved incidents post-action.\n",
            "\n",
            "## 4. Incorporate Memory Graph Usage\n",
            "\n",
            "- The memory graph serves as a repository for past incidents, decisions, and outcomes. It allows the deliberation controller to access historical data quickly, facilitating informed decision-making. The graph should be updated after each incident to include new learnings.\n",
            "\n",
            "## 5. Create a Checklist for Implementation in LangGraph\n",
            "\n",
            "- [ ] Define roles and responsibilities for each component.\n",
            "- [ ] Establish guidelines for tool governance.\n",
            "- [ ] Design the memory graph structure for incident data.\n",
            "- [ ] Implement the deliberation controller logic.\n",
            "- [ ] Create mechanisms for reflexion and feedback.\n",
            "- [ ] Develop incident detection and logging capabilities.\n",
            "- [ ] Integrate monitoring tools for real-time feedback.\n",
            "- [ ] Test the workflow with simulated incidents.\n",
            "- [ ] Review and iterate based on feedback and performance.\n",
            "```\n",
            "\n",
            "--- Note created ---\n",
            "note_id: note_1766218297270\n",
            "\n",
            "Memory search sample:\n",
            "[{\"note_id\": \"note_1766218297270\", \"score\": 0.598, \"title\": \"On-Call Incident Triage Workflow\", \"tags\": [\"workflow\", \"incident triage\", \"leadership\", \"engineering\", \"reflexion\", \"memory graph\", \"tool governance\", \"continuous improvement\", \"failure modes\", \"real-time feedback\"], \"content_preview\": \"{\\n  \\\"goal\\\": \\\"Now compress the workflow into: (1) 10 bullets for leadership and (2) 10 bullets for engineers. Reuse memory + rules from the previous run.\\\",\\n  \\\"final\\\": \\\"```markdown\\\\n# Workflow Design for On-Call Incident Triage\\\\n\\\\n## 1. Define the Roles of Each Component\\\\n\\\\n- **Del…\"}, {\"note_id\": \"note_1766218255928\", \"score\": 0.5541, \"title\": \"On-Call Incident Triage Workflow Review\", \"tags\": [\"workflow\", \"incident triage\", \"LangGraph\", \"reflexion\", \"memory graph\", \"tool governance\", \"deliberation controller\", \"checklist\", \"failure modes\", \"continuous improvement\"], \"content_preview\": \"{\\n  \\\"goal\\\": \\\"Design a robust agentic workflow for on-call incident triage with: deliberation controller, tool governance, memory graph usage, and reflexion. Output a practical markdown spec + checklist.\\\",\\n  \\\"final\\\": \\\"```markdown\\\\n# Workflow Design for On-Call Incident Triage\\\\n\\\\n#…\"}]\n"
          ]
        }
      ],
      "source": [
        "g = StateGraph(AgentState)\n",
        "g.add_node(\"deliberate\", deliberate)\n",
        "g.add_node(\"agent\", agent)\n",
        "g.add_node(\"tools\", tools_node)\n",
        "g.add_node(\"finalize\", finalize)\n",
        "g.add_node(\"reflect\", reflect)\n",
        "\n",
        "g.add_edge(START, \"deliberate\")\n",
        "g.add_edge(\"deliberate\", \"agent\")\n",
        "g.add_conditional_edges(\"agent\", route, [\"tools\", \"finalize\"])\n",
        "g.add_edge(\"tools\", \"agent\")\n",
        "g.add_edge(\"finalize\", \"reflect\")\n",
        "g.add_edge(\"reflect\", END)\n",
        "\n",
        "graph = g.compile(checkpointer=InMemorySaver())\n",
        "\n",
        "def run_agent(goal, constraints=None, thread_id=\"demo\"):\n",
        "    if constraints is None:\n",
        "        constraints = []\n",
        "    spec = RunSpec(\n",
        "        goal=goal,\n",
        "        constraints=constraints,\n",
        "        deliverable_format=\"markdown\",\n",
        "        must_use_memory=True,\n",
        "        max_tool_calls=6\n",
        "    ).model_dump()\n",
        "\n",
        "    return graph.invoke({\n",
        "        \"run_spec\": spec,\n",
        "        \"messages\": [],\n",
        "        \"budget_calls_remaining\": 10,\n",
        "        \"tool_calls_used\": 0,\n",
        "        \"max_tool_calls\": 6\n",
        "    }, config={\"configurable\": {\"thread_id\": thread_id}})"
      ]
    }
  ]
}