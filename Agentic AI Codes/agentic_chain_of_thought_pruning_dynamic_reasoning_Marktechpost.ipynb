{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U transformers accelerate bitsandbytes networkx scikit-learn\n",
        "\n",
        "import re, time, random, math\n",
        "import numpy as np\n",
        "import torch\n",
        "import networkx as nx\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "SEED = 7\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "SYSTEM = \"You are a careful problem solver. Keep reasoning brief and output a final numeric answer.\"\n",
        "FINAL_RE = re.compile(r\"Final:\\s*([-\\d]+(?:\\.\\d+)?)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnPMBQ6SpAi3",
        "outputId": "7f183ec4-1f07-4dc0-aa82-9ef62a02487b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(q):\n",
        "    return (\n",
        "        f\"{SYSTEM}\\n\\n\"\n",
        "        f\"Problem: {q}\\n\"\n",
        "        f\"Reasoning: (brief)\\n\"\n",
        "        f\"Final: \"\n",
        "    )\n",
        "\n",
        "def parse_final_number(text):\n",
        "    m = FINAL_RE.search(text)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    nums = re.findall(r\"[-]?\\d+(?:\\.\\d+)?\", text)\n",
        "    return nums[-1] if nums else None\n",
        "\n",
        "def is_correct(pred, gold):\n",
        "    if pred is None:\n",
        "        return 0\n",
        "    try:\n",
        "        return int(abs(float(pred) - float(gold)) < 1e-9)\n",
        "    except:\n",
        "        return int(str(pred).strip() == str(gold).strip())\n",
        "\n",
        "def tok_len(text):\n",
        "    return len(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "OuOTXi-4pATf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_paths(question, n, max_new_tokens=64, temperature=0.7, top_p=0.9):\n",
        "    prompt = make_prompt(question)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    gen_cfg = GenerationConfig(\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        num_return_sequences=n\n",
        "    )\n",
        "\n",
        "    out = model.generate(**inputs, generation_config=gen_cfg)\n",
        "    prompt_tok = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    paths = []\n",
        "    for i in range(out.shape[0]):\n",
        "        seq = out[i]\n",
        "        gen_ids = seq[prompt_tok:]\n",
        "        completion = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "        paths.append({\n",
        "            \"prompt_tokens\": int(prompt_tok),\n",
        "            \"gen_tokens\": int(gen_ids.shape[0]),\n",
        "            \"completion\": completion\n",
        "        })\n",
        "    return paths"
      ],
      "metadata": {
        "id": "wNgcFt5RpARI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consensus_strength(completions, sim_threshold=0.22):\n",
        "    if len(completions) <= 1:\n",
        "        return [0.0] * len(completions)\n",
        "\n",
        "    vec = TfidfVectorizer(ngram_range=(1,2), max_features=2500)\n",
        "    X = vec.fit_transform(completions)\n",
        "    S = cosine_similarity(X)\n",
        "\n",
        "    G = nx.Graph()\n",
        "    n = len(completions)\n",
        "    G.add_nodes_from(range(n))\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(i+1, n):\n",
        "            w = float(S[i, j])\n",
        "            if w >= sim_threshold:\n",
        "                G.add_edge(i, j, weight=w)\n",
        "\n",
        "    strength = [0.0] * n\n",
        "    for u, v, d in G.edges(data=True):\n",
        "        w = float(d.get(\"weight\", 0.0))\n",
        "        strength[u] += w\n",
        "        strength[v] += w\n",
        "\n",
        "    return strength"
      ],
      "metadata": {
        "id": "PXBx9M0HpAOl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pick_final_answer(paths):\n",
        "    answers = [parse_final_number(p[\"completion\"]) for p in paths]\n",
        "    strengths = consensus_strength([p[\"completion\"] for p in paths])\n",
        "\n",
        "    groups = {}\n",
        "    for i, a in enumerate(answers):\n",
        "        if a is None:\n",
        "            continue\n",
        "        groups.setdefault(a, {\"idx\": [], \"strength\": 0.0, \"tokens\": 0})\n",
        "        groups[a][\"idx\"].append(i)\n",
        "        groups[a][\"strength\"] += strengths[i]\n",
        "        groups[a][\"tokens\"] += paths[i][\"gen_tokens\"]\n",
        "\n",
        "    if not groups:\n",
        "        return None, {\"answers\": answers, \"strengths\": strengths}\n",
        "\n",
        "    ranked = sorted(\n",
        "        groups.items(),\n",
        "        key=lambda kv: (len(kv[1][\"idx\"]), kv[1][\"strength\"], -kv[1][\"tokens\"]),\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    best_answer = ranked[0][0]\n",
        "    best_indices = ranked[0][1][\"idx\"]\n",
        "    best_i = sorted(best_indices, key=lambda i: (paths[i][\"gen_tokens\"], -strengths[i]))[0]\n",
        "\n",
        "    return best_answer, {\"answers\": answers, \"strengths\": strengths, \"best_i\": best_i}\n",
        "\n",
        "def pruned_agent_answer(\n",
        "    question,\n",
        "    batch_size=2,\n",
        "    k_max=10,\n",
        "    max_new_tokens=64,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    stop_min_samples=4,\n",
        "    stop_ratio=0.67,\n",
        "    stop_margin=2\n",
        "):\n",
        "    paths = []\n",
        "    prompt_tokens_once = tok_len(make_prompt(question))\n",
        "    total_gen_tokens = 0\n",
        "\n",
        "    while len(paths) < k_max:\n",
        "        n = min(batch_size, k_max - len(paths))\n",
        "        new_paths = generate_paths(\n",
        "            question,\n",
        "            n=n,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p\n",
        "        )\n",
        "        paths.extend(new_paths)\n",
        "        total_gen_tokens += sum(p[\"gen_tokens\"] for p in new_paths)\n",
        "\n",
        "        if len(paths) >= stop_min_samples:\n",
        "            answers = [parse_final_number(p[\"completion\"]) for p in paths]\n",
        "            counts = {}\n",
        "            for a in answers:\n",
        "                if a is None:\n",
        "                    continue\n",
        "                counts[a] = counts.get(a, 0) + 1\n",
        "            if counts:\n",
        "                sorted_counts = sorted(counts.items(), key=lambda kv: kv[1], reverse=True)\n",
        "                top_a, top_c = sorted_counts[0]\n",
        "                second_c = sorted_counts[1][1] if len(sorted_counts) > 1 else 0\n",
        "                if top_c >= math.ceil(stop_ratio * len(paths)) and (top_c - second_c) >= stop_margin:\n",
        "                    final, dbg = pick_final_answer(paths)\n",
        "                    return {\n",
        "                        \"final\": final,\n",
        "                        \"paths\": paths,\n",
        "                        \"early_stopped_at\": len(paths),\n",
        "                        \"tokens_total\": int(prompt_tokens_once * len(paths) + total_gen_tokens),\n",
        "                        \"debug\": dbg\n",
        "                    }\n",
        "\n",
        "    final, dbg = pick_final_answer(paths)\n",
        "    return {\n",
        "        \"final\": final,\n",
        "        \"paths\": paths,\n",
        "        \"early_stopped_at\": None,\n",
        "        \"tokens_total\": int(prompt_tokens_once * len(paths) + total_gen_tokens),\n",
        "        \"debug\": dbg\n",
        "    }"
      ],
      "metadata": {
        "id": "jCx7TrRhpALt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def baseline_answer(question, k=10, max_new_tokens=64):\n",
        "    paths = generate_paths(question, n=k, max_new_tokens=max_new_tokens)\n",
        "    prompt_tokens_once = tok_len(make_prompt(question))\n",
        "    total_gen_tokens = sum(p[\"gen_tokens\"] for p in paths)\n",
        "\n",
        "    answers = [parse_final_number(p[\"completion\"]) for p in paths]\n",
        "    counts = {}\n",
        "    for a in answers:\n",
        "        if a is None:\n",
        "            continue\n",
        "        counts[a] = counts.get(a, 0) + 1\n",
        "    final = max(counts.items(), key=lambda kv: kv[1])[0] if counts else None\n",
        "\n",
        "    return {\n",
        "        \"final\": final,\n",
        "        \"paths\": paths,\n",
        "        \"tokens_total\": int(prompt_tokens_once * k + total_gen_tokens)\n",
        "    }\n",
        "\n",
        "DATA = [\n",
        "    {\"q\": \"If a store sells 3 notebooks for $12, how much does 1 notebook cost?\", \"a\": \"4\"},\n",
        "    {\"q\": \"What is 17*6?\", \"a\": \"102\"},\n",
        "    {\"q\": \"A rectangle has length 9 and width 4. What is its area?\", \"a\": \"36\"},\n",
        "    {\"q\": \"If you buy 5 apples at $2 each, how much do you pay?\", \"a\": \"10\"},\n",
        "    {\"q\": \"What is 144 divided by 12?\", \"a\": \"12\"},\n",
        "    {\"q\": \"If x=8, what is 3x+5?\", \"a\": \"29\"},\n",
        "    {\"q\": \"A jar has 30 candies. You eat 7. How many remain?\", \"a\": \"23\"},\n",
        "    {\"q\": \"If a train travels 60 km in 1.5 hours, what is its average speed (km/h)?\", \"a\": \"40\"},\n",
        "    {\"q\": \"Compute: (25 - 9) * 3\", \"a\": \"48\"},\n",
        "    {\"q\": \"What is the next number in the pattern: 2, 4, 8, 16, ?\", \"a\": \"32\"},\n",
        "]\n",
        "\n",
        "base_acc, base_tok = [], []\n",
        "prun_acc, prun_tok = [], []\n",
        "\n",
        "for item in DATA:\n",
        "    b = baseline_answer(item[\"q\"], k=8, max_new_tokens=56)\n",
        "    base_acc.append(is_correct(b[\"final\"], item[\"a\"]))\n",
        "    base_tok.append(b[\"tokens_total\"])\n",
        "\n",
        "    p = pruned_agent_answer(item[\"q\"], max_new_tokens=56)\n",
        "    prun_acc.append(is_correct(p[\"final\"], item[\"a\"]))\n",
        "    prun_tok.append(p[\"tokens_total\"])\n",
        "\n",
        "print(\"Baseline accuracy:\", float(np.mean(base_acc)))\n",
        "print(\"Baseline avg tokens:\", float(np.mean(base_tok)))\n",
        "print(\"Pruned accuracy:\", float(np.mean(prun_acc)))\n",
        "print(\"Pruned avg tokens:\", float(np.mean(prun_tok)))"
      ],
      "metadata": {
        "id": "vwIAozJaoQog"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}