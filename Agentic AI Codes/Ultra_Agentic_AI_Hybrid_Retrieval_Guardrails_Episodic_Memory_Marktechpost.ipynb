{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai openai-agents pydantic httpx beautifulsoup4 lxml scikit-learn numpy\n",
        "\n",
        "import os, re, json, time, getpass, asyncio, sqlite3, hashlib\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "\n",
        "import numpy as np\n",
        "import httpx\n",
        "from bs4 import BeautifulSoup\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from openai import AsyncOpenAI\n",
        "from agents import Agent, Runner, SQLiteSession\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY not provided.\")\n",
        "print(\"✅ OpenAI API key loaded securely.\")\n",
        "oa = AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "\n",
        "def sha1(s: str) -> str:\n",
        "    return hashlib.sha1(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "def normalize_url(u: str) -> str:\n",
        "    u = (u or \"\").strip()\n",
        "    return u.rstrip(\").,]\\\"'\")\n",
        "\n",
        "def clean_html_to_text(html: str) -> str:\n",
        "    soup = BeautifulSoup(html, \"lxml\")\n",
        "    for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        tag.decompose()\n",
        "    txt = soup.get_text(\"\\n\")\n",
        "    txt = re.sub(r\"\\n{3,}\", \"\\n\\n\", txt).strip()\n",
        "    txt = re.sub(r\"[ \\t]+\", \" \", txt)\n",
        "    return txt\n",
        "\n",
        "def chunk_text(text: str, chunk_chars: int = 1600, overlap_chars: int = 320) -> List[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    n = len(text)\n",
        "    step = max(1, chunk_chars - overlap_chars)\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        chunks.append(text[i:i + chunk_chars])\n",
        "        i += step\n",
        "    return chunks\n",
        "\n",
        "def canonical_chunk_id(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = str(s).strip()\n",
        "    s = s.strip(\"<>\\\"'()[]{}\")\n",
        "    s = s.rstrip(\".,;:\")\n",
        "    return s\n",
        "\n",
        "def inject_exec_summary_citations(exec_summary: str, citations: List[str], allowed_chunk_ids: List[str]) -> str:\n",
        "    exec_summary = exec_summary or \"\"\n",
        "    cset = []\n",
        "    for c in citations:\n",
        "        c = canonical_chunk_id(c)\n",
        "        if c and c in allowed_chunk_ids and c not in cset:\n",
        "            cset.append(c)\n",
        "        if len(cset) >= 2:\n",
        "            break\n",
        "    if len(cset) < 2:\n",
        "        for c in allowed_chunk_ids:\n",
        "            if c not in cset:\n",
        "                cset.append(c)\n",
        "            if len(cset) >= 2:\n",
        "                break\n",
        "    if len(cset) >= 2:\n",
        "        needed = [c for c in cset if c not in exec_summary]\n",
        "        if needed:\n",
        "            exec_summary = exec_summary.strip()\n",
        "            if exec_summary and not exec_summary.endswith(\".\"):\n",
        "                exec_summary += \".\"\n",
        "            exec_summary += f\" (cite: {cset[0]}) (cite: {cset[1]})\"\n",
        "    return exec_summary"
      ],
      "metadata": {
        "id": "DIP67v9i5hY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def fetch_many(urls: List[str], timeout_s: float = 25.0, per_url_char_limit: int = 60000) -> Dict[str, str]:\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (AgenticAI/4.2)\"}\n",
        "    urls = [normalize_url(u) for u in urls]\n",
        "    urls = [u for u in urls if u.startswith(\"http\")]\n",
        "    urls = list(dict.fromkeys(urls))\n",
        "    out: Dict[str, str] = {}\n",
        "    async with httpx.AsyncClient(timeout=timeout_s, follow_redirects=True, headers=headers) as client:\n",
        "        async def _one(url: str):\n",
        "            try:\n",
        "                r = await client.get(url)\n",
        "                r.raise_for_status()\n",
        "                out[url] = clean_html_to_text(r.text)[:per_url_char_limit]\n",
        "            except Exception as e:\n",
        "                out[url] = f\"__FETCH_ERROR__ {type(e).__name__}: {e}\"\n",
        "        await asyncio.gather(*[_one(u) for u in urls])\n",
        "    return out\n",
        "\n",
        "def dedupe_texts(sources: Dict[str, str]) -> Dict[str, str]:\n",
        "    seen = set()\n",
        "    out = {}\n",
        "    for url, txt in sources.items():\n",
        "        if not isinstance(txt, str) or txt.startswith(\"__FETCH_ERROR__\"):\n",
        "            continue\n",
        "        h = sha1(txt[:25000])\n",
        "        if h in seen:\n",
        "            continue\n",
        "        seen.add(h)\n",
        "        out[url] = txt\n",
        "    return out\n",
        "\n",
        "class ChunkRecord(BaseModel):\n",
        "    chunk_id: str\n",
        "    url: str\n",
        "    chunk_index: int\n",
        "    text: str\n",
        "\n",
        "class RetrievalHit(BaseModel):\n",
        "    chunk_id: str\n",
        "    url: str\n",
        "    chunk_index: int\n",
        "    score_sparse: float = 0.0\n",
        "    score_dense: float = 0.0\n",
        "    score_fused: float = 0.0\n",
        "    text: str\n",
        "\n",
        "class EvidencePack(BaseModel):\n",
        "    query: str\n",
        "    hits: List[RetrievalHit]"
      ],
      "metadata": {
        "id": "rWtR-Opo5hKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODE_DB = \"agentic_episode_memory.db\"\n",
        "\n",
        "def episode_db_init():\n",
        "    con = sqlite3.connect(EPISODE_DB)\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS episodes (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        ts INTEGER NOT NULL,\n",
        "        question TEXT NOT NULL,\n",
        "        urls_json TEXT NOT NULL,\n",
        "        retrieval_queries_json TEXT NOT NULL,\n",
        "        useful_sources_json TEXT NOT NULL\n",
        "    )\n",
        "    \"\"\")\n",
        "    con.commit()\n",
        "    con.close()\n",
        "\n",
        "def episode_store(question: str, urls: List[str], retrieval_queries: List[str], useful_sources: List[str]):\n",
        "    con = sqlite3.connect(EPISODE_DB)\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\n",
        "        \"INSERT INTO episodes(ts, question, urls_json, retrieval_queries_json, useful_sources_json) VALUES(?,?,?,?,?)\",\n",
        "        (int(time.time()), question, json.dumps(urls), json.dumps(retrieval_queries), json.dumps(useful_sources)),\n",
        "    )\n",
        "    con.commit()\n",
        "    con.close()\n",
        "\n",
        "def episode_recall(question: str, top_k: int = 2) -> List[Dict[str, Any]]:\n",
        "    con = sqlite3.connect(EPISODE_DB)\n",
        "    cur = con.cursor()\n",
        "    cur.execute(\"SELECT ts, question, urls_json, retrieval_queries_json, useful_sources_json FROM episodes ORDER BY ts DESC LIMIT 200\")\n",
        "    rows = cur.fetchall()\n",
        "    con.close()\n",
        "    q_tokens = set(re.findall(r\"[A-Za-z]{3,}\", (question or \"\").lower()))\n",
        "    scored = []\n",
        "    for ts, q2, u, rq, us in rows:\n",
        "        t2 = set(re.findall(r\"[A-Za-z]{3,}\", (q2 or \"\").lower()))\n",
        "        if not t2:\n",
        "            continue\n",
        "        score = len(q_tokens & t2) / max(1, len(q_tokens))\n",
        "        if score > 0:\n",
        "            scored.append((score, {\n",
        "                \"ts\": ts,\n",
        "                \"question\": q2,\n",
        "                \"urls\": json.loads(u),\n",
        "                \"retrieval_queries\": json.loads(rq),\n",
        "                \"useful_sources\": json.loads(us),\n",
        "            }))\n",
        "    scored.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [x[1] for x in scored[:top_k]]\n",
        "\n",
        "episode_db_init()"
      ],
      "metadata": {
        "id": "Jbwa3R665hHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HybridIndex:\n",
        "    def __init__(self):\n",
        "        self.records: List[ChunkRecord] = []\n",
        "        self.tfidf: Optional[TfidfVectorizer] = None\n",
        "        self.tfidf_mat = None\n",
        "        self.emb_mat: Optional[np.ndarray] = None\n",
        "\n",
        "    def build_sparse(self):\n",
        "        corpus = [r.text for r in self.records] if self.records else [\"\"]\n",
        "        self.tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 2), max_features=80000)\n",
        "        self.tfidf_mat = self.tfidf.fit_transform(corpus)\n",
        "\n",
        "    def search_sparse(self, query: str, k: int) -> List[Tuple[int, float]]:\n",
        "        if not self.records or self.tfidf is None or self.tfidf_mat is None:\n",
        "            return []\n",
        "        qv = self.tfidf.transform([query])\n",
        "        sims = cosine_similarity(qv, self.tfidf_mat).flatten()\n",
        "        top = np.argsort(-sims)[:k]\n",
        "        return [(int(i), float(sims[i])) for i in top]\n",
        "\n",
        "    def set_dense(self, mat: np.ndarray):\n",
        "        self.emb_mat = mat.astype(np.float32)\n",
        "\n",
        "    def search_dense(self, q_emb: np.ndarray, k: int) -> List[Tuple[int, float]]:\n",
        "        if self.emb_mat is None or not self.records:\n",
        "            return []\n",
        "        M = self.emb_mat\n",
        "        q = q_emb.astype(np.float32).reshape(1, -1)\n",
        "        M_norm = M / (np.linalg.norm(M, axis=1, keepdims=True) + 1e-9)\n",
        "        q_norm = q / (np.linalg.norm(q) + 1e-9)\n",
        "        sims = (M_norm @ q_norm.T).flatten()\n",
        "        top = np.argsort(-sims)[:k]\n",
        "        return [(int(i), float(sims[i])) for i in top]\n",
        "\n",
        "def rrf_fuse(rankings: List[List[int]], k: int = 60) -> Dict[int, float]:\n",
        "    scores: Dict[int, float] = {}\n",
        "    for r in rankings:\n",
        "        for pos, idx in enumerate(r, start=1):\n",
        "            scores[idx] = scores.get(idx, 0.0) + 1.0 / (k + pos)\n",
        "    return scores\n",
        "\n",
        "HYBRID = HybridIndex()\n",
        "ALLOWED_URLS: List[str] = []\n",
        "\n",
        "EMBED_MODEL = \"text-embedding-3-small\"\n",
        "\n",
        "async def embed_batch(texts: List[str]) -> np.ndarray:\n",
        "    resp = await oa.embeddings.create(model=EMBED_MODEL, input=texts, encoding_format=\"float\")\n",
        "    vecs = [np.array(item.embedding, dtype=np.float32) for item in resp.data]\n",
        "    return np.vstack(vecs) if vecs else np.zeros((0, 0), dtype=np.float32)\n",
        "\n",
        "async def embed_texts(texts: List[str], batch_size: int = 96, max_concurrency: int = 3) -> np.ndarray:\n",
        "    sem = asyncio.Semaphore(max_concurrency)\n",
        "    mats: List[Tuple[int, np.ndarray]] = []\n",
        "\n",
        "    async def _one(start: int, batch: List[str]):\n",
        "        async with sem:\n",
        "            m = await embed_batch(batch)\n",
        "            mats.append((start, m))\n",
        "\n",
        "    tasks = []\n",
        "    for start in range(0, len(texts), batch_size):\n",
        "        batch = [t[:7000] for t in texts[start:start + batch_size]]\n",
        "        tasks.append(_one(start, batch))\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "    mats.sort(key=lambda x: x[0])\n",
        "    emb = np.vstack([m for _, m in mats]) if mats else np.zeros((len(texts), 0), dtype=np.float32)\n",
        "    if emb.shape[0] != len(texts):\n",
        "        raise RuntimeError(f\"Embedding rows mismatch: got {emb.shape[0]} expected {len(texts)}\")\n",
        "    return emb\n",
        "\n",
        "async def embed_query(query: str) -> np.ndarray:\n",
        "    m = await embed_batch([query[:7000]])\n",
        "    return m[0] if m.shape[0] else np.zeros((0,), dtype=np.float32)\n",
        "\n",
        "async def build_index(urls: List[str], max_chunks_per_url: int = 60):\n",
        "    global ALLOWED_URLS\n",
        "    fetched = await fetch_many(urls)\n",
        "    fetched = dedupe_texts(fetched)\n",
        "\n",
        "    records: List[ChunkRecord] = []\n",
        "    allowed: List[str] = []\n",
        "\n",
        "    for url, txt in fetched.items():\n",
        "        if not isinstance(txt, str) or txt.startswith(\"__FETCH_ERROR__\"):\n",
        "            continue\n",
        "        allowed.append(url)\n",
        "        chunks = chunk_text(txt)[:max_chunks_per_url]\n",
        "        for i, ch in enumerate(chunks):\n",
        "            cid = f\"{sha1(url)}:{i}\"\n",
        "            records.append(ChunkRecord(chunk_id=cid, url=url, chunk_index=i, text=ch))\n",
        "\n",
        "    if not records:\n",
        "        err_view = {normalize_url(u): fetched.get(normalize_url(u), \"\") for u in urls}\n",
        "        raise RuntimeError(\"No sources fetched successfully.\\n\" + json.dumps(err_view, indent=2)[:4000])\n",
        "\n",
        "    ALLOWED_URLS = allowed\n",
        "    HYBRID.records = records\n",
        "    HYBRID.build_sparse()\n",
        "\n",
        "    texts = [r.text for r in HYBRID.records]\n",
        "    emb = await embed_texts(texts, batch_size=96, max_concurrency=3)\n",
        "    HYBRID.set_dense(emb)"
      ],
      "metadata": {
        "id": "XkQeSX3r5hE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_evidence_pack(query: str, sparse: List[Tuple[int,float]], dense: List[Tuple[int,float]], k: int = 10) -> EvidencePack:\n",
        "    sparse_rank = [i for i,_ in sparse]\n",
        "    dense_rank  = [i for i,_ in dense]\n",
        "    sparse_scores = {i:s for i,s in sparse}\n",
        "    dense_scores  = {i:s for i,s in dense}\n",
        "    fused = rrf_fuse([sparse_rank, dense_rank], k=60) if dense_rank else rrf_fuse([sparse_rank], k=60)\n",
        "    top = sorted(fused.keys(), key=lambda i: fused[i], reverse=True)[:k]\n",
        "\n",
        "    hits: List[RetrievalHit] = []\n",
        "    for idx in top:\n",
        "        r = HYBRID.records[idx]\n",
        "        hits.append(RetrievalHit(\n",
        "            chunk_id=r.chunk_id, url=r.url, chunk_index=r.chunk_index,\n",
        "            score_sparse=float(sparse_scores.get(idx, 0.0)),\n",
        "            score_dense=float(dense_scores.get(idx, 0.0)),\n",
        "            score_fused=float(fused.get(idx, 0.0)),\n",
        "            text=r.text\n",
        "        ))\n",
        "    return EvidencePack(query=query, hits=hits)\n",
        "\n",
        "async def gather_evidence(queries: List[str], per_query_k: int = 10, sparse_k: int = 60, dense_k: int = 60):\n",
        "    evidence: List[EvidencePack] = []\n",
        "    useful_sources_count: Dict[str, int] = {}\n",
        "    all_chunk_ids: List[str] = []\n",
        "\n",
        "    for q in queries:\n",
        "        sparse = HYBRID.search_sparse(q, k=sparse_k)\n",
        "        q_emb = await embed_query(q)\n",
        "        dense = HYBRID.search_dense(q_emb, k=dense_k)\n",
        "        pack = build_evidence_pack(q, sparse, dense, k=per_query_k)\n",
        "        evidence.append(pack)\n",
        "        for h in pack.hits[:6]:\n",
        "            useful_sources_count[h.url] = useful_sources_count.get(h.url, 0) + 1\n",
        "        for h in pack.hits:\n",
        "            all_chunk_ids.append(h.chunk_id)\n",
        "\n",
        "    useful_sources = sorted(useful_sources_count.keys(), key=lambda u: useful_sources_count[u], reverse=True)\n",
        "    all_chunk_ids = sorted(list(dict.fromkeys(all_chunk_ids)))\n",
        "    return evidence, useful_sources[:8], all_chunk_ids\n",
        "\n",
        "class Plan(BaseModel):\n",
        "    objective: str\n",
        "    subtasks: List[str]\n",
        "    retrieval_queries: List[str]\n",
        "    acceptance_checks: List[str]\n",
        "\n",
        "class UltraAnswer(BaseModel):\n",
        "    title: str\n",
        "    executive_summary: str\n",
        "    architecture: List[str]\n",
        "    retrieval_strategy: List[str]\n",
        "    agent_graph: List[str]\n",
        "    implementation_notes: List[str]\n",
        "    risks_and_limits: List[str]\n",
        "    citations: List[str]\n",
        "    sources: List[str]\n",
        "\n",
        "def normalize_answer(ans: UltraAnswer, allowed_chunk_ids: List[str]) -> UltraAnswer:\n",
        "    data = ans.model_dump()\n",
        "    data[\"citations\"] = [canonical_chunk_id(x) for x in (data.get(\"citations\") or [])]\n",
        "    data[\"citations\"] = [x for x in data[\"citations\"] if x in allowed_chunk_ids]\n",
        "    data[\"executive_summary\"] = inject_exec_summary_citations(data.get(\"executive_summary\",\"\"), data[\"citations\"], allowed_chunk_ids)\n",
        "    return UltraAnswer(**data)\n",
        "\n",
        "def validate_ultra(ans: UltraAnswer, allowed_chunk_ids: List[str]) -> None:\n",
        "    extras = [u for u in ans.sources if u not in ALLOWED_URLS]\n",
        "    if extras:\n",
        "        raise ValueError(f\"Non-allowed sources in output: {extras}\")\n",
        "\n",
        "    cset = set(ans.citations or [])\n",
        "    missing = [cid for cid in cset if cid not in set(allowed_chunk_ids)]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Citations reference unknown chunk_ids (not retrieved): {missing}\")\n",
        "\n",
        "    if len(cset) < 6:\n",
        "        raise ValueError(\"Need at least 6 distinct chunk_id citations in ultra mode.\")\n",
        "\n",
        "    es_text = ans.executive_summary or \"\"\n",
        "    es_count = sum(1 for cid in cset if cid in es_text)\n",
        "    if es_count < 2:\n",
        "        raise ValueError(\"Executive summary must include at least 2 chunk_id citations verbatim.\")\n",
        "\n",
        "PLANNER = Agent(\n",
        "    name=\"Planner\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=(\n",
        "        \"Return a technical Plan schema.\\n\"\n",
        "        \"Make 10-16 retrieval_queries.\\n\"\n",
        "        \"Acceptance must include: at least 6 citations and exec_summary contains at least 2 citations verbatim.\"\n",
        "    ),\n",
        "    output_type=Plan,\n",
        ")\n",
        "\n",
        "SYNTHESIZER = Agent(\n",
        "    name=\"Synthesizer\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=(\n",
        "        \"Return UltraAnswer schema.\\n\"\n",
        "        \"Hard constraints:\\n\"\n",
        "        \"- executive_summary MUST include at least TWO citations verbatim as: (cite: <chunk_id>).\\n\"\n",
        "        \"- citations must be chosen ONLY from ALLOWED_CHUNK_IDS list.\\n\"\n",
        "        \"- citations list must include at least 6 unique chunk_ids.\\n\"\n",
        "        \"- sources must be subset of allowed URLs.\\n\"\n",
        "    ),\n",
        "    output_type=UltraAnswer,\n",
        ")\n",
        "\n",
        "FIXER = Agent(\n",
        "    name=\"Fixer\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=(\n",
        "        \"Repair to satisfy guardrails.\\n\"\n",
        "        \"Ensure executive_summary includes at least TWO citations verbatim.\\n\"\n",
        "        \"Choose citations ONLY from ALLOWED_CHUNK_IDS list.\\n\"\n",
        "        \"Return UltraAnswer schema.\"\n",
        "    ),\n",
        "    output_type=UltraAnswer,\n",
        ")\n",
        "\n",
        "session = SQLiteSession(\"ultra_agentic_user\", \"ultra_agentic_session.db\")"
      ],
      "metadata": {
        "id": "SPLEvqKE5hCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YWLdmTZriWX",
        "outputId": "0473f415-abd0-4e85-ed2c-dbbf2f88a7ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ OpenAI API key loaded securely.\n",
            "\n",
            "TITLE:\n",
            " Advanced Agentic AI Workflow Design\n",
            "\n",
            "EXECUTIVE SUMMARY:\n",
            " This document outlines an advanced AI workflow using a production-lean strategy integrating hybrid retrieval, provenance-first citations, critique-and-repair loops, and episodic memory. Each component serves critical purposes, enhancing the AI's capabilities while also embedding resilience and accountability in the system. The OpenAI Agents SDK enables you to build agentic AI apps in a lightweight, easy-to-use package with very few abstractions (cite: <8a71742bb19231ca6c0cdc71f278b51e66f1f396:1>). It also supports built-in session memory to automatically maintain conversation history across multiple agent runs, which enhances user engagement (cite: <a1e2c5f2d56aedaabcf2e6e824f35abe1d7c25f9:10>). (cite: a1e2c5f2d56aedaabcf2e6e824f35abe1d7c25f9:6) (cite: 8a71742bb19231ca6c0cdc71f278b51e66f1f396:1)\n",
            "\n",
            "ARCHITECTURE:\n",
            "- Hybrid Retrieval System\n",
            "- Provenance-First Citations Layer\n",
            "- Critique-and-Repair Loop Module\n",
            "- Episodic Memory Component\n",
            "\n",
            "RETRIEVAL STRATEGY:\n",
            "- Utilizes a combination of real-time data and historical knowledge to improve accuracy and relevance of responses.\n",
            "- Incorporates machine learning techniques to refine retrieval based on user interactions and feedback.\n",
            "- Ensures quick adaptation through mechanisms that optimize retrieval processes as more data becomes available.\n",
            "\n",
            "AGENT GRAPH:\n",
            "- Agent A for natural language understanding\n",
            "- Agent B for retrieval tasks\n",
            "- Agent C for citation management\n",
            "- Agent D for evaluating responses\n",
            "\n",
            "IMPLEMENTATION NOTES:\n",
            "- Integrate the OpenAI Agents SDK for streamlined agent management and development (cite: <8a71742bb19231ca6c0cdc71f278b51e66f1f396:1>).\n",
            "- Utilize SQLite or Redis for session management, enabling scalable episodic memory functionality (cite: <a1e2c5f2d56aedaabcf2e6e824f35abe1d7c25f9:10>).\n",
            "- Implement rigorous testing for each layer to ensure robustness under variable scenarios.\n",
            "\n",
            "RISKS & LIMITS:\n",
            "- Dependency on the quality of data sources for hybrid retrieval, which could affect overall performance.\n",
            "- Failures in the critique-and-repair loop could lead to the propagation of errors if feedback mechanisms are weak.\n",
            "- Challenges in maintaining accurate episodic memory as information grows.\n",
            "\n",
            "CITATIONS (chunk_ids):\n",
            "- a1e2c5f2d56aedaabcf2e6e824f35abe1d7c25f9:6\n",
            "- 8a71742bb19231ca6c0cdc71f278b51e66f1f396:1\n",
            "- a1e2c5f2d56aedaabcf2e6e824f35abe1d7c25f9:0\n",
            "- 8a71742bb19231ca6c0cdc71f278b51e66f1f396:2\n",
            "- a1e2c5f2d56aedaabcf2e6e824f35abe1d7c25f9:7\n",
            "- b8a0491d2ed8578b5ed2e488f19cdf778526f735:0\n",
            "\n",
            "SOURCES:\n",
            "- https://openai.github.io/openai-agents-python/agents/\n",
            "- https://openai.github.io/openai-agents-python/\n",
            "- https://openai.github.io/openai-agents-python/running_agents/\n",
            "- https://github.com/openai/openai-agents-python\n"
          ]
        }
      ],
      "source": [
        "async def run_ultra_agentic(question: str, urls: List[str], max_repairs: int = 2) -> UltraAnswer:\n",
        "    await build_index(urls)\n",
        "    recall_hint = json.dumps(episode_recall(question, top_k=2), indent=2)[:2000]\n",
        "\n",
        "    plan_res = await Runner.run(\n",
        "        PLANNER,\n",
        "        f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\nRecall:\\n{recall_hint}\\n\",\n",
        "        session=session\n",
        "    )\n",
        "    plan: Plan = plan_res.final_output\n",
        "    queries = (plan.retrieval_queries or [])[:16]\n",
        "\n",
        "    evidence_packs, useful_sources, allowed_chunk_ids = await gather_evidence(queries)\n",
        "\n",
        "    evidence_json = json.dumps([p.model_dump() for p in evidence_packs], indent=2)[:16000]\n",
        "    allowed_chunk_ids_json = json.dumps(allowed_chunk_ids[:200], indent=2)\n",
        "\n",
        "    draft_res = await Runner.run(\n",
        "        SYNTHESIZER,\n",
        "        f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\n\"\n",
        "        f\"ALLOWED_CHUNK_IDS:\\n{allowed_chunk_ids_json}\\n\\n\"\n",
        "        f\"Evidence packs:\\n{evidence_json}\\n\\n\"\n",
        "        \"Return UltraAnswer.\",\n",
        "        session=session\n",
        "    )\n",
        "    draft = normalize_answer(draft_res.final_output, allowed_chunk_ids)\n",
        "\n",
        "    last_err = None\n",
        "    for i in range(max_repairs + 1):\n",
        "        try:\n",
        "            validate_ultra(draft, allowed_chunk_ids)\n",
        "            episode_store(question, ALLOWED_URLS, plan.retrieval_queries, useful_sources)\n",
        "            return draft\n",
        "        except Exception as e:\n",
        "            last_err = str(e)\n",
        "            if i >= max_repairs:\n",
        "                draft = normalize_answer(draft, allowed_chunk_ids)\n",
        "                validate_ultra(draft, allowed_chunk_ids)\n",
        "                return draft\n",
        "\n",
        "            fixer_res = await Runner.run(\n",
        "                FIXER,\n",
        "                f\"Question:\\n{question}\\n\\nAllowed URLs:\\n{json.dumps(ALLOWED_URLS, indent=2)}\\n\\n\"\n",
        "                f\"ALLOWED_CHUNK_IDS:\\n{allowed_chunk_ids_json}\\n\\n\"\n",
        "                f\"Guardrail error:\\n{last_err}\\n\\n\"\n",
        "                f\"Draft:\\n{json.dumps(draft.model_dump(), indent=2)[:12000]}\\n\\n\"\n",
        "                f\"Evidence packs:\\n{evidence_json}\\n\\n\"\n",
        "                \"Return corrected UltraAnswer that passes guardrails.\",\n",
        "                session=session\n",
        "            )\n",
        "            draft = normalize_answer(fixer_res.final_output, allowed_chunk_ids)\n",
        "\n",
        "    raise RuntimeError(f\"Unexpected failure: {last_err}\")\n",
        "\n",
        "question = (\n",
        "    \"Design a production-lean but advanced agentic AI workflow in Python with hybrid retrieval, \"\n",
        "    \"provenance-first citations, critique-and-repair loops, and episodic memory. \"\n",
        "    \"Explain why each layer matters, failure modes, and evaluation.\"\n",
        ")\n",
        "\n",
        "urls = [\n",
        "    \"https://openai.github.io/openai-agents-python/\",\n",
        "    \"https://openai.github.io/openai-agents-python/agents/\",\n",
        "    \"https://openai.github.io/openai-agents-python/running_agents/\",\n",
        "    \"https://github.com/openai/openai-agents-python\",\n",
        "]\n",
        "\n",
        "ans = await run_ultra_agentic(question, urls, max_repairs=2)\n",
        "\n",
        "print(\"\\nTITLE:\\n\", ans.title)\n",
        "print(\"\\nEXECUTIVE SUMMARY:\\n\", ans.executive_summary)\n",
        "print(\"\\nARCHITECTURE:\")\n",
        "for x in ans.architecture:\n",
        "    print(\"-\", x)\n",
        "print(\"\\nRETRIEVAL STRATEGY:\")\n",
        "for x in ans.retrieval_strategy:\n",
        "    print(\"-\", x)\n",
        "print(\"\\nAGENT GRAPH:\")\n",
        "for x in ans.agent_graph:\n",
        "    print(\"-\", x)\n",
        "print(\"\\nIMPLEMENTATION NOTES:\")\n",
        "for x in ans.implementation_notes:\n",
        "    print(\"-\", x)\n",
        "print(\"\\nRISKS & LIMITS:\")\n",
        "for x in ans.risks_and_limits:\n",
        "    print(\"-\", x)\n",
        "print(\"\\nCITATIONS (chunk_ids):\")\n",
        "for c in ans.citations:\n",
        "    print(\"-\", c)\n",
        "print(\"\\nSOURCES:\")\n",
        "for s in ans.sources:\n",
        "    print(\"-\", s)"
      ]
    }
  ]
}