{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U llama-index llama-index-llms-openai llama-index-embeddings-openai nest_asyncio\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter OPENAI_API_KEY: \")"
      ],
      "metadata": {
        "id": "qXArewybZb-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Document, VectorStoreIndex, Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.2)\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
        "\n",
        "texts = [\n",
        "    \"Reliable RAG systems separate retrieval, synthesis, and verification. Common failures include hallucination and shallow retrieval.\",\n",
        "    \"RAG evaluation focuses on faithfulness, answer relevancy, and retrieval quality.\",\n",
        "    \"Tool-using agents require constrained tools, validation, and self-review loops.\",\n",
        "    \"A robust workflow follows retrieve, answer, evaluate, and revise steps.\"\n",
        "]\n",
        "\n",
        "docs = [Document(text=t) for t in texts]\n",
        "index = VectorStoreIndex.from_documents(docs)\n",
        "query_engine = index.as_query_engine(similarity_top_k=4)"
      ],
      "metadata": {
        "id": "NWzYpWOPZb79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
        "\n",
        "faith_eval = FaithfulnessEvaluator(llm=Settings.llm)\n",
        "rel_eval = RelevancyEvaluator(llm=Settings.llm)\n",
        "\n",
        "def retrieve_evidence(q: str) -> str:\n",
        "    r = query_engine.query(q)\n",
        "    out = []\n",
        "    for i, n in enumerate(r.source_nodes or []):\n",
        "        out.append(f\"[{i+1}] {n.node.get_content()[:300]}\")\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "def score_answer(q: str, a: str) -> str:\n",
        "    r = query_engine.query(q)\n",
        "    ctx = [n.node.get_content() for n in r.source_nodes or []]\n",
        "    f = faith_eval.evaluate(query=q, response=a, contexts=ctx)\n",
        "    r = rel_eval.evaluate(query=q, response=a, contexts=ctx)\n",
        "    return f\"Faithfulness: {f.score}\\nRelevancy: {r.score}\""
      ],
      "metadata": {
        "id": "7Su8rbCjZb5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import ReActAgent\n",
        "from llama_index.core.workflow import Context\n",
        "\n",
        "agent = ReActAgent(\n",
        "    tools=[retrieve_evidence, score_answer],\n",
        "    llm=Settings.llm,\n",
        "    system_prompt=\"\"\"\n",
        "Always retrieve evidence first.\n",
        "Produce a structured answer.\n",
        "Evaluate the answer and revise once if scores are low.\n",
        "\"\"\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "ctx = Context(agent)"
      ],
      "metadata": {
        "id": "CnVGa6HxZb28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10dWlsLJC7n3",
        "outputId": "40f81b29-7036-4c02-fc6f-79af2cbdbda6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STREAM (agent reasoning + tool use) ---\n",
            "\n",
            "```\n",
            "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: retrieve_evidence\n",
            "Action Input: {\"question\":\"Design a reliable RAG + tool-using agent workflow and how to evaluate it.\"}\n",
            "```Thought: I have gathered relevant evidence regarding the design of a reliable RAG + tool-using agent workflow and how to evaluate it. Now, I can formulate a comprehensive answer.\n",
            "\n",
            "Answer: To design a reliable RAG (Retrieval-Augmented Generation) + tool-using agent workflow, follow these steps:\n",
            "\n",
            "1. **Clarify Task and Success Criteria**: Clearly define the task at hand and establish what success looks like for the agent.\n",
            "\n",
            "2. **Retrieve Evidence**: Implement a robust retrieval mechanism to gather relevant information from a knowledge base or external sources.\n",
            "\n",
            "3. **Produce an Answer**: Generate an answer using the retrieved evidence, ensuring to include citations or \"evidence blocks\" to support the claims made.\n",
            "\n",
            "4. **Self-Check for Faithfulness and Relevancy**: The agent should evaluate its output for faithfulness (are the statements supported by the retrieved context?) and relevancy (does the answer address the question asked?).\n",
            "\n",
            "5. **Revise Until Thresholds are Met**: If the output does not meet the established criteria, the agent should revise its answer. If it continues to fall short, escalate the issue for further review.\n",
            "\n",
            "To evaluate the workflow, consider the following metrics:\n",
            "\n",
            "- **Faithfulness/Groundedness**: Are the statements made by the agent supported by the retrieved context?\n",
            "- **Answer Relevancy**: Does the answer effectively address the question posed?\n",
            "- **Retrieval Quality**: Assess the relevance of the retrieved chunks using metrics like hit-rate, mean reciprocal rank (MRR), and recall proxies.\n",
            "\n",
            "For high-stakes applications, it is advisable to incorporate adversarial tests and human review of sampled outputs to ensure reliability and accuracy. Additionally, good practices for tool-using agents include constraining tools, validating tool input/output, logging traces, and employing a judge to detect unsupported claims. A simple iterative loop of Draft -> Judge -> Revise can help mitigate issues like hallucinations by ensuring that the judge reviews the retrieved contexts.\n",
            "\n",
            "====================================================================================================\n",
            "FINAL OUTPUT\n",
            "\n",
            "To design a reliable RAG (Retrieval-Augmented Generation) + tool-using agent workflow, follow these steps:\n",
            "\n",
            "1. **Clarify Task and Success Criteria**: Clearly define the task at hand and establish what success looks like for the agent.\n",
            "\n",
            "2. **Retrieve Evidence**: Implement a robust retrieval mechanism to gather relevant information from a knowledge base or external sources.\n",
            "\n",
            "3. **Produce an Answer**: Generate an answer using the retrieved evidence, ensuring to include citations or \"evidence blocks\" to support the claims made.\n",
            "\n",
            "4. **Self-Check for Faithfulness and Relevancy**: The agent should evaluate its output for faithfulness (are the statements supported by the retrieved context?) and relevancy (does the answer address the question asked?).\n",
            "\n",
            "5. **Revise Until Thresholds are Met**: If the output does not meet the established criteria, the agent should revise its answer. If it continues to fall short, escalate the issue for further review.\n",
            "\n",
            "To evaluate the workflow, consider the following metrics:\n",
            "\n",
            "- **Faithfulness/Groundedness**: Are the statements made by the agent supported by the retrieved context?\n",
            "- **Answer Relevancy**: Does the answer effectively address the question posed?\n",
            "- **Retrieval Quality**: Assess the relevance of the retrieved chunks using metrics like hit-rate, mean reciprocal rank (MRR), and recall proxies.\n",
            "\n",
            "For high-stakes applications, it is advisable to incorporate adversarial tests and human review of sampled outputs to ensure reliability and accuracy. Additionally, good practices for tool-using agents include constraining tools, validating tool input/output, logging traces, and employing a judge to detect unsupported claims. A simple iterative loop of Draft -> Judge -> Revise can help mitigate issues like hallucinations by ensuring that the judge reviews the retrieved contexts.\n"
          ]
        }
      ],
      "source": [
        "async def run_brief(topic: str):\n",
        "    q = f\"Design a reliable RAG + tool-using agent workflow and how to evaluate it. Topic: {topic}\"\n",
        "    handler = agent.run(q, ctx=ctx)\n",
        "    async for ev in handler.stream_events():\n",
        "        print(getattr(ev, \"delta\", \"\"), end=\"\")\n",
        "    res = await handler\n",
        "    return str(res)\n",
        "\n",
        "topic = \"RAG agent reliability and evaluation\"\n",
        "loop = asyncio.get_event_loop()\n",
        "result = loop.run_until_complete(run_brief(topic))\n",
        "\n",
        "print(\"\\n\\nFINAL OUTPUT\\n\")\n",
        "print(result)"
      ]
    }
  ]
}