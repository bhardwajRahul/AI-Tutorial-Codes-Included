{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"mlflow>=3.0.0\" scikit-learn pandas numpy matplotlib requests\n",
        "\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import socket\n",
        "import signal\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "from mlflow.models.signature import infer_signature\n",
        "\n",
        "def _is_port_open(host: str, port: int, timeout_s: float = 0.2) -> bool:\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(timeout_s)\n",
        "        return s.connect_ex((host, port)) == 0\n",
        "\n",
        "def _wait_for_http(url: str, timeout_s: int = 30) -> None:\n",
        "    t0 = time.time()\n",
        "    last_err = None\n",
        "    while time.time() - t0 < timeout_s:\n",
        "        try:\n",
        "            r = requests.get(url, timeout=1)\n",
        "            if r.status_code < 500:\n",
        "                return\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "        time.sleep(0.5)\n",
        "    raise RuntimeError(f\"Server not ready at {url}. Last error: {last_err}\")\n",
        "\n",
        "def _safe_kill(proc: subprocess.Popen):\n",
        "    if proc is None:\n",
        "        return\n",
        "    try:\n",
        "        proc.terminate()\n",
        "        try:\n",
        "            proc.wait(timeout=5)\n",
        "        except subprocess.TimeoutExpired:\n",
        "            proc.kill()\n",
        "    except Exception:\n",
        "        pass"
      ],
      "metadata": {
        "id": "5KJqMZfmTiRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = Path(\"/content/mlflow_colab_demo\").resolve()\n",
        "BACKEND_DB = BASE_DIR / \"mlflow.db\"\n",
        "ARTIFACT_ROOT = BASE_DIR / \"mlartifacts\"\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "os.makedirs(ARTIFACT_ROOT, exist_ok=True)\n",
        "\n",
        "HOST = \"127.0.0.1\"\n",
        "PORT = 5000\n",
        "TRACKING_URI = f\"http://{HOST}:{PORT}\"\n",
        "\n",
        "if _is_port_open(HOST, PORT):\n",
        "    for p in range(5001, 5015):\n",
        "        if not _is_port_open(HOST, p):\n",
        "            PORT = p\n",
        "            TRACKING_URI = f\"http://{HOST}:{PORT}\"\n",
        "            break\n",
        "\n",
        "print(\"Using TRACKING_URI:\", TRACKING_URI)\n",
        "print(\"Backend DB:\", BACKEND_DB)\n",
        "print(\"Artifact root:\", ARTIFACT_ROOT)\n",
        "\n",
        "server_cmd = [\n",
        "    \"mlflow\",\n",
        "    \"server\",\n",
        "    \"--host\", HOST,\n",
        "    \"--port\", str(PORT),\n",
        "    \"--backend-store-uri\", f\"sqlite:///{BACKEND_DB}\",\n",
        "    \"--default-artifact-root\", str(ARTIFACT_ROOT),\n",
        "]\n",
        "\n",
        "mlflow_server = subprocess.Popen(\n",
        "    server_cmd,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "_wait_for_http(TRACKING_URI, timeout_s=45)\n",
        "mlflow.set_tracking_uri(TRACKING_URI)\n",
        "print(\"MLflow server is up.\")\n",
        "\n",
        "EXPERIMENT_NAME = \"colab-advanced-mlflow-sklearn\"\n",
        "mlflow.set_experiment(EXPERIMENT_NAME)"
      ],
      "metadata": {
        "id": "obCclt_gTiMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_breast_cancer(as_frame=True)\n",
        "df = data.frame.copy()\n",
        "target_col = \"target\"\n",
        "\n",
        "X = df.drop(columns=[target_col])\n",
        "y = df[target_col].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "mlflow.sklearn.autolog(\n",
        "    log_input_examples=False,\n",
        "    log_model_signatures=False,\n",
        "    silent=True\n",
        ")\n",
        "\n",
        "C_VALUES = [0.01, 0.1, 1.0, 3.0]\n",
        "SOLVERS = [\"liblinear\", \"lbfgs\"]\n",
        "\n",
        "best = {\"auc\": -1.0, \"run_id\": None, \"params\": None}"
      ],
      "metadata": {
        "id": "jGuaArF7TiDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"parent_sweep_run\") as parent_run:\n",
        "    mlflow.log_param(\"dataset\", \"sklearn_breast_cancer\")\n",
        "    mlflow.log_param(\"n_features\", X_train.shape[1])\n",
        "    mlflow.log_param(\"n_train\", X_train.shape[0])\n",
        "    mlflow.log_param(\"n_test\", X_test.shape[0])\n",
        "\n",
        "    for C in C_VALUES:\n",
        "        for solver in SOLVERS:\n",
        "            with mlflow.start_run(run_name=f\"child_C={C}_solver={solver}\", nested=True) as child_run:\n",
        "                pipe = Pipeline([\n",
        "                    (\"scaler\", StandardScaler()),\n",
        "                    (\"clf\", LogisticRegression(\n",
        "                        C=C,\n",
        "                        solver=solver,\n",
        "                        penalty=\"l2\",\n",
        "                        max_iter=2000,\n",
        "                        random_state=42\n",
        "                    ))\n",
        "                ])\n",
        "\n",
        "                pipe.fit(X_train, y_train)\n",
        "                proba = pipe.predict_proba(X_test)[:, 1]\n",
        "                pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "                auc = roc_auc_score(y_test, proba)\n",
        "                acc = accuracy_score(y_test, pred)\n",
        "                prec = precision_score(y_test, pred, zero_division=0)\n",
        "                rec = recall_score(y_test, pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, pred, zero_division=0)\n",
        "\n",
        "                mlflow.log_metrics({\n",
        "                    \"test_auc\": float(auc),\n",
        "                    \"test_accuracy\": float(acc),\n",
        "                    \"test_precision\": float(prec),\n",
        "                    \"test_recall\": float(rec),\n",
        "                    \"test_f1\": float(f1),\n",
        "                })\n",
        "\n",
        "                cm = confusion_matrix(y_test, pred)\n",
        "                disp = ConfusionMatrixDisplay(cm, display_labels=data.target_names)\n",
        "                fig, ax = plt.subplots(figsize=(5, 4))\n",
        "                disp.plot(ax=ax, values_format=\"d\")\n",
        "                ax.set_title(f\"Confusion Matrix (C={C}, solver={solver})\")\n",
        "                cm_path = BASE_DIR / \"confusion_matrix.png\"\n",
        "                fig.tight_layout()\n",
        "                fig.savefig(cm_path, dpi=140)\n",
        "                plt.close(fig)\n",
        "                mlflow.log_artifact(str(cm_path), artifact_path=\"diagnostics\")\n",
        "\n",
        "                if auc > best[\"auc\"]:\n",
        "                    best.update({\n",
        "                        \"auc\": float(auc),\n",
        "                        \"run_id\": child_run.info.run_id,\n",
        "                        \"params\": {\"C\": C, \"solver\": solver}\n",
        "                    })\n",
        "\n",
        "    mlflow.log_dict(best, \"best_run_summary.json\")\n",
        "    print(\"Best config:\", best)"
      ],
      "metadata": {
        "id": "dDvfeXv3TiAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_C = best[\"params\"][\"C\"]\n",
        "best_solver = best[\"params\"][\"solver\"]\n",
        "\n",
        "final_pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        C=best_C,\n",
        "        solver=best_solver,\n",
        "        penalty=\"l2\",\n",
        "        max_iter=2000,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "with mlflow.start_run(run_name=\"final_model_run\") as final_run:\n",
        "    final_pipe.fit(X_train, y_train)\n",
        "\n",
        "    proba = final_pipe.predict_proba(X_test)[:, 1]\n",
        "    pred = (proba >= 0.5).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        \"test_auc\": float(roc_auc_score(y_test, proba)),\n",
        "        \"test_accuracy\": float(accuracy_score(y_test, pred)),\n",
        "        \"test_precision\": float(precision_score(y_test, pred, zero_division=0)),\n",
        "        \"test_recall\": float(recall_score(y_test, pred, zero_division=0)),\n",
        "        \"test_f1\": float(f1_score(y_test, pred, zero_division=0)),\n",
        "    }\n",
        "    mlflow.log_metrics(metrics)\n",
        "    mlflow.log_params({\"C\": best_C, \"solver\": best_solver, \"model\": \"LogisticRegression+StandardScaler\"})\n",
        "\n",
        "    input_example = X_test.iloc[:5].copy()\n",
        "    signature = infer_signature(input_example, final_pipe.predict_proba(input_example)[:, 1])\n",
        "\n",
        "    model_info = mlflow.sklearn.log_model(\n",
        "        sk_model=final_pipe,\n",
        "        artifact_path=\"model\",\n",
        "        signature=signature,\n",
        "        input_example=input_example,\n",
        "        registered_model_name=None,\n",
        "    )\n",
        "\n",
        "    print(\"Final run_id:\", final_run.info.run_id)\n",
        "    print(\"Logged model URI:\", model_info.model_uri)\n",
        "\n",
        "    eval_df = X_test.copy()\n",
        "    eval_df[\"label\"] = y_test.values\n",
        "\n",
        "    eval_result = mlflow.models.evaluate(\n",
        "        model=model_info.model_uri,\n",
        "        data=eval_df,\n",
        "        targets=\"label\",\n",
        "        model_type=\"classifier\",\n",
        "        evaluators=\"default\",\n",
        "    )\n",
        "\n",
        "    eval_summary = {\n",
        "        \"metrics\": {k: float(v) if isinstance(v, (int, float, np.floating)) else str(v)\n",
        "                    for k, v in eval_result.metrics.items()},\n",
        "        \"artifacts\": {k: str(v) for k, v in eval_result.artifacts.items()},\n",
        "    }\n",
        "    mlflow.log_dict(eval_summary, \"evaluation/eval_summary.json\")"
      ],
      "metadata": {
        "id": "-rlmM2WQTh9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SERVE_PORT = 6000\n",
        "if _is_port_open(HOST, SERVE_PORT):\n",
        "    for p in range(6001, 6020):\n",
        "        if not _is_port_open(HOST, p):\n",
        "            SERVE_PORT = p\n",
        "            break\n",
        "\n",
        "MODEL_URI = model_info.model_uri\n",
        "\n",
        "serve_cmd = [\n",
        "    \"mlflow\", \"models\", \"serve\",\n",
        "    \"-m\", MODEL_URI,\n",
        "    \"-p\", str(SERVE_PORT),\n",
        "    \"--host\", HOST,\n",
        "    \"--env-manager\", \"local\"\n",
        "]\n",
        "\n",
        "mlflow_serve = subprocess.Popen(\n",
        "    serve_cmd,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "serve_url = f\"http://{HOST}:{SERVE_PORT}/invocations\"\n",
        "_wait_for_http(f\"http://{HOST}:{SERVE_PORT}\", timeout_s=60)\n",
        "print(\"Model server is up at:\", serve_url)\n",
        "\n",
        "payload = {\n",
        "    \"dataframe_split\": {\n",
        "        \"columns\": list(X_test.columns),\n",
        "        \"data\": X_test.iloc[:3].values.tolist()\n",
        "    }\n",
        "}\n",
        "\n",
        "r = requests.post(\n",
        "    serve_url,\n",
        "    headers={\"Content-Type\": \"application/json\"},\n",
        "    data=json.dumps(payload),\n",
        "    timeout=10\n",
        ")\n",
        "print(\"Serve status:\", r.status_code)\n",
        "print(\"Predictions (probabilities or outputs):\", r.text)\n",
        "\n",
        "print(\"\\nOpen the MLflow UI by visiting:\", TRACKING_URI)\n",
        "print(\"Artifacts are stored under:\", ARTIFACT_ROOT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RJR64Ofc53w",
        "outputId": "f11776f8-9f59-48bb-f468-54ec185e1457"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing TRACKING_URI: http://127.0.0.1:5000\n",
            "Backend DB: /content/mlflow_colab_demo/mlflow.db\n",
            "Artifact root: /content/mlflow_colab_demo/mlartifacts\n",
            "MLflow server is up.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/06 18:51:43 INFO mlflow.tracking.fluent: Experiment with name 'colab-advanced-mlflow-sklearn' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸƒ View run child_C=0.01_solver=liblinear at: http://127.0.0.1:5000/#/experiments/1/runs/5369f6209af84e018198717442c6118a\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "ğŸƒ View run child_C=0.01_solver=lbfgs at: http://127.0.0.1:5000/#/experiments/1/runs/106619f400eb45568a990569fab72ff0\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "ğŸƒ View run child_C=0.1_solver=liblinear at: http://127.0.0.1:5000/#/experiments/1/runs/f48f1af6bf1642098bf31d97bd6cc4db\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "ğŸƒ View run child_C=0.1_solver=lbfgs at: http://127.0.0.1:5000/#/experiments/1/runs/ad1725fcd44b4aacac88f81af05124ad\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "ğŸƒ View run child_C=1.0_solver=liblinear at: http://127.0.0.1:5000/#/experiments/1/runs/34f337f003dd45109376b6e58ffa2dff\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "ğŸƒ View run child_C=1.0_solver=lbfgs at: http://127.0.0.1:5000/#/experiments/1/runs/2fd741a1bd444a37b511460ba53798d1\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "ğŸƒ View run child_C=3.0_solver=liblinear at: http://127.0.0.1:5000/#/experiments/1/runs/df1645cc6d034e9197fc841d7e126a18\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "ğŸƒ View run child_C=3.0_solver=lbfgs at: http://127.0.0.1:5000/#/experiments/1/runs/d2e8347156d94e708753db125edb9399\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "Best config: {'auc': 0.996031746031746, 'run_id': 'f48f1af6bf1642098bf31d97bd6cc4db', 'params': {'C': 0.1, 'solver': 'liblinear'}}\n",
            "ğŸƒ View run parent_sweep_run at: http://127.0.0.1:5000/#/experiments/1/runs/26bd8d826dd946879e6698e105beea7d\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2026/02/06 18:52:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/models/model.py:1209: FutureWarning: Saving scikit-learn models in the pickle or cloudpickle format requires exercising caution because these formats rely on Python's object serialization mechanism, which can execute arbitrary code during deserialization.The recommended safe alternative is the 'skops' format.\n",
            "  flavor.save_model(path=local_path, mlflow_model=mlflow_model, **kwargs)\n",
            "2026/02/06 18:52:44 INFO mlflow.tracking.fluent: Active model is set to the logged model with ID: m-0e957a3fb4e847ecad4e7a5d31f8b60a\n",
            "2026/02/06 18:52:44 INFO mlflow.tracking.fluent: Use `mlflow.set_active_model` to set the active model to a different one if needed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final run_id: 212e682de11045dcb4cc0916d21a810d\n",
            "Logged model URI: models:/m-0e957a3fb4e847ecad4e7a5d31f8b60a\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
            "  warnings.warn(\n",
            "2026/02/06 18:52:44 WARNING mlflow.models.evaluation.evaluators.classifier: According to the evaluation dataset label values, the model type looks like None, but you specified model type 'classifier'. Please verify that you set the `model_type` and `dataset` arguments correctly.\n",
            "2026/02/06 18:52:45 INFO mlflow.models.evaluation.evaluators.classifier: The evaluation dataset is inferred as binary dataset, positive label is 1, negative label is 0.\n",
            "2026/02/06 18:52:45 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
            "2026/02/06 18:52:48 INFO mlflow.models.evaluation.evaluators.shap: Shap explainer PermutationExplainer is used.\n",
            "/usr/local/lib/python3.12/dist-packages/mlflow/models/evaluation/evaluators/shap.py:256: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
            "  shap.summary_plot(shap_values, show=False, color_bar=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸƒ View run final_model_run at: http://127.0.0.1:5000/#/experiments/1/runs/212e682de11045dcb4cc0916d21a810d\n",
            "ğŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/1\n",
            "Model server is up at: http://127.0.0.1:6000/invocations\n",
            "Serve status: 200\n",
            "Predictions (probabilities or outputs): {\"predictions\": [0, 1, 0]}\n",
            "\n",
            "Open the MLflow UI by visiting: http://127.0.0.1:5000\n",
            "Artifacts are stored under: /content/mlflow_colab_demo/mlartifacts\n"
          ]
        }
      ]
    }
  ]
}