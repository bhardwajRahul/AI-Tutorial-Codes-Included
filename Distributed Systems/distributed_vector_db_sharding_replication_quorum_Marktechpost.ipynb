{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os, sys, time, math, json, random, threading, hashlib\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Optional, Any\n",
        "import numpy as np\n",
        "import subprocess\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"faiss-cpu\", \"xxhash\"])\n",
        "import faiss\n",
        "import xxhash\n",
        "\n",
        "def now_ms() -> int:\n",
        "    return int(time.time() * 1000)\n",
        "\n",
        "def stable_hash64(s: str) -> int:\n",
        "    return xxhash.xxh3_64_intdigest(s)\n",
        "\n",
        "def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
        "    n = np.linalg.norm(x, axis=-1, keepdims=True)\n",
        "    return x / np.clip(n, eps, None)\n",
        "\n",
        "def merge_topk(results: List[Tuple[np.ndarray, np.ndarray]], k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    scores = np.concatenate([r[0] for r in results], axis=0)\n",
        "    ids = np.concatenate([r[1] for r in results], axis=0)\n",
        "    m = ids >= 0\n",
        "    scores, ids = scores[m], ids[m]\n",
        "    if len(ids) == 0:\n",
        "        return np.array([], dtype=np.float32), np.array([], dtype=np.int64)\n",
        "    idx = np.argpartition(-scores, min(k, len(scores)-1))[:k]\n",
        "    idx = idx[np.argsort(-scores[idx])]\n",
        "    return scores[idx].astype(np.float32), ids[idx].astype(np.int64)\n",
        "\n",
        "def sha1_bytes(b: bytes) -> str:\n",
        "    return hashlib.sha1(b).hexdigest()\n",
        "\n",
        "def deterministic_rng(seed: int) -> np.random.Generator:\n",
        "    return np.random.default_rng(seed)\n",
        "\n",
        "@dataclass\n",
        "class VectorRecord:\n",
        "    key: str\n",
        "    vec: np.ndarray\n",
        "    payload: Dict[str, Any]\n",
        "    version: int\n",
        "    ts_ms: int\n",
        "\n",
        "@dataclass\n",
        "class NodeStatus:\n",
        "    node_id: str\n",
        "    is_up: bool = True\n",
        "    last_seen_ms: int = field(default_factory=now_ms)"
      ],
      "metadata": {
        "id": "bKQ2ukeTGVH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShardNode:\n",
        "    def __init__(self, node_id: str, dim: int, metric: str = \"cosine\"):\n",
        "        self.node_id = node_id\n",
        "        self.dim = dim\n",
        "        self.metric = metric\n",
        "        self.status = NodeStatus(node_id=node_id)\n",
        "        self._store: Dict[str, VectorRecord] = {}\n",
        "        self._next_int_id = 0\n",
        "        self._key_to_int: Dict[str, int] = {}\n",
        "        self._int_to_key: Dict[int, str] = {}\n",
        "        if metric == \"cosine\":\n",
        "            self.index = faiss.IndexFlatIP(dim)\n",
        "        elif metric == \"l2\":\n",
        "            self.index = faiss.IndexFlatL2(dim)\n",
        "        else:\n",
        "            raise ValueError(\"metric must be 'cosine' or 'l2'\")\n",
        "        self._vectors: List[np.ndarray] = []\n",
        "        self._lock = threading.RLock()\n",
        "\n",
        "    def set_up(self, up: bool):\n",
        "        with self._lock:\n",
        "            self.status.is_up = up\n",
        "            self.status.last_seen_ms = now_ms()\n",
        "\n",
        "    def is_up(self) -> bool:\n",
        "        return self.status.is_up\n",
        "\n",
        "    def _ensure_up(self):\n",
        "        if not self.status.is_up:\n",
        "            raise RuntimeError(f\"Node {self.node_id} is DOWN\")\n",
        "\n",
        "    def upsert(self, rec: VectorRecord) -> bool:\n",
        "        with self._lock:\n",
        "            self._ensure_up()\n",
        "            prev = self._store.get(rec.key)\n",
        "            if prev is not None:\n",
        "                if (rec.version, rec.ts_ms) <= (prev.version, prev.ts_ms):\n",
        "                    return False\n",
        "            v = rec.vec.astype(np.float32)\n",
        "            if v.shape != (self.dim,):\n",
        "                raise ValueError(f\"vec must be shape ({self.dim},), got {v.shape}\")\n",
        "            if self.metric == \"cosine\":\n",
        "                v = l2_normalize(v)\n",
        "            self._store[rec.key] = VectorRecord(\n",
        "                key=rec.key,\n",
        "                vec=v,\n",
        "                payload=dict(rec.payload),\n",
        "                version=int(rec.version),\n",
        "                ts_ms=int(rec.ts_ms),\n",
        "            )\n",
        "            if rec.key not in self._key_to_int:\n",
        "                int_id = self._next_int_id\n",
        "                self._next_int_id += 1\n",
        "                self._key_to_int[rec.key] = int_id\n",
        "                self._int_to_key[int_id] = rec.key\n",
        "                self._vectors.append(v)\n",
        "                self.index.add(v.reshape(1, -1))\n",
        "            else:\n",
        "                int_id = self._key_to_int[rec.key]\n",
        "                self._vectors[int_id] = v\n",
        "                self._rebuild_index()\n",
        "            return True\n",
        "\n",
        "    def _rebuild_index(self):\n",
        "        self.index.reset()\n",
        "        if len(self._vectors) == 0:\n",
        "            return\n",
        "        mat = np.stack(self._vectors, axis=0).astype(np.float32)\n",
        "        self.index.add(mat)\n",
        "\n",
        "    def get(self, key: str) -> Optional[VectorRecord]:\n",
        "        with self._lock:\n",
        "            self._ensure_up()\n",
        "            rec = self._store.get(key)\n",
        "            if rec is None:\n",
        "                return None\n",
        "            return VectorRecord(\n",
        "                key=rec.key,\n",
        "                vec=rec.vec.copy(),\n",
        "                payload=dict(rec.payload),\n",
        "                version=rec.version,\n",
        "                ts_ms=rec.ts_ms,\n",
        "            )\n",
        "\n",
        "    def batch_get(self, keys: List[str]) -> Dict[str, VectorRecord]:\n",
        "        out = {}\n",
        "        with self._lock:\n",
        "            self._ensure_up()\n",
        "            for k in keys:\n",
        "                r = self._store.get(k)\n",
        "                if r is not None:\n",
        "                    out[k] = VectorRecord(\n",
        "                        key=r.key, vec=r.vec.copy(), payload=dict(r.payload),\n",
        "                        version=r.version, ts_ms=r.ts_ms\n",
        "                    )\n",
        "        return out\n",
        "\n",
        "    def search(self, q: np.ndarray, k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        with self._lock:\n",
        "            self._ensure_up()\n",
        "            if len(self._vectors) == 0:\n",
        "                return np.array([], dtype=np.float32), np.array([], dtype=np.int64)\n",
        "            qq = q.astype(np.float32).reshape(1, -1)\n",
        "            if qq.shape != (1, self.dim):\n",
        "                raise ValueError(f\"query must be shape ({self.dim},), got {q.shape}\")\n",
        "            if self.metric == \"cosine\":\n",
        "                qq = l2_normalize(qq)\n",
        "            D, I = self.index.search(qq, k)\n",
        "            D = D.reshape(-1)\n",
        "            I = I.reshape(-1)\n",
        "            node_tag = stable_hash64(self.node_id) & 0xFFFFFFFF\n",
        "            global_ids = np.array([(node_tag << 32) | (int(i) & 0xFFFFFFFF) for i in I], dtype=np.int64)\n",
        "            return D.astype(np.float32), global_ids\n",
        "\n",
        "    def decode_global_id(self, gid: int) -> Optional[str]:\n",
        "        with self._lock:\n",
        "            node_tag = stable_hash64(self.node_id) & 0xFFFFFFFF\n",
        "            if ((gid >> 32) & 0xFFFFFFFF) != node_tag:\n",
        "                return None\n",
        "            int_id = int(gid & 0xFFFFFFFF)\n",
        "            return self._int_to_key.get(int_id)\n",
        "\n",
        "    def digest(self) -> str:\n",
        "        with self._lock:\n",
        "            self._ensure_up()\n",
        "            items = sorted((k, r.version, r.ts_ms) for k, r in self._store.items())\n",
        "            b = json.dumps(items, separators=(\",\", \":\")).encode(\"utf-8\")\n",
        "            return sha1_bytes(b)\n",
        "\n",
        "    def diff_keys(self, other_meta: Dict[str, Tuple[int, int]]) -> List[str]:\n",
        "        with self._lock:\n",
        "            self._ensure_up()\n",
        "            need = []\n",
        "            for k, (v, t) in other_meta.items():\n",
        "                mine = self._store.get(k)\n",
        "                if mine is None or (mine.version, mine.ts_ms) < (v, t):\n",
        "                    need.append(k)\n",
        "            return need\n",
        "\n",
        "    def meta_snapshot(self) -> Dict[str, Tuple[int, int]]:\n",
        "        with self._lock:\n",
        "            self._ensure_up()\n",
        "            return {k: (r.version, r.ts_ms) for k, r in self._store.items()}"
      ],
      "metadata": {
        "id": "vZ_-Iq-eGVFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConsistentHashRing:\n",
        "    def __init__(self, node_ids: List[str], vnodes: int = 64):\n",
        "        self.vnodes = int(vnodes)\n",
        "        self._ring: List[Tuple[int, str]] = []\n",
        "        for nid in node_ids:\n",
        "            for v in range(self.vnodes):\n",
        "                h = stable_hash64(f\"{nid}::vn{v}\")\n",
        "                self._ring.append((h, nid))\n",
        "        self._ring.sort(key=lambda x: x[0])\n",
        "        self._hashes = [h for h, _ in self._ring]\n",
        "\n",
        "    def owners(self, key: str, rf: int) -> List[str]:\n",
        "        h = stable_hash64(key)\n",
        "        import bisect\n",
        "        i = bisect.bisect_left(self._hashes, h)\n",
        "        if i == len(self._hashes):\n",
        "            i = 0\n",
        "        owners = []\n",
        "        seen = set()\n",
        "        j = i\n",
        "        while len(owners) < rf and len(seen) < len(set(n for _, n in self._ring)):\n",
        "            nid = self._ring[j][1]\n",
        "            if nid not in seen:\n",
        "                owners.append(nid)\n",
        "                seen.add(nid)\n",
        "            j = (j + 1) % len(self._ring)\n",
        "        return owners"
      ],
      "metadata": {
        "id": "h7d2amLIGVDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DistributedVectorDB:\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        node_ids: List[str],\n",
        "        rf: int = 2,\n",
        "        w: int = 2,\n",
        "        r: int = 1,\n",
        "        vnodes: int = 64,\n",
        "        metric: str = \"cosine\",\n",
        "        anti_entropy_interval_s: float = 2.0,\n",
        "        anti_entropy_enabled: bool = True,\n",
        "        seed: int = 7,\n",
        "    ):\n",
        "        if rf < 1:\n",
        "            raise ValueError(\"rf must be >= 1\")\n",
        "        if w < 1 or r < 1:\n",
        "            raise ValueError(\"w and r must be >= 1\")\n",
        "        if w > rf or r > rf:\n",
        "            raise ValueError(\"w and r must be <= rf\")\n",
        "\n",
        "        self.dim = int(dim)\n",
        "        self.node_ids = list(node_ids)\n",
        "        self.rf = int(rf)\n",
        "        self.w = int(w)\n",
        "        self.r = int(r)\n",
        "        self.metric = metric\n",
        "        self.ring = ConsistentHashRing(self.node_ids, vnodes=vnodes)\n",
        "        self.nodes: Dict[str, ShardNode] = {nid: ShardNode(nid, dim=self.dim, metric=metric) for nid in self.node_ids}\n",
        "\n",
        "        self._lamport: Dict[str, int] = {}\n",
        "        self._lamport_lock = threading.Lock()\n",
        "\n",
        "        self._rng = deterministic_rng(seed)\n",
        "\n",
        "        self._anti_entropy_interval_s = float(anti_entropy_interval_s)\n",
        "        self._anti_entropy_enabled = bool(anti_entropy_enabled)\n",
        "        self._stop = threading.Event()\n",
        "        self._bg = None\n",
        "        if self._anti_entropy_enabled:\n",
        "            self._bg = threading.Thread(target=self._anti_entropy_loop, daemon=True)\n",
        "            self._bg.start()\n",
        "\n",
        "    def stop(self):\n",
        "        self._stop.set()\n",
        "        if self._bg is not None:\n",
        "            self._bg.join(timeout=2)\n",
        "\n",
        "    def set_node_up(self, node_id: str, up: bool):\n",
        "        self.nodes[node_id].set_up(up)\n",
        "\n",
        "    def _owners(self, key: str) -> List[ShardNode]:\n",
        "        return [self.nodes[nid] for nid in self.ring.owners(key, self.rf)]\n",
        "\n",
        "    def _next_version(self, key: str) -> int:\n",
        "        with self._lamport_lock:\n",
        "            v = self._lamport.get(key, 0) + 1\n",
        "            self._lamport[key] = v\n",
        "            return v\n",
        "\n",
        "    def upsert(self, key: str, vec: np.ndarray, payload: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:\n",
        "        payload = payload or {}\n",
        "        owners = self._owners(key)\n",
        "\n",
        "        rec = VectorRecord(\n",
        "            key=key,\n",
        "            vec=np.asarray(vec, dtype=np.float32),\n",
        "            payload=dict(payload),\n",
        "            version=self._next_version(key),\n",
        "            ts_ms=now_ms(),\n",
        "        )\n",
        "\n",
        "        acks = 0\n",
        "        errors = []\n",
        "        applied_nodes = []\n",
        "        for n in owners:\n",
        "            try:\n",
        "                ok = n.upsert(rec)\n",
        "                if ok:\n",
        "                    acks += 1\n",
        "                    applied_nodes.append(n.node_id)\n",
        "            except Exception as e:\n",
        "                errors.append(f\"{n.node_id}: {e}\")\n",
        "\n",
        "        return {\n",
        "            \"key\": key,\n",
        "            \"owners\": [n.node_id for n in owners],\n",
        "            \"acks\": acks,\n",
        "            \"required_w\": self.w,\n",
        "            \"applied_nodes\": applied_nodes,\n",
        "            \"ok\": (acks >= self.w),\n",
        "            \"errors\": errors,\n",
        "            \"version\": rec.version,\n",
        "            \"ts_ms\": rec.ts_ms,\n",
        "        }\n",
        "\n",
        "    def get(self, key: str) -> Dict[str, Any]:\n",
        "        owners = self._owners(key)\n",
        "        responses: List[Tuple[str, Optional[VectorRecord], Optional[str]]] = []\n",
        "        good = 0\n",
        "\n",
        "        for n in owners:\n",
        "            try:\n",
        "                rec = n.get(key)\n",
        "                responses.append((n.node_id, rec, None))\n",
        "                if rec is not None:\n",
        "                    good += 1\n",
        "            except Exception as e:\n",
        "                responses.append((n.node_id, None, str(e)))\n",
        "\n",
        "        up_contacts = sum(1 for _, _, err in responses if err is None)\n",
        "        ok = up_contacts >= self.r\n",
        "\n",
        "        latest = None\n",
        "        for _, rec, err in responses:\n",
        "            if err is not None or rec is None:\n",
        "                continue\n",
        "            if latest is None or (rec.version, rec.ts_ms) > (latest.version, latest.ts_ms):\n",
        "                latest = rec\n",
        "\n",
        "        repaired = []\n",
        "        if latest is not None:\n",
        "            for nid, rec, err in responses:\n",
        "                if err is not None:\n",
        "                    continue\n",
        "                if rec is None or (rec.version, rec.ts_ms) < (latest.version, latest.ts_ms):\n",
        "                    try:\n",
        "                        self.nodes[nid].upsert(latest)\n",
        "                        repaired.append(nid)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "        return {\n",
        "            \"key\": key,\n",
        "            \"owners\": [n.node_id for n in owners],\n",
        "            \"required_r\": self.r,\n",
        "            \"up_contacts\": up_contacts,\n",
        "            \"ok\": ok,\n",
        "            \"latest\": None if latest is None else {\n",
        "                \"version\": latest.version,\n",
        "                \"ts_ms\": latest.ts_ms,\n",
        "                \"payload\": latest.payload,\n",
        "                \"vec_preview\": latest.vec[:6].tolist(),\n",
        "            },\n",
        "            \"read_repair_applied_to\": repaired,\n",
        "            \"responses\": [\n",
        "                {\n",
        "                    \"node\": nid,\n",
        "                    \"has\": (rec is not None),\n",
        "                    \"version\": None if rec is None else rec.version,\n",
        "                    \"ts_ms\": None if rec is None else rec.ts_ms,\n",
        "                    \"err\": err,\n",
        "                }\n",
        "                for nid, rec, err in responses\n",
        "            ],\n",
        "        }"
      ],
      "metadata": {
        "id": "VS0xHYpMGVAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def knn_search(self, q: np.ndarray, k: int = 5, fanout: Optional[int] = None) -> Dict[str, Any]:\n",
        "        q = np.asarray(q, dtype=np.float32).reshape(-1)\n",
        "        if q.shape != (self.dim,):\n",
        "            raise ValueError(f\"q must be shape ({self.dim},), got {q.shape}\")\n",
        "\n",
        "        node_list = list(self.nodes.values())\n",
        "        if fanout is not None:\n",
        "            fanout = min(int(fanout), len(node_list))\n",
        "            node_list = list(self._rng.choice(node_list, size=fanout, replace=False))\n",
        "\n",
        "        partials = []\n",
        "        per_node = []\n",
        "        for n in node_list:\n",
        "            try:\n",
        "                D, I = n.search(q, k)\n",
        "                partials.append((D, I))\n",
        "                per_node.append({\"node\": n.node_id, \"count\": int(len(I)), \"ok\": True})\n",
        "            except Exception as e:\n",
        "                per_node.append({\"node\": n.node_id, \"count\": 0, \"ok\": False, \"err\": str(e)})\n",
        "\n",
        "        scores, gids = merge_topk(partials, k=k)\n",
        "\n",
        "        hits = []\n",
        "        for score, gid in zip(scores.tolist(), gids.tolist()):\n",
        "            key = None\n",
        "            for n in self.nodes.values():\n",
        "                key = n.decode_global_id(gid)\n",
        "                if key is not None:\n",
        "                    break\n",
        "            if key is None:\n",
        "                continue\n",
        "            doc = self.get(key)\n",
        "            payload = None if doc[\"latest\"] is None else doc[\"latest\"][\"payload\"]\n",
        "            hits.append({\"score\": float(score), \"key\": key, \"payload\": payload})\n",
        "\n",
        "        return {\n",
        "            \"k\": int(k),\n",
        "            \"fanout\": None if fanout is None else int(fanout),\n",
        "            \"metric\": self.metric,\n",
        "            \"per_node\": per_node,\n",
        "            \"hits\": hits,\n",
        "        }\n",
        "\n",
        "    def _anti_entropy_loop(self):\n",
        "        while not self._stop.is_set():\n",
        "            try:\n",
        "                self._anti_entropy_round()\n",
        "            except Exception:\n",
        "                pass\n",
        "            self._stop.wait(self._anti_entropy_interval_s)\n",
        "\n",
        "    def _anti_entropy_round(self):\n",
        "        nids = self.node_ids[:]\n",
        "        self._rng.shuffle(nids)\n",
        "        pairs = list(zip(nids[::2], nids[1::2]))\n",
        "        for a, b in pairs:\n",
        "            na, nb = self.nodes[a], self.nodes[b]\n",
        "            if not (na.is_up() and nb.is_up()):\n",
        "                continue\n",
        "            da, db = na.digest(), nb.digest()\n",
        "            if da == db:\n",
        "                continue\n",
        "\n",
        "            meta_a = na.meta_snapshot()\n",
        "            meta_b = nb.meta_snapshot()\n",
        "\n",
        "            need_a = na.diff_keys(meta_b)\n",
        "            need_b = nb.diff_keys(meta_a)\n",
        "\n",
        "            if need_a:\n",
        "                got = nb.batch_get(need_a)\n",
        "                for rec in got.values():\n",
        "                    na.upsert(rec)\n",
        "            if need_b:\n",
        "                got = na.batch_get(need_b)\n",
        "                for rec in got.values():\n",
        "                    nb.upsert(rec)\n",
        "\n",
        "    def bulk_upsert(self, items: List[Tuple[str, np.ndarray, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        ok = 0\n",
        "        bad = 0\n",
        "        details = []\n",
        "        for key, vec, payload in items:\n",
        "            res = self.upsert(key, vec, payload)\n",
        "            if res[\"ok\"]:\n",
        "                ok += 1\n",
        "            else:\n",
        "                bad += 1\n",
        "            details.append({\"key\": key, \"ok\": res[\"ok\"], \"acks\": res[\"acks\"], \"owners\": res[\"owners\"], \"errors\": res[\"errors\"]})\n",
        "        return {\"ok\": ok, \"bad\": bad, \"total\": ok + bad, \"details_preview\": details[:5]}"
      ],
      "metadata": {
        "id": "2b5aGuMKGU-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj9_vMdiEOx3",
        "outputId": "8151c766-c4b4-4aa5-effc-007c4eba6af4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Distributed Vector DB demo: sharding + replication + quorums\n",
            "============================================================\n",
            "\n",
            "Ingesting data (bulk upsert)...\n",
            "{\n",
            "  \"ok\": 500,\n",
            "  \"bad\": 0,\n",
            "  \"total\": 500\n",
            "}\n",
            "\n",
            "KNN search (fanout all nodes)...\n",
            "Top hits:\n",
            "  score=0.9999  key=doc:0133  tag=topic-3\n",
            "  score=0.4128  key=doc:0033  tag=topic-3\n",
            "  score=0.3697  key=doc:0137  tag=topic-7\n",
            "  score=0.3572  key=doc:0214  tag=topic-4\n",
            "  score=0.3321  key=doc:0267  tag=topic-7\n",
            "  score=0.3168  key=doc:0155  tag=topic-5\n",
            "  score=0.2767  key=doc:0462  tag=topic-2\n",
            "  score=0.2767  key=doc:0462  tag=topic-2\n",
            "\n",
            "Shard ownership examples (consistent hashing + RF=2):\n",
            "  doc:0001 -> owners=['node-b', 'node-d']\n",
            "  doc:0133 -> owners=['node-a', 'node-c']\n",
            "  doc:0499 -> owners=['node-c', 'node-d']\n",
            "\n",
            "--- Simulating failure: bringing DOWN one owner and testing writes ---\n",
            "Key doc:0001 owners=['node-b', 'node-d'] -> taking DOWN node-b\n",
            "\n",
            "Upserting an update for that key (W=2, RF=2): expect failure if one replica down.\n",
            "{\n",
            "  \"key\": \"doc:0001\",\n",
            "  \"owners\": [\n",
            "    \"node-b\",\n",
            "    \"node-d\"\n",
            "  ],\n",
            "  \"acks\": 1,\n",
            "  \"required_w\": 2,\n",
            "  \"applied_nodes\": [\n",
            "    \"node-d\"\n",
            "  ],\n",
            "  \"ok\": false,\n",
            "  \"errors\": [\n",
            "    \"node-b: Node node-b is DOWN\"\n",
            "  ],\n",
            "  \"version\": 2,\n",
            "  \"ts_ms\": 1771675438761\n",
            "}\n",
            "\n",
            "Reading that key (R=1): should still succeed if at least one replica up.\n",
            "{\n",
            "  \"key\": \"doc:0001\",\n",
            "  \"owners\": [\n",
            "    \"node-b\",\n",
            "    \"node-d\"\n",
            "  ],\n",
            "  \"ok\": true,\n",
            "  \"latest\": {\n",
            "    \"version\": 2,\n",
            "    \"ts_ms\": 1771675438761,\n",
            "    \"payload\": {\n",
            "      \"title\": \"Document 1 (UPDATED)\",\n",
            "      \"tag\": \"topic-updated\",\n",
            "      \"i\": 1\n",
            "    },\n",
            "    \"vec_preview\": [\n",
            "      -0.17692522704601288,\n",
            "      0.16016821563243866,\n",
            "      0.05422502011060715,\n",
            "      -0.06879984587430954,\n",
            "      0.04374275356531143,\n",
            "      -0.03912888094782829\n",
            "    ]\n",
            "  },\n",
            "  \"read_repair_applied_to\": []\n",
            "}\n",
            "\n",
            "Bringing node back UP and letting anti-entropy repair state...\n",
            "\n",
            "Reading again: should show consistent latest state across replicas (and possibly repaired).\n",
            "{\n",
            "  \"key\": \"doc:0001\",\n",
            "  \"owners\": [\n",
            "    \"node-b\",\n",
            "    \"node-d\"\n",
            "  ],\n",
            "  \"ok\": true,\n",
            "  \"latest\": {\n",
            "    \"version\": 2,\n",
            "    \"ts_ms\": 1771675438761,\n",
            "    \"payload\": {\n",
            "      \"title\": \"Document 1 (UPDATED)\",\n",
            "      \"tag\": \"topic-updated\",\n",
            "      \"i\": 1\n",
            "    },\n",
            "    \"vec_preview\": [\n",
            "      -0.17692522704601288,\n",
            "      0.16016821563243866,\n",
            "      0.05422502011060715,\n",
            "      -0.06879984587430954,\n",
            "      0.04374275356531143,\n",
            "      -0.03912888094782829\n",
            "    ]\n",
            "  },\n",
            "  \"read_repair_applied_to\": []\n",
            "}\n",
            "\n",
            "--- Forcing staleness + read-repair demo ---\n",
            "doc:0420 owners=['node-b', 'node-a']\n",
            "Write result:\n",
            "{\n",
            "  \"key\": \"doc:0420\",\n",
            "  \"owners\": [\n",
            "    \"node-b\",\n",
            "    \"node-a\"\n",
            "  ],\n",
            "  \"acks\": 1,\n",
            "  \"required_w\": 2,\n",
            "  \"applied_nodes\": [\n",
            "    \"node-b\"\n",
            "  ],\n",
            "  \"ok\": false,\n",
            "  \"errors\": [\n",
            "    \"node-a: Node node-a is DOWN\"\n",
            "  ],\n",
            "  \"version\": 2,\n",
            "  \"ts_ms\": 1771675441964\n",
            "}\n",
            "\n",
            "Now GET (R=1) should return latest from any replica and read-repair the stale one if needed:\n",
            "{\n",
            "  \"key\": \"doc:0420\",\n",
            "  \"owners\": [\n",
            "    \"node-b\",\n",
            "    \"node-a\"\n",
            "  ],\n",
            "  \"ok\": true,\n",
            "  \"latest\": {\n",
            "    \"version\": 2,\n",
            "    \"ts_ms\": 1771675441964,\n",
            "    \"payload\": {\n",
            "      \"title\": \"Doc 420 NEW\",\n",
            "      \"tag\": \"topic-new\",\n",
            "      \"i\": 420\n",
            "    },\n",
            "    \"vec_preview\": [\n",
            "      -0.16486862301826477,\n",
            "      -0.021113602444529533,\n",
            "      0.04285320267081261,\n",
            "      -0.11799521744251251,\n",
            "      0.25182056427001953,\n",
            "      0.13627614080905914\n",
            "    ]\n",
            "  },\n",
            "  \"read_repair_applied_to\": [\n",
            "    \"node-a\"\n",
            "  ]\n",
            "}\n",
            "\n",
            "KNN search after repairs (fanout=2 random nodes, showing robustness):\n",
            "  score=0.9999  key=doc:0133  title=Document 133\n",
            "  score=0.4128  key=doc:0033  title=Document 33\n",
            "  score=0.3697  key=doc:0137  title=Document 137\n",
            "  score=0.3572  key=doc:0214  title=Document 214\n",
            "  score=0.3321  key=doc:0267  title=Document 267\n",
            "  score=0.3168  key=doc:0155  title=Document 155\n",
            "\n",
            "We now have a working simulated distributed vector DB with sharding + replication + quorum + repair.\n"
          ]
        }
      ],
      "source": [
        "print(\"============================================================\")\n",
        "print(\"Distributed Vector DB demo: sharding + replication + quorums\")\n",
        "print(\"============================================================\")\n",
        "\n",
        "DIM = 64\n",
        "NODES = [\"node-a\", \"node-b\", \"node-c\", \"node-d\"]\n",
        "RF = 2\n",
        "W = 2\n",
        "R = 1\n",
        "\n",
        "db = DistributedVectorDB(\n",
        "    dim=DIM,\n",
        "    node_ids=NODES,\n",
        "    rf=RF,\n",
        "    w=W,\n",
        "    r=R,\n",
        "    vnodes=128,\n",
        "    metric=\"cosine\",\n",
        "    anti_entropy_interval_s=1.5,\n",
        "    anti_entropy_enabled=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "rng = np.random.default_rng(123)\n",
        "num_items = 500\n",
        "base = rng.normal(size=(num_items, DIM)).astype(np.float32)\n",
        "\n",
        "items = []\n",
        "for i in range(num_items):\n",
        "    key = f\"doc:{i:04d}\"\n",
        "    vec = base[i]\n",
        "    payload = {\"title\": f\"Document {i}\", \"tag\": f\"topic-{i%10}\", \"i\": i}\n",
        "    items.append((key, vec, payload))\n",
        "\n",
        "print(\"\\nIngesting data (bulk upsert)...\")\n",
        "ing = db.bulk_upsert(items)\n",
        "print(json.dumps({k: ing[k] for k in [\"ok\", \"bad\", \"total\"]}, indent=2))\n",
        "\n",
        "target_idx = 133\n",
        "q = base[target_idx] + 0.01 * rng.normal(size=(DIM,)).astype(np.float32)\n",
        "\n",
        "print(\"\\nKNN search (fanout all nodes)...\")\n",
        "res = db.knn_search(q, k=8, fanout=None)\n",
        "print(\"Top hits:\")\n",
        "for h in res[\"hits\"][:8]:\n",
        "    print(f\"  score={h['score']:.4f}  key={h['key']}  tag={h['payload']['tag'] if h['payload'] else None}\")\n",
        "\n",
        "print(\"\\nShard ownership examples (consistent hashing + RF=2):\")\n",
        "for key in [\"doc:0001\", \"doc:0133\", \"doc:0499\"]:\n",
        "    owners = [n.node_id for n in db._owners(key)]\n",
        "    print(f\"  {key} -> owners={owners}\")\n",
        "\n",
        "print(\"\\n--- Simulating failure: bringing DOWN one owner and testing writes ---\")\n",
        "test_key = \"doc:0001\"\n",
        "owners = [n.node_id for n in db._owners(test_key)]\n",
        "down_node = owners[0]\n",
        "print(f\"Key {test_key} owners={owners} -> taking DOWN {down_node}\")\n",
        "db.set_node_up(down_node, False)\n",
        "\n",
        "print(\"\\nUpserting an update for that key (W=2, RF=2): expect failure if one replica down.\")\n",
        "update_vec = (base[1] + 0.2 * rng.normal(size=(DIM,)).astype(np.float32))\n",
        "wr = db.upsert(test_key, update_vec, {\"title\": \"Document 1 (UPDATED)\", \"tag\": \"topic-updated\", \"i\": 1})\n",
        "print(json.dumps(wr, indent=2))\n",
        "\n",
        "print(\"\\nReading that key (R=1): should still succeed if at least one replica up.\")\n",
        "gr = db.get(test_key)\n",
        "print(json.dumps({k: gr[k] for k in [\"key\", \"owners\", \"ok\", \"latest\", \"read_repair_applied_to\"]}, indent=2))\n",
        "\n",
        "print(\"\\nBringing node back UP and letting anti-entropy repair state...\")\n",
        "db.set_node_up(down_node, True)\n",
        "time.sleep(3.2)\n",
        "\n",
        "print(\"\\nReading again: should show consistent latest state across replicas (and possibly repaired).\")\n",
        "gr2 = db.get(test_key)\n",
        "print(json.dumps({k: gr2[k] for k in [\"key\", \"owners\", \"ok\", \"latest\", \"read_repair_applied_to\"]}, indent=2))\n",
        "\n",
        "print(\"\\n--- Forcing staleness + read-repair demo ---\")\n",
        "key2 = \"doc:0420\"\n",
        "owners2 = [n.node_id for n in db._owners(key2)]\n",
        "print(f\"{key2} owners={owners2}\")\n",
        "\n",
        "db.set_node_up(owners2[1], False)\n",
        "wr2 = db.upsert(key2, base[420] + 0.3 * rng.normal(size=(DIM,)).astype(np.float32), {\"title\": \"Doc 420 NEW\", \"tag\": \"topic-new\", \"i\": 420})\n",
        "db.set_node_up(owners2[1], True)\n",
        "\n",
        "print(\"Write result:\")\n",
        "print(json.dumps(wr2, indent=2))\n",
        "\n",
        "print(\"\\nNow GET (R=1) should return latest from any replica and read-repair the stale one if needed:\")\n",
        "gr3 = db.get(key2)\n",
        "print(json.dumps({k: gr3[k] for k in [\"key\", \"owners\", \"ok\", \"latest\", \"read_repair_applied_to\"]}, indent=2))\n",
        "\n",
        "print(\"\\nKNN search after repairs (fanout=2 random nodes, showing robustness):\")\n",
        "res2 = db.knn_search(q, k=6, fanout=2)\n",
        "for h in res2[\"hits\"]:\n",
        "    print(f\"  score={h['score']:.4f}  key={h['key']}  title={h['payload']['title'] if h['payload'] else None}\")\n",
        "\n",
        "db.stop()\n",
        "print(\"\\nWe now have a working simulated distributed vector DB with sharding + replication + quorum + repair.\")"
      ]
    }
  ]
}