{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Softmax From Scratch: Avoiding the Numerical Stability Trap\n",
        "In deep learning, classification models don’t just need to make predictions—they need to express confidence. That’s where the Softmax activation function comes in. Softmax takes the raw, unbounded scores produced by a neural network and transforms them into a well-defined probability distribution, making it possible to interpret each output as the likelihood of a specific class.\n",
        "\n",
        "This property makes Softmax a cornerstone of multi-class classification tasks, from image recognition to language modeling."
      ],
      "metadata": {
        "id": "LPVdMBYu1zwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Naive Softmax\n",
        "This function implements the Softmax activation in its most straightforward form. It exponentiates each logit and normalizes it by the sum of all exponentiated values across classes, producing a probability distribution for each input sample.\n",
        "\n",
        "While this implementation is mathematically correct and easy to read, it is numerically unstable—large positive logits can cause overflow, and large negative logits can underflow to zero. As a result, this version should be avoided in real training pipelines.\n"
      ],
      "metadata": {
        "id": "epwvsbXW14L3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def softmax_naive(logits):\n",
        "    exp_logits = torch.exp(logits)\n",
        "    return exp_logits / exp_logits.sum(dim=1, keepdim=True)"
      ],
      "metadata": {
        "id": "JB7kUQvbsr7s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Logits and Target Labels\n",
        "This example defines a small batch with three samples and three classes to illustrate both normal and failure cases. The first and third samples contain reasonable logit values and behave as expected during Softmax computation. The second sample intentionally includes extreme values (1000 and -1000) to demonstrate numerical instability—this is where the naive Softmax implementation breaks down.\n",
        "\n",
        "The targets tensor specifies the correct class index for each sample and will be used to compute the classification loss and observe how instability propagates during backpropagation.\n"
      ],
      "metadata": {
        "id": "tVLiPo8b17un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch of 3 samples, 3 classes\n",
        "logits = torch.tensor([\n",
        "    [2.0, 1.0, 0.1],\n",
        "    [1000.0, 1.0, -1000.0],\n",
        "    [3.0, 2.0, 1.0]\n",
        "], requires_grad=True)\n",
        "\n",
        "targets = torch.tensor([0, 2, 1])"
      ],
      "metadata": {
        "id": "B2OsUTLVsu5j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Pass: Softmax Output and the Failure Case\n",
        "During the forward pass, the naive Softmax function is applied to the logits to produce class probabilities. For normal logit values (first and third samples), the output is a valid probability distribution where values lie between 0 and 1 and sum to 1.\n",
        "\n",
        "However, the second sample clearly exposes the numerical issue: exponentiating 1000 overflows to infinity, while -1000 underflows to zero. This results in invalid operations during normalization, producing NaN values and zero probabilities. Once NaN appears at this stage, it contaminates all subsequent computations, making the model unusable for training.\n",
        "\n"
      ],
      "metadata": {
        "id": "4h04HFik1_MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward pass\n",
        "probs = softmax_naive(logits)\n",
        "\n",
        "print(\"Softmax probabilities:\")\n",
        "print(probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9WbWVHFs0Wi",
        "outputId": "f879d791-7c9d-4984-8e44-43a221908228"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax probabilities:\n",
            "tensor([[0.6590, 0.2424, 0.0986],\n",
            "        [   nan, 0.0000, 0.0000],\n",
            "        [0.6652, 0.2447, 0.0900]], grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Probabilities and Loss Breakdown\n",
        "Here, we extract the predicted probability corresponding to the true class for each sample. While the first and third samples return valid probabilities, the second sample’s target probability is 0.0, caused by numerical underflow in the Softmax computation. When the loss is calculated using -log(p), taking the logarithm of 0.0 results in +∞.\n",
        "\n",
        "This makes the overall loss infinite, which is a critical failure during training. Once the loss becomes infinite, gradient computation becomes unstable, leading to NaNs during backpropagation and effectively halting learning.\n"
      ],
      "metadata": {
        "id": "j86WpO7L2I5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract target probabilities\n",
        "target_probs = probs[torch.arange(len(targets)), targets]\n",
        "\n",
        "print(\"\\nTarget probabilities:\")\n",
        "print(target_probs)\n",
        "\n",
        "# Compute loss\n",
        "loss = -torch.log(target_probs).mean()\n",
        "print(\"\\nLoss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSeayiO0s4Cq",
        "outputId": "b2e54f19-a730-4301-e4e2-90f72cb1e950"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target probabilities:\n",
            "tensor([0.6590, 0.0000, 0.2447], grad_fn=<IndexBackward0>)\n",
            "\n",
            "Loss: tensor(inf, grad_fn=<NegBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation: Gradient Corruption\n",
        "When backpropagation is triggered, the impact of the infinite loss becomes immediately visible. The gradients for the first and third samples remain finite because their Softmax outputs were well-behaved. However, the second sample produces NaN gradients across all classes due to the log(0) operation in the loss.\n",
        "\n",
        "These NaNs propagate backward through the network, contaminating weight updates and effectively breaking training. This is why numerical instability at the Softmax–loss boundary is so dangerous—once NaNs appear, recovery is nearly impossible without restarting training.\n"
      ],
      "metadata": {
        "id": "zD0AxP2-2MOC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "\n",
        "print(\"\\nGradients:\")\n",
        "print(logits.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ5XyZGts8WC",
        "outputId": "9cbbf125-63c7-4158-b4fa-5a94b41dd3b3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gradients:\n",
            "tensor([[-0.1137,  0.0808,  0.0329],\n",
            "        [    nan,     nan,     nan],\n",
            "        [ 0.2217, -0.2518,  0.0300]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Stable Cross-Entropy Loss Using LogSumExp\n",
        "This implementation computes cross-entropy loss directly from raw logits without explicitly calculating Softmax probabilities. To maintain numerical stability, the logits are first shifted by subtracting the maximum value per sample, ensuring exponentials stay within a safe range.\n",
        "\n",
        "The LogSumExp trick is then used to compute the normalization term, after which the original (unshifted) target logit is subtracted to obtain the correct loss. This approach avoids overflow, underflow, and NaN gradients, and mirrors how cross-entropy is implemented in production-grade deep learning frameworks.\n"
      ],
      "metadata": {
        "id": "mcCpcTTVzB3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stable_cross_entropy(logits, targets):\n",
        "\n",
        "    # Find max logit per sample\n",
        "    max_logits, _ = torch.max(logits, dim=1, keepdim=True)\n",
        "\n",
        "    # Shift logits for numerical stability\n",
        "    shifted_logits = logits - max_logits\n",
        "\n",
        "    # Compute LogSumExp\n",
        "    log_sum_exp = torch.log(torch.sum(torch.exp(shifted_logits), dim=1)) + max_logits.squeeze(1)\n",
        "\n",
        "    # Compute loss using ORIGINAL logits\n",
        "    loss = log_sum_exp - logits[torch.arange(len(targets)), targets]\n",
        "\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "id": "ofmdF27GxouN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable Forward and Backward Pass\n",
        "Running the stable cross-entropy implementation on the same extreme logits produces a finite loss and well-defined gradients. Even though one sample contains very large values (1000 and -1000), the LogSumExp formulation keeps all intermediate computations in a safe numerical range. As a result, backpropagation completes successfully without producing NaNs, and each class receives a meaningful gradient signal.\n",
        "\n",
        "This confirms that the instability seen earlier was not caused by the data itself, but by the naive separation of Softmax and cross-entropy—an issue fully resolved by using a numerically stable, fused loss formulation.\n"
      ],
      "metadata": {
        "id": "fBNd_P252W8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = torch.tensor([\n",
        "    [2.0, 1.0, 0.1],\n",
        "    [1000.0, 1.0, -1000.0],\n",
        "    [3.0, 2.0, 1.0]\n",
        "], requires_grad=True)\n",
        "\n",
        "targets = torch.tensor([0, 2, 1])\n",
        "\n",
        "loss = stable_cross_entropy(logits, targets)\n",
        "print(\"Stable loss:\", loss)\n",
        "\n",
        "loss.backward()\n",
        "print(\"\\nGradients:\")\n",
        "print(logits.grad)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIWwWjIfxulN",
        "outputId": "81ef4e69-deb4-4522-952a-78c43415f71c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stable loss: tensor(667.2748, grad_fn=<MeanBackward0>)\n",
            "\n",
            "Gradients:\n",
            "tensor([[-0.1137,  0.0808,  0.0329],\n",
            "        [ 0.3333,  0.0000, -0.3333],\n",
            "        [ 0.2217, -0.2518,  0.0300]])\n"
          ]
        }
      ]
    }
  ]
}