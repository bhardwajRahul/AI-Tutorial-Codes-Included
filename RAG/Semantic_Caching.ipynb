{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB11r3mUzli_",
        "outputId": "4728427a-b75c-4fd8-92a6-a89138b9886c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the dependencies"
      ],
      "metadata": {
        "id": "L5vd0psh-Diu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter OpenAI API Key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YINENiBGzqjM",
        "outputId": "43d36a6e-42c3-4be5-a59a-1f2fb3ff4e7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenAI API Key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "6UtIppHIzyVq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Repeated Queries Without Caching\n",
        "In this section, we run the same query 10 times directly through the GPT-4.1 model to observe how long it takes when no caching mechanism is applied. Each call triggers a full LLM computation and response generation, leading to repetitive processing for identical inputs.\n",
        "\n",
        "This helps establish a baseline for total time and cost before we implement semantic caching in the next part."
      ],
      "metadata": {
        "id": "gAgwbzF--Ie4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def ask_gpt(query):\n",
        "    start = time.time()\n",
        "    response = client.responses.create(\n",
        "      model=\"gpt-4.1\",\n",
        "      input=query\n",
        "    )\n",
        "    end = time.time()\n",
        "    return response.output[0].content[0].text, end - start"
      ],
      "metadata": {
        "id": "qHf1N7y6z1Wi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Explain the concept of semantic caching in just 2 lines.\"\n",
        "total_time = 0\n",
        "\n",
        "for i in range(10):\n",
        "    _, duration = ask_gpt(query)\n",
        "    total_time += duration\n",
        "    print(f\"Run {i+1} took {duration:.2f} seconds\")\n",
        "\n",
        "print(f\"\\nTotal time for 10 runs: {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5gNG4SY0Dur",
        "outputId": "043322e9-944e-4f56-c0e7-3a88a048af75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run 1 took 3.10 seconds\n",
            "Run 2 took 1.36 seconds\n",
            "Run 3 took 1.36 seconds\n",
            "Run 4 took 3.03 seconds\n",
            "Run 5 took 1.27 seconds\n",
            "Run 6 took 3.45 seconds\n",
            "Run 7 took 1.60 seconds\n",
            "Run 8 took 1.60 seconds\n",
            "Run 9 took 1.88 seconds\n",
            "Run 10 took 3.21 seconds\n",
            "\n",
            "Total time for 10 runs: 21.87 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the query remains the same, every call still takes between 1‚Äì3 seconds, resulting in a total of ~22 seconds for 10 runs. This inefficiency highlights why semantic caching can be so valuable ‚Äî it allows us to reuse previous responses for semantically identical queries and save both time and API cost."
      ],
      "metadata": {
        "id": "kfCp5x4y-Spc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Semantic Caching for Faster Responses\n",
        "In this section, we enhance the previous setup by introducing semantic caching, which allows our application to reuse responses for semantically similar queries instead of repeatedly calling the GPT-4.1 API.\n",
        "\n",
        "Here‚Äôs how it works: each incoming query is converted into a vector embedding using the text-embedding-3-small model. This embedding captures the semantic meaning of the text. When a new query arrives, we calculate its cosine similarity with embeddings already stored in our cache. If a match is found with a similarity score above the defined threshold (e.g., 0.85), the system instantly returns the cached response ‚Äî avoiding another API call.\n",
        "\n",
        "If no sufficiently similar query exists in the cache, the model generates a fresh response, which is then stored along with its embedding for future use. Over time, this approach dramatically reduces both response time and API costs, especially for frequently asked or rephrased queries."
      ],
      "metadata": {
        "id": "qvcSmFYw-gRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "semantic_cache = []\n",
        "\n",
        "def get_embedding(text):\n",
        "    emb = client.embeddings.create(model=\"text-embedding-3-small\", input=text)\n",
        "    return np.array(emb.data[0].embedding)"
      ],
      "metadata": {
        "id": "vXweZ3SH0t59"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (norm(a) * norm(b))\n",
        "\n",
        "def ask_gpt_with_cache(query, threshold=0.85):\n",
        "    query_embedding = get_embedding(query)\n",
        "\n",
        "    # Check similarity with existing cache\n",
        "    for cached_query, cached_emb, cached_resp in semantic_cache:\n",
        "        sim = cosine_similarity(query_embedding, cached_emb)\n",
        "        if sim > threshold:\n",
        "            print(f\"üîÅ Using cached response (similarity: {sim:.2f})\")\n",
        "            return cached_resp, 0.0  # no API time\n",
        "\n",
        "    # Otherwise, call GPT\n",
        "    start = time.time()\n",
        "    response = client.responses.create(\n",
        "        model=\"gpt-4.1\",\n",
        "        input=query\n",
        "    )\n",
        "    end = time.time()\n",
        "    text = response.output[0].content[0].text\n",
        "\n",
        "    # Store in cache\n",
        "    semantic_cache.append((query, query_embedding, text))\n",
        "    return text, end - start"
      ],
      "metadata": {
        "id": "nAwQh6CT0ym6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [\n",
        "    \"Explain semantic caching in simple terms.\",\n",
        "    \"What is semantic caching and how does it work?\",\n",
        "    \"How does caching work in LLMs?\",\n",
        "    \"Tell me about semantic caching for LLMs.\",\n",
        "    \"Explain semantic caching simply.\",\n",
        "]"
      ],
      "metadata": {
        "id": "hH5LLitR00cB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_time = 0\n",
        "for q in queries:\n",
        "    resp, t = ask_gpt_with_cache(q)\n",
        "    total_time += t\n",
        "    print(f\"‚è±Ô∏è Query took {t:.2f} seconds\\n\")\n",
        "\n",
        "print(f\"\\nTotal time with caching: {total_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UJEAnYe1AdW",
        "outputId": "51188bfa-d5c2-4e88-bbad-3a3fcff64603"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è±Ô∏è Query took 8.28 seconds\n",
            "\n",
            "üîÅ Using cached response (similarity: 0.86)\n",
            "‚è±Ô∏è Query took 0.00 seconds\n",
            "\n",
            "‚è±Ô∏è Query took 15.84 seconds\n",
            "\n",
            "‚è±Ô∏è Query took 11.77 seconds\n",
            "\n",
            "üîÅ Using cached response (similarity: 0.97)\n",
            "‚è±Ô∏è Query took 0.00 seconds\n",
            "\n",
            "\n",
            "Total time with caching: 35.89 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the output, the first query took around 8 seconds as there was no cache and the model had to generate a fresh response. When a similar question was asked next, the system identified a high semantic similarity (0.86) and instantly reused the cached answer, saving time. Some queries, like ‚ÄúHow does caching work in LLMs?‚Äù and ‚ÄúTell me about semantic caching for LLMs,‚Äù were sufficiently different, so the model generated new responses, each taking over 10 seconds. The final query was nearly identical to the first one (similarity 0.97) and was served from cache instantly."
      ],
      "metadata": {
        "id": "EhZpaM7--zpL"
      }
    }
  ]
}